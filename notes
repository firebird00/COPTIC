SOR NOTES

Problems with constructing the MPI red/black boundary communicators
___________________________________________________________________

In general, the construction of the multidimensional odd/even
communicator can be thought of as follows:

odd(0)=this element
even(0)=null element
for dimension n=1,nd
	laminate alternating odd(n-1)/even(n-1) into odd(n) face
	laminate alternating even(n-1)/odd(n-1) into even(n) face
endfor

For n>0, the number of non-null elements is the same for odd and even.
But for n=0, the even element is of length 0 which is troublesome.

Therefore it might be better to start at level 1:

odd(1)=odd elements in this dimension
even(1)=even elements in this dimension
for dimension n=2,nd
	laminate alternating odd(n-1)/even(n-1) into odd(n) face
	laminate alternating even(n-1)/odd(n-1) into even(n) face
endfor

This construction must be done nd times, for each one, omitting the 
direction of the face normal nn from the dimension iteration. So really
it is

for dimension n=1,nd
	if(n.ne.nn) then
	laminate alternating odd(n-1)/even(n-1) into odd(n) face
	laminate alternating even(n-1)/odd(n-1) into even(n) face
	endif
endfor

or maybe better:

for dimension nc=nn,nn+nd-2
	n=mod(nc,nd)+1
	laminate alternating odd(n-1)/even(n-1) into odd(n) face
	laminate alternating even(n-1)/odd(n-1) into even(n) face
endfor

Because we are alternating types, we have to use
MPI_TYPE_CREATE_STRUCT, which is the only constructor that allows
different types in the creation.  In that case, we need arrays of
lengths,displacements,types. The length of these arrays is (1/2 times)
the block-side-length, which won't be known. We would either have to
choose a long length or allocate working storage.

Also, it is not clear that there exists a null element in MPI.

If we have to provide working storage, then maybe we ought just to
construct our own indexes and create the communicator directly by
indexing. Then we only have to provide an index array. But it might
have to be substantially bigger.

If we do the lamination directly, what does it consist of?

The even or odd type can be considered to consist of an array of
indexes, referring to the addresses of the values, plus a total length.
A lamination then consists of taking an odd type, taking an even type
shifted by appropriate amount, and concatenating them, then taking
a shifted odd concatenating that, ...

oddnm1=start of odd
evennm1=start of even
odd=start of new odd
do i=1,nsidei/2
   do j=1,nsidej/2
      index(jc)=index(j+odd)
      jc=jc+1
   enddo
   do j=1,nsidej/2
      index(jc)=index(j+even)
      jc=jc+1
   enddo
enddo
ditto for even

-------------------------------------------------------------------
7 Jan 06

Got the block-divided SOR to work, running on MPI processes (on same
machine). Then realized that the gather, to get the individual
solutions back into one big matrix is quite tricky.

One problem is that the order of the blocks generated by the MPI_CART
creation is row major, not column major like fortran, i.e. it goes
in the order 1 (0,0); 2 (0,1); 3 (0,2); 4 (1,0); 5 (1,1); ... etc.
Consequently, when you do a gather, the blocks come back not in the
order you want to store them in in fortran, which would be column
major. That makes it much more difficult to specify the gather, because
a gather puts the data from process-j, in the j-th storage position,
relative to the starting point. It would all be trivial if the blocks
were all the same size, and the order was right. 

It seems that this ought to be fixable by reversing the order of the
suffices. It would be too confusing to do this in the outer fortran
call and specification, I think. But presumably it could be done inside
the block-specification program. The problem can presumably be fixed
by reversing the order of dimensions in the calls to MPI_CARTs.
Yes that seems to work fixing all the MPI_CART calls (create, get, shift).

Now in principle one ought for equal sized blocks to be able to do a
simple gather. There are still major problems with unequal sized blocks.
I suppose that one can construct a generalized call that will interleave
the larger blocks. The iorig pointers to block start are now in the order
of mycartid. This enables iorig-1 to be used for the displacement array.
If all blockstypes were equal we could do
call MPI_[ALL]GATHERV(u,1,blocktype,u,[1],[iorig-1],blocktype,
	icommcart,ierr)
but the problem is that [iorig-1] means an array with values equal
to iorig(j)-1, because of the zero-based C offsets used in the MPI
routines, compared with the 1-based fortran pointers. It would be
possible to change my convention to zero-based. Then a few modifications
elsewhere would be necessary.

Alternatively, one could presumably not bother with all this reordering
of the mycartid, and instead manually reorder the blocks for the
purposes of calls. Do this by creating an array that translates 
mycartid into fortran order. In other words ifortorder(mycartid) =
fortran column major ordered indices, then iorig(ifortorder(j))-1
is the displacement of the jth block. 
	do j=1,nproc
	   idisp(j)=iorig(ifortorder(j))-1
	   irecvcounts(j)=1
	enddo
call MPI_[ALL]GATHERV(u,1,blocktype,u,irecvcounts,idisp,blocktype,
	icommcart,ierr)
Then none of the fiddling around with dimension reversal ought to be 
necessary. There might be a slight memory access hit because the blocks
will come in a strange order. For that reason, the dimension reversal
might still be worth it. 

But wait, it says that the equivalent receive is 
MPI_Recv(recvbuf+disp[i].extent(recvtype),recvtype,i...)
whereas I am taking the displacement to be in units of the underlying
REAL, not the extent of the structure: A problem.

On p207 there seems to be a thing called MPI_UB which is the equivalent
of a null send, but takes up space in the MPI Type. I guess this is 
within a struct. Apparently this is for setting the Upper Bound of a
structure, and there is a type MPI_LB for lower bound. They are not
listed in the type map but affect the value of extent. Apparently
"MPI_TYPE_STRUCT is the only constructor that allows MPI pseudo types
MPI_UB and LB"

-----------------------------------------------------------------------
16 Jan 06
Now have a 2d specific sor2dmpi and a general-dimension sormpi version
going that includes the MPI communications. The general dimension
version is not really fully tested at this stage. Also its Jacobi
radius has a rather different optimum for convergence.

After a day of work we seem to have a multidimensional version of sormpi
working. 

There is a bug somewhere because although different block divisions
of the first two dimensions give identical results, different block
numbers in the third dimension give subtly different results. This 
should not happen.

18 Jan 06
Found the bug, it was in the choice of parity when using the blocks
with the boundary excluded. The parity must be changed by ndims.


_______________________________________________________________________

Turning SORMPI into a solver for embedded boundaries.
_______________________________________________________________________

29 Dec 06

Implemented a test routine comparing solution with smt.out. This is
to discover any errors that I might accidentally introduce. smt.fixed is the
output with which to compare. Comes from the charge-ball case. 

Plan is to introduce boundary handling into this solver code.
For memory access optimization, the plan is to make the pointer to 
the boundary information part of cij, namely the element 2*ndims+1. 
For this I need to fix some addressing in the routines, but not very much.

Added the extra cij slot as cij(2*ndims+1,i,j,...). Works. 
Document minimally. 

Passed the object data obj(idata,jpointer) through to sorelaxgen, which
is where it will be needed for the treatment. Also passed the length
nobj of the obj data (leading dimension). Demonstrated calling it.

30 Dec 06

Now we need a way to implement the objects. I have decided to separate
the object implementation from the rest of the code. That way the instability
of geometric object representation is separated. One can use whatever 
representation one likes provided one has a way to translate it into the
object data that we are going to use. However, we need an object-data
api or specification for how the object data is to be represented. This
needs to be fairly compact, and also not to require lots of processing
during the iterations. There are questions about different numbers of 
dimensions. 

If we make the handling of object data through a function or subroutine
call, we can make the api a matter for the user to specify. However, if
it is too general, then lots of things have to be passed. That would
be cumbersome. It is probably best to make sure that the cij do really 
contain all of the weight from adjacent points that we wish to put into
the numerator. Then the effect of the object code will be to add some
constant quantities. It seems as if this is all it really needs to be able
to do. In fact the main thing appears to be to have the quantity (bdy in
ES2D) to add to numerator, and maybe diag, since this is not now reliably
calculated by adding cijs. For the purposes of potential solving, this may
be all one needs for any given point. 2 quantities. For field interpolation
other information is almost certainly needed.

1 Jan 07

Slow progress. Implemented a cijroutine that is called by mditerate to
set the cij. The cijset.f was an explicit iteration, that I don't really
need since I use mditerate.

Fiddled with getting it to avoid the boundaries properly. Now ok.

Set up series of smt.out to do checking with. smt.1 is the output
from the original, Jan 2006 verified version. smtround.1 is an alternative
output with reduced significant figures. The idea is that if we
get things nearly right, they will match.
i
smt.2 and smtround.2 are the versions that implement the cij in terms
of dxs. They differ by rounding arising from the redefinition. The
difference between smtround.1 .2 are all in the final place. Still 
some differences at the 10^-4 level. Move the test to smt.2 so we don't
have to retain the nint that prevents rounding errors.

Implemented much of the infrastructure for handling the potential
intersections. What remains is to construct potlsect which returns
for any point, dimension, and direction the fraction and potential
value for any intersection between it and its neighbour. It returns
fraction 1 if there's no intersection.

There's a trap with the offsetting of indi that needs to be fixed.

2 Jan 07

Fixed the trap. Rationalized the routines. Wrote a plotting routine
to display the results of the cij setting. There's a problem with that
to some extent that myid is not known prior to the sormpi call, so all
the nodes do plotting, unless you do it after calling. If you do it before,
then all nodes call. One approach is to just call ./sormpitest then. 
That crashes after the plot as it tries to sormpi. Never mind.

Test obj setting seems to work for one point.

Got a sphere apparently working in cij and its plotting, but potential
solution is crazy. Saved its output as smt.b1 for now. 

That problem was that I was not doing the obj treatment in the solver.
Implemented it. Seems to work. 

Switched to pure Poisson (not shielded Poisson) solver. Looks good.
Have to be careful with the external declaration.
Turned off charge density for the test saved as smtsphere.1. Had to turn
off the hand-set object data too. Done.

We seem to have a working solver. Ran a 80x80x36 case in about 10s
on laptop. Took 321 iterations.

3 Jan 07

I realize that there might be good reason to include additional data
into obj. A couple of ideas: 

1. the object number(s) that the intersections correspond to. This
would be useful for calculating the total flux out of a given
object. However, it would have to be specified for each of the
directions. So that would be a very cumbersome number of additional
elements, almost doubling the storage, if done directly. Needs thought.

2. the address of the cij (or ijk pointer) of the mesh node that this
object belongs to. This would enable one to scan the obj data, and refer
back to the mesh, rather than having to scan the mesh and refer to the 
obj data. 

To store the latter in a real is somewhat problematic, since the mesh 
might become larger than the integer available. Probably one way to 
solve that is to use an equivalence of obj to an integer.

Another issue is the namespace that obj is in needs to be observed now
that it is in common. Start by fixing that. Done

Separated the specific objects into 3dobjects.f
Took out the save from circleset and created smtsphere.2 to match.

Object information: probably the best way to handle this is NOT to put
the additional information we might want to retain into the sor_obj
data. Instead we can optionally create and populate additional object
data structures that parallel the sor_obj and give the information.
This would be implemented in routine potlsect, where the intersections
and fractions are calculated. There's a slight problem in that isor_oi
is incremented only after potlsect has returned, so syncronizing the 
additional data with isor_oi is a little tricky. One way to syncronize
would be to pass iset to potsect, telling it thereby whether we have
already incremented the isor_oi or not. Then one just writes the additional
data into isor_oi-1+iset. If we are storing the pointer to cij, we
may also have to pass that to potsect. The parallel information could
be integer.

11 Jan 07
Implementing general boundary conditions. 
Converted the fixed boundary to general form. Some thoughts about
how to pass the a,b,c. 

Also need to pass back more information for the continuity condition.
Do that by changing from passing in dpm the fractional distance. Instead
pass the total distance, so that dp = dpm(i)*fraction(i) etc.
Unfortunately this changes the rounding. So store this result as
smtsphere.3

The general condition does not seem to be working correctly. There's a 
problem with the inactive side, it seems. Need better diagnostics.
Implemented surfmark. This seems to show that the mismatch is really 
occurring at the surface, not on adjacent points. It also shows the directions
of the connections are not what one might intuit from the surface plot.
So you have to be careful.

13 Jan 07

Finally found my problem. It was that I was storing Cd in the object instead
of the DIFFERENCE Cd-Cij, which is what I should have been storing. 

Adjusted the namespace control by changing to a suffix rather than a prefix
_sor. This enables variables to determine their type in the usual way.
objcom.f fixed. ctl_sor also fixed.

Change to using a proper 0-1 range for the variables. Replace smt.out
to smtsphere.4 smtround.3.

Doing numerical checks, I find that the logarithmic derivative setting
is not giving correct solutions. I find that adjusting the coefficients
inside cijroutine can correct. It seems that the problem is with my
implementation of the form for the general BC. I make d+ equal to 
apb+b/apb times dxp1, but then for the opposite direction, I have not used
d- equal to this same value. I have used just dxp1. That is incorrect.

Rebuilt the cij routine using the proper values for dminus (and the full
continuity expression). Gives correct answers 40x40x16 has maximum
error for the 1/r case of .0032 and SD .001. 
For 40x40x40
 Max error=  0.00121030153  Standard Deviation=  0.000311983633
This seems to be second order small.
For 80x80x80 the oi_sor overflowed. Doubling it solves. then
 Max error= -0.000307714043  Standard Deviation=  9.30291717E-05
Reduced by faster than dx. Not quite factor of 4 in SD, but yes in max.
Looks as if we are getting second order accuracy.
For 20x20x20
 Max error= -0.00420894753  Standard Deviation=  0.00136070442
Definitely looks like dx^2.
For 10x10x10
 Max error= -0.0172496308  Standard Deviation=  0.00511154439
For 100x100x100
 Max error= -0.000233839804  Standard Deviation=  7.77521054E-05
This is getting closer to the eps convergence:
 mi_sor,k_sor,xjac_sor,del_sor 1210 401  0.999440014  1.9293886E-05 0
Wrote plotconv to plot the convergence of these cases. Very clearly
quadratic except for the 100 case where we seem to be getting less than
quadratic reduction, which is not surprising since we are not necessarily
converging the iterations to significantly better accuracy.

Running quantitative test case again finds rounding differences, but
zero rounded differences. So consolidate in smtsphere.5, smtround.5

Summary: We have the scheme working and tested with both fixed potential
and logarithmic gradient boundary conditions. 

To do: media. Rethinking stored data (for charge assignment, object
identification, reverse reference to cij, etc).

Longer term ideas: curvilinear coordinates.

What to store in the object data? Prior idea was fraction, potential. 
This works for a fixed potential boundary and gives enough information
to generate an interpolated position. With other types of boundary it
is not so clear what one should store. 

18 Jan 07

Begun adjustment of the object data. First generalize the code so
that ndata_sor is a parameter that describes the amount of data per
direction. Ensure that it works for ndata_sor=3, even if we are only
using the prior two spaces. Of course, at present the data is only 
being used by the plotting routines for a graphic display.

Then I can add 1 to the start of each of the object data descriptor
pointers:
      parameter (ndims_sor=3,ndata_sor=3)
      parameter (nobj_sor=1+2*ndims_sor*ndata_sor+3)
      parameter (idgs_sor=1+2*ndims_sor*ndata_sor+1)
      parameter (ibdy_sor=1+2*ndims_sor*ndata_sor+2)
      parameter (iflag_sor=1+2*ndims_sor*ndata_sor+3)

Need to correct things all the way through sormpi.f. Probably it is better
to remove the process of passing the object data through and rely on 
external calls. So implement ddn_sor(ip,dden,dnum) that adjusts the numerator
and denominator. Unfortunately this gives rounding error changes.
Make smt.6

Remove the passing of nobj, and obj from the sormpi and sorrelaxgen calls.
This is now entirely in the common and its effects within the sormpi call
are purely the dden and dnum adjustment via ddn_sor(ip,dden,dnum). There is
negligible hit on the speed.

27 Jan 07

Planning and development of box handling for incorporation of surface
descriptions through box analysis.

boxedge iterates over the edges of a box in levels going out from the
	base node 000... 

gaussij is a modification of the numerical recipes gauss-jordan elimination
	to make it robust to singular matrices. 

Before incorporating these into the sor, it is probably best to implement
the c/a, b/a aspects of the code. Did that.

Now got a call to boxedge going after the cijs have been set. It seems
to be giving sensible results. For example the replacement of intersections
shows that when replacement of a real intersection takes place (more often
than is nice) it is because of rounding in late digits. 

Developed wireframe plotting diagnostics in cijplot to tell if I am
doing the right thing. Plotted the true intersections. They look fine,
although there are box recuts that I don't understand being
tried. Perhaps that is a sign that I need a better points-chosing
scheme.  This does not validate the rest of the fractions,
though. Need some inspiration on how to verify the other fractions in
a graphic way.

31 Jan 07

Changed additional fraction calculations to use all the points on levels 
up to that being examined, and svdsol to fit a plane to them. This gives
results the same within rounding as the gaussij when the number of points
is 3, and similar results otherwise. 

There is still an issue about recutting the cell. 
I still do not seem to have a clear verification that I have got the
additional fractions correct. 

An idea. The plane equation is \sum fn_i^-1 x_i = \sum a_i x_i= 1. 

For a sphere this ought to be a tangential plane (approximately). We
could test whether it is by evaluating the distance of the sphere
center from the plane, which is (\sum a_i.xc_i -1)/|a|, and seeing if
this is equal to the sphere radius.
 
Note that any box that has one of its fractions unset (=1.) should be
considered to have no plane crossing it. Thus each unset fraction 
rules out all boxes in that direction. If a node has three unset fractions
then there is only one of its boxes that is cut. There are some of these
among the Added. But there are others that have more than 3 fractions set.

3 Feb 07

There seem to be persistent erroneous boxes among the vast majority of 
correct boxes. I am pursuing the idea that these arise because of the
overloading of the fraction settings. If all three of the fractions
for a particular box happen to get set by the adjacent boxes, then this
box will erroneously be intepreted as having a plane intersecting it.

The only way out of this that I see is to use the flags to indicate whether
a box is in fact cut by a plane or not. There are 2**dims possible boxes.
Thus if we use all of the 32 bits of a 4-byte word, we can cope with 
2**5, i.e. 5-dimensions. 3-D requires only 8 bits. The intrinsic 
btest(n,ipos) yields true if bit pos of integer n is set. This extension
is in the mil-spec and available on practically all compilers. There is 
also an intrinsic ibset(n,ipos) and ibclr(n,ipos) (I think I got those right)
Their availability is less certain. They are not mentioned in current
gfortran documentation. 

After substantial additional work, I found that the main problem was that
the handling was in the wrong place in boxedge and hence incorrect.
Moving it to the proper end of level gives good results. 
If we use the test that we don't account for planes that fail to intersect
the cell, we get only about 40 additional pointers for the correct planes.
We get no recuts. We get no errors above .01. Also we find that only 
6-intersection cases are found. This is the effect of the cut criterion.
If we remove the cut criterion, we get many more additional points which
have 3,4, or 5 intersections.

I think we have the thing working now. Flags signal whether a box is
relevant or not. They are set only according to the boxedge call.

The next thing to do is to implement the field (gradient) interpolation.

6 Feb 07

Further cases with different radii shows there are still some problems
to sort out. I had a test that replaced the fraction only if b/a,c/a
are zero. If you have a zero potential setting. That is the case even
for previously found fractions. Then problems arise. Therefore, I ought
to reset the fraction in the box code only if it has not been already
set, i.e. not adjust it smaller in succeeding calls, as had been the 
previous algorithm. I think that algorithm was supposed to handle
cases where it had been set by previous box calls (not direct fractions). 
Now choose only to permit fraction setting if fraction =1., i.e. it has
not previously been set at all.


2 Jun 07
Converted the object code to depend on reading an input file in which
different types of object can be specified. Also coded the ability
to detect whether we are inside (or outside) such objects.

9 June 07
Got the gradient interpolation working with correction of the node about
which interpolation is done to use the fraction information to determine
when we need to use something other than the nearest node. 
Packaged this into gradinterpcorrect. So the call is packaged.

15 Jun 07

Added ipointer to the idob_sor data: a reverse pointer back to the
u/c arrays. Created routine:  indexexpand(ndims,ifull,index,ix)
to obtain the multi-D index ix, from the pointer index.
Then added routine iregioninit(ndims,ifull) to initialize the 
iregion flags for the idob_sor to the insideall value telling what
region they are in. It cycles over all the object data that is
created by the cijroutine, uses indexexpand.

Thus we now have ipoint_sor and iregion_sor set to sensible values
after calling cijroutine and iregioninit. Then at any interpolation,
we can access the region of any point that has object-boundary data.
A point that does not have such data is not adjacent to any boundary.
Therefore, it is safe to assume that it is in the same region as
a nearby point that we might wish to compare with. 

20 Jun 07
Attempting to compare the gradinterpregion call with the other, we find
that the iregion values are apparently incorrect. Misaligned by 1 in
all dimensions. This appears to be because when the cijroutine is called,
it is called by mditerate, and this is relative to the origin
(2,2,2) in the 3-d cij array. Consequently the reverse pointer is 
set incorrectly. Or, to put it another way, the reverse pointer is
relative to the (2,2,2) origin. Fix this in the reverse lookup during
the regioninit. Now gradinterpregion gives identical results.

22 Jun 07
Got getfield working. It gives the same value as the raw calls. Also
seems to give plausible looking interpolations.

23 Jun07
Constructed fieldtest which plots a field profile along a radius of the
sphere and a chosen angle. It shows there's a bug in getfield. 
Made get3simple to do totally simple box interpolation with no 
gradient extrapolation and neglect of boundaries. 
Found the bug in getfield logic. Incorrectly handling the construction
of the inputs to boxinterp for general dimensions. Temporarily use
the 3-D version from get3simple for iteration. That works and now things
agree away from the boundaries.

16 July 07

Getting back to this. The fieldtest code is close to working, but it
currently does not do interpolation correctly, because it produces
cases where iflags(1)=0, contradicting the assumptions of the
box2interp version. 

Extended box2interp to cope with f00 absent. Then tests show that the 
errors are reduced by a factor of 2 in the prior worst cases. So this
really helps (getting the interpolation right). 

18 July 07

Implemented an extrapolation scheme that is used for a box in which
more than two nodes are absent. If an absent node has precisely one
(of two) neighbors present then extrapolate through the present node
using the value one step further away to the desired (absent) node.
Otherwise do nothing because if only one node is absent (i.e. a node
has two present neighbors) we are already doing the sensible extrapolation.

This code has the effect of reducing errors in E-field by about a
factor of 2 when it kicks in. But there are fairly significant errors
in which it does not. Generally we find now that the field error is
less than about 10% for the 16x16x20 mesh near the sphere
boundary. Since the boundary is at r=0.2, which defines the
scale-length of the phi-solution there, and the node spacing is 1/16 or 1/20.
which is 0.05 (optimistically), the ratio of node spacing to characteristic 
scale-length is 0.25. Therefore if we get 10% errors we aren't doing
badly. Going to twice the mesh numbers (half the spacing) the errors drop to 
about 2-3% max. Which seems to be falling like (s/l)^2 (or at least faster
than linearly). There was a strange result with 
 fieldtest -p -p1 -t2.4
some kind of error inside the sphere but not too bad.

Looks like the difference between using extrapolation or not is still only
about a factor of 2 at smaller s. The extrapolation does not change the 
order of interpolation (as expected). That order is presumably that we
are correct to first order, since we are doing linear E-interpolation.
The error being order (s/l)^2 thus makes sense.


23 Aug 07

Started padvnc.f for advancing.
Updated getfield to use general number of dimensions.
Removed extrapolation from it, but saved old version.
Removed passing of iLsc, just multiply iLs by (2*ndims+1).


24 Aug 07

Further cleaning and confirmation that padvnc is working.
Changed getfield so that it is permissible to pass the full mesh
position, not just the fractional mesh position. This gives a version
that can either be used by passing the local origin of arrays, or
be used with the global origin and full mesh position.

There is a puzzle about the following. It seem to be using incorrect 
offset (ought to be (ix-1)*iLs ) but it works as is.

      do id=1,ndims_mesh
c Offset to start of dimension-id-position array.
         ioff=ixnp(id)
c xn is the position array for each dimension arranged linearly.
c Find the index of xprime in the array xn:
         ix=interp(xn(ioff+1),ixnp(id+1)-ioff,x_part(id,i),xm)
         xfrac(id)=xm-ix
         x_part(ndimsx2+id,i)=xm
         ixp(id)=ix
         iu=iu+ix*iLs(id)
      enddo

Interp returns a value >=1. But if we have ix=1, that is the first 
value in the array, so the offset should be zero and iu zero.
In the interpolate.f file (ix-1) is used, correctly.
In padvnc.f it is not. The getfield calculation in fieldtest appears
to treat things correctly, because it uses u(ix,ix,ix) explicitly.

I think this is bound up with the fact that I've assumed that we can
treat the shifted/fractional and full position cases the same, but
perhaps we can't since the fractional treatment passes 0.5 but the
full position value at 0.5 is 1.5 reading from a lowest index of 1.

Actually I think the previous result was wrong and the present one is
right. Played around with the padvnc printing out the force and comparing
with analytic. The corrected results are within about 2%, but the orbit
closure stinks.

Yes this is the correct alignment. The orbit closure improves lots
as one goes to finer mesh. This agrees quite well with analytic approx.
Fixed some little problems. I think we can declare padvnc working 
using (ix-1) everywhere.

25 Aug 07

Completed the initial chargetomesh assignment code.

Thinking about how to handle changes to the boundary potential between
steps, which would be needed for floating cases. Introduced a new
iinter flag in dob_sor to indicate which object was intersected. (Not 
yet populated.)

There seem to be three levels of cij update that would make sense, in
increasing level of computational cost.

1. Directly scale the C/A and ibdy components by the changed potential
for a specific object whose potential is varying. Assumes that objects
are not moving and that the changing BC is fixed potential. And that
each node that intersects the changing object intersects no others.
Involves searching the existing dob_sor data.

2. Rerun cijroutine for those nodes which intersect a changed object.
Assumes objects not moving, but would work for arbitrary changes to 
BC(s). Just operate on the existing dob_sor data.

3. Do 2, but in addition, search nodes in the neighborhood of a moving
object to determine nodes that become boundary which weren't before
(and presumably those that stop being boundary but were before). 
This requires substantially more effort to be sure you don't miss new
nodes. But is still short of re-searching the entire mesh, which would
be very costly.

A further possibility would be to limit the intersection investigation to
only the object(s) that have changed. This might be a significant time
saving. In other words, ignore the other objects in calculating intersections.

Implemented writing of the object number into iinter_sor. Test is
that we use cijplot to plot wireframe with color equal to the number.
Works.

27 Aug 07

Looking into MPI id's and communication, with a view being able 
conveniently to call the setup before starting the sormpi solution.
This enables me to know what my process is etc.
There are currently two separate initializations. 

1. bbdydefine just sets the iorig vector which tells the block sizes
for the specified arrays and process arrangement

2. first call to bbdy sets up the communications and initializes MPI.

It hardly seems necessary to have these separate calls.
There's an issue about how iorig is saved. Currently it is only
saved during the operation of sormpi (lost on return). 
Certainly there's no need to do what is currently done which is
to call bbdydefine anew each time sormpi is called. It really ought
only to be needed the first time.

There is a general save in bbdy which is where most things are saved
(although apparently not iorig because it's an argument). I see no
real reason why iorig should not be saved. Indeed there does not seem
to be any reason why it normally should be accessed outside of 
bbdy, so it might simply be defined in a common (in case access is needed)
within bbdy, and not passed to it. Then bbdydefine would be called by
bbdy itself, not separately. If we keep the arguments to bbdy the same,
then iorig has to be saved in sormpi. That's a problem because if we
call bbdy outside of sormpi, and then think we've done setup, it won't
then be given to sormpi for subsequent storage, unless we reinitialize,
which would be a bad thing.

Better to delete iorig from the bbdy arguments. Places:
mpibbdy.f mpibbdytest.f sormpi.f bbdydecl.f
However, we also then need to add ifull to the bbdy arguments
otherwise the initialization doesn't know what they are.

Done all that. mpibbdytest works (2 processes). fieldtest works.
Fieldtest had no modifications to it to handle this change. 
The whole interface is unchanged in respect of anything outside sormpi.

Added the code for special case kc=-2 purely (re)initialization.
A prior call with kc=-2 will set up the communicator but not try
to actually do any communication.

This all works, but there's still a problem with calling the bbdy
directly, that is that all its (other) arguments don't end up in the
sormpi places, which is where they are needed. Probably the best
thing is to make the bbdy call through sormpi somehow. The integer
switch ictl is the right place to do this. 

Implemented that using the 3rd bit of ictl.

Found a big problem with the initial call causing sorrelaxgen errors
and segfaults. Traced one issue to new usage of iLs which is not 
correct now because in bbdy bbdydecl of integer iLs(ndims+1) has ndims
as a parameter but iLs not passed. That's ok in bbdytest, but not in
sormpi. The error is not detected by the compiler and then shows
up later only when the save is required. In general there's an issue
with using bbdydecl in sormpi because a number of the bbdydecl parameters
are not passed in to sormpi. Therefore they are really local definitions
and can't be sized by anything other than a parameter. 

Fixed that for now with a hack to make it a parameter. Annoying compiler
bug!

28 Aug 07

Initialization of 32x32x40 now takes about 4 seconds.  When thinking
about the initialization, and possible reinitialization if there are
boundary changes, it is clear that for a multiprocessing environment,
we ought to make each processor do its own cij initialization.  This
potentially makes a big time saving.

However, the way things are currently set up, the cartesian layout
is hidden from the main routine, but the main routine calls mditerate
of cijroutine. Thus the information is not available to divide the 
work between different processors.

There's another issue which is that cijroutine uses an indexed array
of object data. As it generates the array, the presumption is that
there is a 1-1 correspondence. If the cijroutine work were divided
between processors, then the would be a 1-many correspondence. The
object data corresponding to a particular pointer would refer to
different mesh- locations in different processors. That would have a
possible benefit in that the object data storage would be
distributed. But it would cause a problem in that we could not simply
gather the data together.  That is a MAJOR problem because the
interpolation routines currently assume that there's a cij with
corresponding pointers covering the whole of the volume. It would
perhaps be possible to segment the object data by offsetting the
pointer for each processor so as to keep the data separate. Then we
could gather the cij back to central location.  Unless we did this, I
shudder to think how we'd proceed. The cost is the segmentation of the 
object data storage, which might make it less efficient. The benefit
is presumably speed of recalculating cij.

Basically we need access to the iorig information in order to do much
with other processors, such as mditerate or iregioninit. Actually that
can be accessed through iorigcom. We might also need information about
our place in the cartesian communicator; this would be in the form of
iobindex or myorig. 

Looking into the partreduce and other mpi code. There's some awkward 
shuffling in sceptic. This is not necessary because using the argument
MPI_IN_PLACE instead of the send buffer causes in-place reduction, 
which avoids all the problems of shuffling. Much more elegant.

Implemented a basic mditerarg of psumtoq to calculate the charge
density. But we still need a way to calculate rhoinfinity which is passed.

Put a new solve of sormpi in the stepping loop. It shows a blip of
charge giving rise to a potential peak. (And also causes the particle
to veer off in a different direction. Probably that is some sort of
image-charge effect). 

On a single processor, a 32x32x40 solve of about 100 iterations takes ~1sec.
Not bad, although the resolution is not that great yet.

Working on horace. Found several compatibility problems. (gfortran)
1. Makefile does not use implied patterns correctly. Seems to be something
   I don't understand about the match-anything rule.
2. I made incorrect assumption about o implying integer. Gfortran detects
   that error and one with pwr2nd, because of checking for real arguments.
   Fixed.

Succeeded in making. Got some complaints about blanket saves. Seems to be
caused by saves in mpif.h.
Also got diagnostics about unused argument variables.

29 Aug 07

Corrected the sign of field to make attraction of the particle to image
charges correct. Things seem fine with last night's changes.
Code will compile and run on unity. There it is back to g77 not gfortran,
I think.

Reinjection. We need to have some sort of approach on this. There needs
to be some reinjection boundary identified, and some way to determine
where on that boundary and with what velocity the particles are going
to be reinjected. 
A spherical boundary is basically the sceptic situation.
A rectangular boundary might also be of interest.
Probably these should simply be provided as subroutines.

Incorporated a somewhat modified version of the old reinject.f routine
from sceptic. Sphere. Seems to work but not quantitatively
verified. The advancing by a partial step is to be incorporated into
the padvnc code. 

Next thing: Loading particles. I'm not wonderfully happy with sceptic
on this. But I don't know if there's a better way. 

30 Aug 07
Implemented pinit. But there's some kind of bug that causes it to hang
in some situations. That was incorrect iregion_part value.

31 Aug 07
Idea about different particle species. We could use if_part to identify
different species 0: no particle 1: species 1, ... They padvnc could be
adjusted to advance each species differently, and reinjection could likewise
reinject different species. This would make it essentially trivial to
generalize the code to make it PIC electrons (for example) or multiple
ion species.

Branched to ccpic as main. Then began packaging the diagnostics so
that we can clean up the code, and use consistent mesh sizes etc.
I realize we need to worry about the wrong field we are getting at the
edge. This is an error that needs to be understood and fixed.

1 Sep 07
Trying to fix the edge incorrect field. First, try to set the iregion
flags in all the edge object data. This sort of works, but I find that
there's a problem with the fact that the reverse pointer is in various
places assumed to be addressed relative to (1,2,2,2). This was because
of the problems with mditerate. 

Therefore change the way that mditerate is called with cijroutine.
This needs us to fix the passing of the ipin to mditerate to do the 
right thing, and to calculate the right indi and save the indinp
in mditerate.

Then there are two places where the 2,2,2 assumption must be removed.
One is in 3dobjects.f iregioninit, called by objstart.
The other is in cijroutine itself. Both fixed.

Now the edge iregion is initialized with iregion as well as the
intersections.  This improves the field diagnostic plot a great deal,
leaving only a tiny region actually outside the outer object where the
field is incorrect. I still don't quite understand why that's there.
I guess there's no reason it shouldn't be, since the presumption is that
getfield returns only the field corresponding to the iregion specified.
Actually, no, I understand this, inside solu3plot the iregion is set
to be the insideall of the actual position being plotted (and without
that one gets rubbish). So we are telling getfield that we want the 
value as if one were in the region in question. Improved solu3plot
to show the region.

Trying out non-uniform mesh. There are big problems. The autocolorcont
is now incorrect. But more important, the getfield is plainly incorrect
and it looks as if there might be a bug in getting the spacing. At least
the solution looks plausible!

3 Sep 07

Fixed the contouring.
Fixed getfield. It was indexing the position array slightly wrong.
Now things seem to be correct. However, I think a numerically 
analytic orbit ought to be examined to prove that we are getting the
correct orbit.

4 Sep 07

Did an installation of an exact circular orbit. Seems to give rather
good accuracy after 100 steps with dt from .1 to 1. (0.7 to 7
turns). In the vicinity of 1-2 % accuracy in the r over that
evolution. About 5% deviations with 400 steps (28 revolutions), but
still only 1-2% for 400 steps 3 revolutions (dt=.1). Thus there's some
effective collisionality but it is not terribly great. Allows many tens
of revolutions. This all with 32x32x40 mesh r=1-5.

Tried 100,000 particles with dt=1. Goes at about 2 iterations per second.
200,000 goes at about 1s per step. No significant difference with dt=.1
This is without -ffortran-bounds-check.

Using profiling and gprof we get

 22.81      3.17     3.17 23890368     0.00     0.00  gradinterp_
 15.11      5.27     2.10 24594336     0.00     0.00  gradlocalregion_
 14.75      7.32     2.05  6148584     0.00     0.00  getfield_
 10.54      8.79     1.47  6763776     0.00     0.00  circlesect_
  6.73      9.72     0.94  6148584     0.00     0.00  interp_

Most of the time is being spent getting the field and in its interpolation.
If we put a short-cut into gradlocalregion to do the calculation directly
if the icp0 is zero (not a boundary), then we get:
 time   seconds   seconds    calls   s/call   s/call  name    
 31.21      4.38     4.38 24594336     0.00     0.00  gradlocalregion_
 16.33      6.67     2.29  6148584     0.00     0.00  getfield_
 12.02      8.35     1.69  6763776     0.00     0.00  circlesect_
  7.49      9.40     1.05  8993020     0.00     0.00  inside_geom__
  6.78     10.35     0.95  6148584     0.00     0.00  interp_
  6.06     11.20     0.85      314     0.00     0.00  sorrelaxgen_
  4.49     11.83     0.63  3739985     0.00     0.00  gradinterp_

There's not much savings of time. The shortcut has added about 16%
(2.3s) to gradlocalregion, which came out of gradinterp. There's no
substantial total savings (<3%). This shows that it is really the
calculations that are costing us:

         uprime= (2.*x+dx0)/(dx0+dx1) * (up-u0)/dx1
     $        +(dx1-2.*x)/(dx0+dx1) * (u0-um)/dx0

If we reorganize that expression to
         uprime= ((2.*x+dx0) * (up-u0)/dx1
     $        +(dx1-2.*x) * (u0-um)/dx0)/(dx0+dx1)
We get
 time   seconds   seconds    calls   s/call   s/call  name    
 30.70      3.91     3.91 24594336     0.00     0.00  gradlocalregion_
 16.20      5.97     2.06  6148584     0.00     0.00  getfield_
  9.79      7.21     1.25  6763776     0.00     0.00  circlesect_
  8.22      8.26     1.05  8993020     0.00     0.00  inside_geom__
  7.47      9.21     0.95  6148584     0.00     0.00  interp_
  6.29     10.01     0.80      314     0.00     0.00  sorrelaxgen_
  3.85     10.50     0.49  3739985     0.00     0.00  gradinterp_
A small saving.
Similarly, we get a small saving from using icp0=cij(1) instead of cij(ix).
Reorganizing test gets time to 3.74s. Another small saving. But we have
not done much. Probably only a 10% saving.

The profiling conclusion is that the main cost is the parabolic
interpolation of the field-gradient, which is called four times for
each dimension for each particle at each step. By fiddling around 
inside that (gradlocalregion) implementing two conditional short-cuts,
I shaved about 30% off the routine for uniform scaling. Everything else
seems pretty marginal, and is not worth effort at this time.

time   seconds   seconds    calls   s/call   s/call  name    
 23.07      2.85     2.85 24594336     0.00     0.00  gradlocalregion_
 17.28      4.98     2.13  6148584     0.00     0.00  getfield_
 10.83      6.31     1.34  6763776     0.00     0.00  circlesect_
  7.38      7.22     0.91      314     0.00     0.00  sorrelaxgen_
  7.22      8.11     0.89  6148584     0.00     0.00  interp_
  7.18      9.00     0.89  8993020     0.00     0.00  inside_geom__
  6.12      9.75     0.76  3739985     0.00     0.00  gradinterp_

5 Sep 07

Reorganized some files to clean up the main program and put plotting
and some other stuff elsewhere. 

Checked the cvs that you really can build from it. 

Now we need to get the volumes correct for calculating the charge
density.  This is trivial for the core nodes. Only difficulty is for
those on object boundaries. In cijroutine and boxedge we do some
elaborate calculation of fractions, which are supposed to represent
the points that define a plane approximation to the bounding surface
cutting a particular box, in the case that the plane actually cuts the
cell surrounding the box (which it can do even if fractions are >1).
However for the CIC particle assignment, a particle contributes
partially to a node if it is in the _box_ (not just in the node-cell,
which extends to the half-node-spacing position). Therefore, it is not
clear that I have done the relevant calculation. There will be some
degree of volume reduction for every node that lies in the center of a
2x2x2 box which is intersected by the boundary.

Things would be different for NGP assignment. Then particles contribute
only if they are in the node-cell, in which case the fractions might have
some value. 

In either case, there is a problem in that currently particles are 
removed based on the iregion. That is not a planar approximation. 
If volumes are calculated on a different basis from the particle removal,
the charge-density calculation will not be done correctly. It would
therefore be inconsistent to use direct iregion evaluations for particles
but fraction-approximations for volumes.

One alternative approach would be a Monte-Carlo integration of the
volume by multiple calls to insideall. If the weighting were uniform
(which it is not) then this amounts to a random choice of region in or
out. So n points are distributed according to a binomial. The average
of a sample therefore has a mean number: np (where p is the
probability of acceptance) and variance np(1-p), so that S.D./mean =
sqrt((1-p)/np), and the uncertainty as a fraction of the total box
volume is sqrt(p(1-p)/n)<=sqrt(1/n)/2.  Consequently, to get 1%
accuracy requires n=50^2=2500. Rather a lot of random points (3 ran0
calls per point). If we used a uniform mesh in each direction, with
this total number of points, we would have 2500^1/3= 14 per direction.
That would give a worse result in terms of uncertainty (probably). 
Monte-Carlo is better in dimensions greater than 2.


11 Sep 07

Started volintegrate and volnode for node volume calculations.
Transitioned active code to using mditerarg instead of mditerate.
However, haven't pruned all mditerate because the calls of mditerarg
are a bit hokey, since they use inconsistent number of arguments.
This should not matter, since the arguments are at the end. However
it probably violates fortran standards. So probably ought to think
about making the calls consistent.

Calls to volintegrate amount to 2152, according to gprof, which is a
major hit on initiation: about 8 seconds. This is for the 16x16x20
mesh and 10000 points for the montecarlo integration. Total pointers
used is 2800, but that includes the edge. As mesh size increases
the boundary points scales like n^2. So a 100x100 mesh would 
probably be about 6^2=36 times longer. One approach might be to 
do this work shared between the nodes. That would reduce it back to
about 8 seconds. Of the used time, about half is calls to inside_geom
which is the routine that is called for each object to determine if
the point is inside it. 

It might well be worth saving the volume data in a file, to be read
back in, in most cases. Done. File storedgeom.dat and seems to work.
There are numerous cross-checks to try to make sure this is valid
data for the case being run. 

12 Sep 07

Implemented a text plot of volume percentage. Seems to show things are
working ok (except for the external volumes). 

Increased the npoints parameter for monte-carlo to 10^5. This reduces the
random errors to <1% as expected. 

Made the volume for nodes external to active region 100%. But perhaps
the more appropriate thing would be to use their region for their
volume, or possibly a very large number so that the charge density
becomes negligible.

Discovered that if the network is in a certain type of unconnected state
then program takes for ever to initialize. This appears to be an MPI
problem.

How accurate does the volume have to be?

Well, the accuracy of the charge density is the real question. But
that has statistical fluctuations at a fractional level of 1/sqrt(N),
where N is the number of particles per cell. Thus, from a purely
statistical viewpoint, all we really need is to calculate the volume
from a monte-carlo sample of points much larger than the number of
particles in the cell. If we have a million cells (the upper range of
what might currently be managable) then 7 million particles would give
only 7 per cell.  That would make the use of 10000 points per cell
overkill.  By contrast, a 20x20x20 mesh has 8000 cells, and so we
would have a maximum of about 1000 particles per cell for this very
small mesh. Therefore, 10000 points is certainly enough to make the
statistical noise from volume calculation less than that from
particles.

There's something of a compensating factor: the particle number might
be averaged over many different realizations, while the volume remains
fixed and does not improve with averaging. Consequently, one might 
find that such quantities as the flux distribution, when averaged over
many steps, might show significant effects from cell volume errors.

14 Sep 07

Implemented a rhoinfcalc based on smax flux. Run with 1000000 particles
and zero sphere potential, it gives about 4000 reinjections per step
when it should give about 2400 or so. Thus the density inside is 
considerably less than rhoinf. Played around with this. Eventually found
that the problem is we don't have electrons turned on and so this gives
a strongly positive potential even with zero on the sphere.

We need to introduce debyelen and set it to some sensible value.
The solution of L\phi = q means that we either have to put the debyelen
scaling into the difference stencil or into q. That is we either have
to consider the equation to be \lambda^2 L\phi =\rho, 
or L\phi = \rho/\lambda^2.
The latter means that the "density" is stored as density/lambda^2.
The former means that the cij coefficients are scaled by \lambda^2.
From a computational viewpoint either is probably fine.
Actually I notice that the sceptic solver adjusts the overrelaxation
according as lambda is large or small. Probably this is not necessary
for ccpic, but might speed up things. In sceptic, I multiplied the 
poisson coefficients by debyelen^2. Perhaps that's what I should do here.
It's slightly awkward because dpm is passed up from the geometry routines.
I don't think the debyelen should be put into them. 

So put it into cijroutine. Then we can get small potential by making
the debyelen very long (with zero potential on sphere). Then we get
correct numbers of reinjections and sensible rho approximately one.
The sor convergence is a bit slower like this. We are committed to 
using mditerarg by this choice. Found that there's a problem, namely
that when setting derivative, there also needs to be a debylen 
factor in this approach. Seems to be fixed by scaling b as well.

Now we have a segfault from the -gt case. OK fixed that it was the 
adjustment I had made to slice3plot. But we still don't have the 
interpolation correct now. Ok one needed to fix another place in
cijroutine.

Need now to turn on the electrons. Doing this things are very broken
for few particles, but seem to work for many. Also there's a problem
outside the active region, where faddu is being added, though it 
shouldn't be. It might be possible to put the q equal to 1 in the 
outside region. That would cancel out the electron function.
But it looks like being a major bother.

18 Sep 07
Actually it is not a problem because volumes is set to a very large 
number outside the active region. This is easily used in psumtoq to
set q=1. Seems to work.

20 Sep 07

Now we'd like to get the code going with constant number of particles being
injected per unit time. This might be a more satisfactory solution than
calculating rhoinfinity each step based on the number that happens to be
reinjected. 

Done that. The total number of particles shows a decay over about 100 steps
of .1, by about 10%. This is presumably the readjustment of the initially
uniform density to reflect the proper final spatial distribution. 
Now we have the choice of specifying -ni the number of ions or specifying
-ri the rhoinfinity (density of ions at infinity). May not have the 
multiprocess numbers set correctly yet.

I wonder if there's a way to speed up the convergence to the final density
other than what we've always used in sceptic, which is large initial steps.
Once we vary the dt, we will need to vary the ninjcomp accordingly. 

Observed that there's an error in u inside the sphere. This arises because
the electron density is not being correctly compensated there. Changing
the setting of rho to be faddu(u) for an external region helps, but does
not fix this. The error mostly goes away if rho is set to zero. This is
a serious puzzle. Seems as if the solver is incorrect in the inner
sphere. It is possible there's an error in the algorithm for the case
when faddu is present. 

It's really bizarre, but putting the rho equal to minus faddu in the
inner region gives a perfectly flat profile. But in the outer region
you need +faddu. Explain that! Actually it seems to be an accident.
It does not work for other sphere potentials.

Found a bug in the faddu part of sorrelaxgen.f. Fixed it. But it's a
bit worrying for sceptic. It's ok. Sceptic uses a hard-wired 2-D sor,
not the sorrelaxgen. I think we now have a correct compensation for
electron density in the external regions.

Fixed some sliceplot bugs in rotation.

21 Sep 07

Things to do next maybe.

Reading and Writing the code state, fields and particles. Probably we
should keep the fields and particles in separate files since the fields
only need to be written by master, while each node must write its own
particles. How do we ensure that random numbers are the same thereafter
for a restart?

Tracking and saving particle exits (and entrances?). This depends to
some extent on the type of object. So we need a uniform way of
handling the documentation of flux distributions. This requires an api
for the object.

MPI communication of particle parameters. 

22 Sep 07

One thing slowing down my decisions is the problem of the overwhelming
amount of data. Even to save the fluxes at each step is going to take
a substantial amount if we do it for every object of 32. Of course we
will rarely or never want to do it for so many objects. Probably we
should not use a structured obj_flux(n_fluxmax,ngeomobjmax) data store
for the object storage, because this is going to be very inefficient.
Instead we should probably use a big storage space accessed by a dynamic
structure. E.g. only providing for the number of objects that actually
exists, or even the number for which we care about the flux. If that's
the case, then the main thing to specify is how that data is structured.
It could be structured the same in a binary output file.

27 Sep 07

Flux data might be structured as follows:

Level	Description					Name	No of values
0	Number of objects for which data is stored.	nof		1
0	Address of object header starts.		iof(nof)	nof

1	Number of quantities stored for this object.	nqf		1
1	Address of quantity starts.			iqf(nqf)	nqf

2	Number of positions for this quantity.		npf		1
2	Quantity descriptors/positions			qdf		npf

3	Timesteps							1

----- End of header----

3	Flux Data of quantity (nqf)					npf

When addressing a place to write flux data it is:

buff(time0+(time-1)*sum_{nof}(nqf*npf)+sum_{iof}(nqf*npf)+sum_{iqf}(npf)+ipf)

Thought again about using HDF5. However, this is really a library. There
does not seem to be any intention for it to be a data structure API for
simple programming. Therefore the only real reason for using HDF5 is to
make the data portable to someone else's calling of the HDF5 libraries.
That's not really what I am looking for. The principles of HDF5 do
embody some of what I am looking for: self-describing data strutures etc.

Perhaps we could reasonably choose the number of quantities to be fixed
so that we simply only need to specify the objects for which we are storing.
Then, the positions could be variable. 


29 Sep 07

Implemented structures in 3dcom.f and initializations in fluxdata.f.
Found that it was block data operating on big common blocks that caused
the very large executable. So removed that and put explicit initialization.

Implemented tallyexit in padvnc, with rudimentary particle flux counting.
It seems to work. Can subsequently obtain the fluxes.

Implement outputflux routine and corresponding readfluxfile routine to
write and read-back the flux data. 

Implement fluxdatatest to see that we really can read it back independently.

13 Dec 07

Over several days working on fedora8 have got rid of warnings. The
main actions are to turn off unused argument warnings and to edit
/usr/local/include/mpif.h to remove the save statements on the common blocks.
It ought not to be necessary to use save statements on them I think.
However, that is a peculiar usage that might be really needed, and I'll
have to watch out for it. In any case, that change did not change the
answers.

[Got extace compiled and working after installing some devel rpms for fftw
and other. Got rununfullread working.]

16 Feb O8
Fiddled a bit with the flux reading and writing.

12 Jun 09
Added documentation to 3dcom.f
Added additional possible dimensions to the position descriptor data,
because a single number might not describe the position sufficiently.
More dimensions (nf_posdim>1) does not affect the actual data averaging
since it is stored as a linear array. But the linear array might really
refer to two angles (or more parameters). 

Fix segfault bug with undefined cworka in orbit3plot.

13 Jun 09

Fixed some other little bugs.
Improved some accis routines to make the rotation of the slice plot work
more smoothly and fixed some bugs in it.

Now want to overplot the 3-D orbit on an existing perspective plot.
Issue is that a mesh has the third axis scale set to something to do
with the function amplitude rather than position. Need the ability to
rescale that axis, plot orbits, then scale back to prior. Also need
a poly3mark routine.

Instead, first modified the cijplot routine to enable choice of objects
to plot and overplot the orbits on that. This avoids any need to rescale
etc. Also made a polymark routine. 

2 July 09

Replaced the ran() function with a purely fortran one ran1 from NR. 
The reason is that we can't get at the internal state of any of the
decent built-in random generators, so we can't do proper restarts.
The time for ccpic -s20 with the old one is 
real    0m33.843s
user    0m16.358s
sys     0m0.378s
And with the new one it is
real    0m33.437s
user    0m16.306s
sys     0m0.304s
There is no detectable time difference for this single-processor case.

In partwriteread we save the random number state ranstate, and read it back
so that we can restart and get exactly what we would have got if we had
not saved the state.

At present, we do not have the mpi infrastructure for gathering particle
data from different processes or broadcasting back the field data. This
is eventually going to be needed.

Ok. Created writing of potential too. 
I think that ought to be enough to be able to restart. We have to have
identical parameters, which is currently assumed rather than written,
although iuds is stored with potential. 

Trying to get the restart going. I find that there is disagreement.
To test the rereading, I do an internal restart by jumping back into the
code after 5 steps. This gives identical results as not restarting.
This shows that everything (relevant) that is read back is being read
back correctly. However, the difference with a true restart means
that there's something in the restart that is not being set correctly 
by the reading process.

Tests based on ccpic, ccpic --restart, ccpic -s10,
with a sample psum, q, and random number show:

The psum and q difference does not start till the 8th iteration.
This is already remarkable, since it means there's no difference
for the first two iterations after restart. 

The psum/q differences start before there are any differences in the
random sample. This strongly suggests there's not a problem with the
random number generator restart.

I find that the u-differences start immediately after the first solve
of the restart. So that is where things are changing.

Probing inside of sormpi with output from individual iterations. This
shows that the differences start to show up on the second return from
sorrelaxgen0 after the restart:

 Restart files read successfully.
 nrein,n_part,ioc_part,rhoinf,dt=        1253       51915       52359   99.97294      0.1000000    
 psum sample   5.197589     u    sample -5.5481944E-02  ran1  0.8150202    
 Return from sorrelaxgen0           1 -2.9151652E-02  -2.000000      5.7159495E-02
 Return from sorrelaxgen0           2 -6.2908038E-02  -2.000000      7.9460606E-02
 Return from sorrelaxgen0           3 -3.9616335E-02  -2.000000      6.9320247E-02
 Return from sorrelaxgen0           4 -3.5938688E-02  -2.000000      6.5017469E-02


Return from sorrelaxgen0          56  2.6678543E-05  -2.000000      6.2136307E-02
 Return from sorrelaxgen0          57 -2.2355829E-05  -2.000000      5.4357897E-02
 Return from sorrelaxgen0          58 -1.8731193E-05  -2.000000      6.2133629E-02
 q    sample   1.502413     u    sample -6.0202956E-02
0006 iterations:  58 Total flux number   147.0000    
nrein,n_part,ioc_part,rhoinf,dt= 1253  51756  52359  99.973   0.100
 psum sample   4.499379     u    sample -6.0202956E-02  ran1  0.8549011    
 Return from sorrelaxgen0           1 -3.3089679E-02  -2.000000      5.8978312E-02
 Return from sorrelaxgen0           2  7.8395963E-02  -2.000000      6.1556634E-02
 Return from sorrelaxgen0           3  4.7830440E-02  -2.000000      6.8785459E-02

 q    sample   1.059864     u    sample -5.5481944E-02
0005 iterations:  58 Total flux number   141.0000    
nrein,n_part,ioc_part,rhoinf,dt= 1253  51915  52359  99.973   0.100
 psum sample   5.197589     u    sample -5.5481944E-02  ran1  0.8150202    
 Return from sorrelaxgen0           1 -2.9151652E-02  -2.000000      5.7159495E-02
 Return from sorrelaxgen0           2 -6.2949345E-02  -2.000000      7.9473235E-02
 Return from sorrelaxgen0           3 -3.9642353E-02  -2.000000      6.9339350E-02

Printing out in addition, relax, omega, xjac_sor shows difference in relax
occuring on the same return that there is a delta difference (but not omega).

 Restart files read successfully.
 nrein,n_part,ioc_part,rhoinf,dt=        1253       51915       52359   99.97294      0.1000000    
 psum sample   5.197589     u    sample -5.5481944E-02  ran1  0.8150202    
 Return from sorrelaxgen0           1 -2.9151652E-02  -2.000000      5.7159495E-02
   1.000000       1.000000      0.9957700    
 Return from sorrelaxgen0           2 -6.2908038E-02  -2.000000      7.9460606E-02
   1.890094       1.983257      0.9957700    
 Return from sorrelaxgen0           3 -3.9616335E-02  -2.000000      6.9320247E-02
   1.876438       1.967066      0.9957700    

 q    sample   1.059864     u    sample -5.5481944E-02
0005 iterations:  58 Total flux number   141.0000    
nrein,n_part,ioc_part,rhoinf,dt= 1253  51915  52359  99.973   0.100
 psum sample   5.197589     u    sample -5.5481944E-02  ran1  0.8150202    
 Return from sorrelaxgen0           1 -2.9151652E-02  -2.000000      5.7159495E-02
   1.000000       1.000000      0.9957700    
 Return from sorrelaxgen0           2 -6.2949345E-02  -2.000000      7.9473235E-02
   1.891335       1.983257      0.9957700    
 Return from sorrelaxgen0           3 -3.9642353E-02  -2.000000      6.9339350E-02
   1.876438       1.967066      0.9957700    

Yes, there seems to be a difference in oaddu. Perhaps this is arising from
the previous solution because there does not seem to be an initialization of
dden. Consequently the dden is what is left over from before. That will make
the prior test solution give a difference, since oaddu is 0 for it. This seems
to be the cause:

 Restart files read successfully.
 nrein,n_part,ioc_part,rhoinf,dt=        1253       51915       52359   99.97294      0.1000000    
 psum sample   5.197589     u    sample -5.5481944E-02  ran1  0.8150202    
 Return from sorrelaxgen0           1 -2.9151652E-02  -2.000000      5.7159495E-02
  7.3466018E-02   1.000000       1.000000      0.9957700    
 Return from sorrelaxgen0           2 -6.2908038E-02  -2.000000      7.9460606E-02
  7.2402194E-02   1.890094       1.983257      0.9957700    
 Return from sorrelaxgen0           3 -3.9616335E-02  -2.000000      6.9320247E-02
  7.2402194E-02   1.876438       1.967066      0.9957700    

 psum sample   5.197589     u    sample -5.5481944E-02  ran1  0.8150202    
 Return from sorrelaxgen0           1 -2.9151652E-02  -2.000000      5.7159495E-02
  7.2402194E-02   1.000000       1.000000      0.9957700    
 Return from sorrelaxgen0           2 -6.2949345E-02  -2.000000      7.9473235E-02
  7.2402194E-02   1.891335       1.983257      0.9957700    

Yes, changing to ensure that it is properly initialized fixes this:


 Restart files read successfully.
 nrein,n_part,ioc_part,rhoinf,dt=        1253       51915       52359   99.97294      0.1000000    
 psum sample   5.197598     u    sample -5.5441111E-02  ran1  0.8150202    
 Return from sorrelaxgen0           1 -2.9148662E-02  0.1710546       1.000000    
 Return from sorrelaxgen0           2 -5.9492927E-02  0.1805826       1.787417    
 Return from sorrelaxgen0           3 -3.5247445E-02  0.1702435       1.765143    


 q    sample   1.059865     u    sample -5.5441111E-02
0005 iterations:  43 Total flux number   141.0000    
nrein,n_part,ioc_part,rhoinf,dt= 1253  51915  52359  99.973   0.100
 psum sample   5.197598     u    sample -5.5441111E-02  ran1  0.8150202    
 Return from sorrelaxgen0           1 -2.9148662E-02  0.1710546       1.000000    
 Return from sorrelaxgen0           2 -5.9492927E-02  0.1805826       1.787417    
 Return from sorrelaxgen0           3 -3.5247445E-02  0.1702435       1.765143    

It also seems to lead to smaller number of iterations. Which is also good.

However, it has not solved the later divergences. They start after the 6th
iteration, which is completed without divergence. How can that be?
The difference first observed is in delta.

 Return from sorrelaxgen0          44 -1.9401210E-05  0.1755715       1.662254    
 q    sample   1.502416     u    sample -6.0158029E-02
0006 iterations:  44 Total flux number   147.0000    
nrein,n_part,ioc_part,rhoinf,dt= 1253  51756  52359  99.973   0.100
 psum sample   4.499377     u    sample -6.0158029E-02  ran1  0.8549011    
 Return from sorrelaxgen0           1 -3.3093132E-02  0.1704593       1.000000    
 Return from sorrelaxgen0           2  7.4114323E-02  0.1755701       1.787982    

 Return from sorrelaxgen0          44 -1.9401210E-05  0.1755715       1.662254    
 q    sample   1.502416     u    sample -6.0158029E-02
0006 iterations:  44 Total flux number   147.0000    
nrein,n_part,ioc_part,rhoinf,dt= 1253  51756  52359  99.973   0.100
 psum sample   4.499377     u    sample -6.0158029E-02  ran1  0.8549011    
 Return from sorrelaxgen0           1 -3.3690698E-02  0.1704593       1.000000    
 Return from sorrelaxgen0           2  6.2475450E-02  0.1755701       1.787982    
 Return from sorrelaxgen0           3  4.0570423E-02  0.1725774       1.769773    


 q    sample   1.502416     u    sample -6.0158029E-02
0006 iterations:  44 Total flux number   147.0000    
nrein,n_part,ioc_part,rhoinf,dt= 1253  51756  52359  99.973   0.100
 psum sample   4.499377     u    sample -6.0158029E-02  ran1  0.8549011    
 Calling sorrelaxgen   0.000000       0.000000       1.000000    
 Return from sorrelaxgen0           1 -3.3093132E-02  0.1704593       1.000000    
 Calling sorrelaxgen   0.000000       0.000000       1.787982    
 Return from sorrelaxgen0           2  7.4114323E-02  0.1755701       1.787982    

 q    sample   1.502416     u    sample -6.0158029E-02
0006 iterations:  44 Total flux number   147.0000    
nrein,n_part,ioc_part,rhoinf,dt= 1253  51756  52359  99.973   0.100
 psum sample   4.499377     u    sample -6.0158029E-02  ran1  0.8549011    
 Calling sorrelaxgen   0.000000       0.000000       1.000000    
 Return from sorrelaxgen0           1 -3.3690698E-02  0.1704593       1.000000    
 Calling sorrelaxgen   0.000000       0.000000       1.787982    
 Return from sorrelaxgen0           2  6.2475450E-02  0.1755701       1.787982    


I think it pretty much has to be an internal state of sorelaxgen. Because
all the parameters are set to be the same going into it. Unless something
very bad has happened with u or q.

3 Jul 09

It is the seventh iteration first call to sorrelaxgen that goes awry.
Perhaps the red-black iteration is the problem? It is controlled by the
value of k_sor which is the same. Hard to see it. We need a better way
of detecting the difference.


 psum sample   279.4092     u    sample   0.000000      ran1  0.2056652    
 Calling sorrelaxgen   0.000000       0.000000       1.000000    
 Return from sorrelaxgen0           1 -7.8246323E-03   2.635117       1.000000    
 Calling sorrelaxgen   0.000000       0.000000       1.070949    
 Return from sorrelaxgen0           2 -8.1036519E-03   2.636601       1.070949    
 q    sample   1.000000     u    sample   0.000000    
0005 iterations:  14 Total flux number   51.00000    
nrein,n_part,ioc_part,rhoinf,dt= 1253  51906  52359  99.973   0.100

Created a checkdelta routine inside sorrelaxgen to read an old file of
deltas and check the current value against it. The files have to be swapped
after the run: mv checknew checkdelta.

I find that even subsequent identical runs do not agree. There are
small differences in the low order bits of the delta. This seems to show
either that there are truly random bit-level errors or that there's
something wrong with the initialization that prevents exact repetition.
The differences arise even in the initial test solution before any
particles have been added. Strangely, there's zero difference in the particle
solutions etc. So I don't understand how there can be delta differences.

Called ran1(-1) earlier to make sure the volumes are the same on
different machines. Found it gave a segfault. IDUM was being set
incorrectly in ran1. Stopped that. Also made the subsequent setting
call -myid-1 to ensure that it's really negative.

With optimization turned off we are getting bigger differences in the 
first solve. And subsequently smaller and smaller differences. I'm not
sure whether that's significant. Still the difference early in the
first solution are not small, and in general the first iteration is
showing it. These are not rounding errors. They appear to be random
initialization differences.

Found that I was not initializing the u to zero (or anything). This
was apparently the cause of the differences. Fixing it got rid of the 
delta differences. But that has not fixed the restart. 

Tried checkuqcij on some different step numbers. Found that step 6 is fine
but that going into the sormpi on step 7 there are q differences (very slight).
How can that be? 

After long struggles with passing errors and segfaults, got testing of
uqcijpsumvolumes going. It shows that after a restart the psum is different,
pretty much everywhere by a moderate amount. The qs are different too, but
not on the boundary (where presumably they are not set). Again this is the
_second_ step after the restart. In other words the first step has caused
the psums to start to differ. They did not differ immediately after
the restart. This check is going into (right before the sormpi call).
So we can't tell if this difference is coming from sormpi. So put the call
right _after_. The answer is that the first uqcij checks fine. Thus the
first sormpi call is working correctly. Consequently it must be the first
particle move that is giving errors. However, putting after padvnc does
not show differences till the second test. Putting after the chargetomesh
shows differences on the second cycle. This is consistent with the particle
move giving differences that are transferred to psum at the first subsequent
chargetomesh. 

We rely on the partlocate in chargetomesh to keep current the mesh position.
Although we save it and restore it, it is the updated by chargetomesh. 

Printed out a sample of the x_part values immediately after the (first)
advance. There's no difference. That's a puzzle:

 Calling sorrelaxgen   0.000000       0.000000       1.072827    
 Return from sorrelaxgen           2 -9.6360259E-03   2.613914       1.072827    
 q    sample   1.000000     u    sample   0.000000    
0006  15 iterations. x_part sample   2.396352     -0.6855422      -3.690475      -1.057408      -1.120257     -0.6776624       4.751034       3.213245       1.688664     -0.4781322       2.480651       2.000758     -0.8974879      0.2682267      0.2117976       3.305810       4.726902       4.489779    
 Total flux number   42.00000    
nrein,n_part,ioc_part,rhoinf,dt= 1253  51802  52359  99.973   0.100
 ***** psum difference           2           1           1  7.6883718E-02  7.7840298E-02
 ***** psum difference           3           1           1   1.262272       1.280330    
 ***** psum difference           4           1           1   1.013222       1.011209    
 *

 Calling sorrelaxgen   0.000000       0.000000       1.072827    
 Return from sorrelaxgen           2 -9.6360259E-03   2.613914       1.072827    
 q    sample   1.000000     u    sample   0.000000    
0006  15 iterations. x_part sample   2.396352     -0.6855422      -3.690475      -1.057408      -1.120257     -0.6776624       4.751034       3.213245       1.688664     -0.4781322       2.480651       2.000758     -0.8974879      0.2682267      0.2117976       3.305810       4.726902       4.489779    
 Total flux number   42.00000    
nrein,n_part,ioc_part,rhoinf,dt= 1253  51802  52359  99.973   0.100
 ***** psum difference           2           1           1  7.7840298E-02  7.6883718E-02
 ***** psum difference           3           1           1   1.280330       1.262272    

They start to deviate very slightly the next step. But in reality if the 
positions of the particles are the cause of the problem, I don't see how they
can be exactly correct after the first push. 

Implement xpartcheck. I find that there are differences in x_part during
the second iteration, before chargetomesh. That is, I confirm differences
after the first particle push, for particle 29 47 54. The velocity differences
are large. Order unity. The position differences are small. Seems to imply
that the acceleration is where there's an error. There's nothing obviously
systematic about the particles with differences. 

Did check with uqcij right before padvnc and xpart right after. The xpart
diffs are present, not uqcij. Thus we've proved there are advancing differences
when there are no uqcij differences. Since no if_part differences appear,
that's clean. The 6-9 x_part values, which are the mesh fractions, are not
in error the first time. But by the third time, they are. Thus, we are 
not calling partlocate. These are not newly injected particles.

This is tough. Construct a diagnostic version of padvnc. It reports the 
value of i and field for the first 100 particles. I see that the particles
with differences are reported twice. That is they are going through
the routine a second time. This is because inewregion is being returned
as 3.

ccpic -s8
          28 -1.6265622E-03 -9.0324413E-03 -3.8206729E-03           2           2
          29 -1.8644669E-03 -6.8034087E-03  2.6758902E-03           2           3
          29 -1.5171642E-03  4.0289122E-03  2.4152363E-03           2           2
          30 -8.1677195E-03 -2.4931235E-03 -1.4990305E-04           2           2

ccpic --restart
          28 -1.6265622E-03 -9.0324413E-03 -3.8206729E-03           2           2
          29 -1.8644669E-03 -6.8034087E-03  2.6758902E-03           2           3
          29 -1.5171642E-03  4.0289122E-03  2.4152363E-03           2           2
          30 -8.1677195E-03 -2.4931235E-03 -1.4990305E-04           2           2

It appears that the field is actually being evaluated the same both times.

-s8
          29 -1.8644669E-03 -6.8034087E-03  2.6758902E-03           2           3
 Reinjected          29  0.5868658      -4.671297       1.682282      0.2139225       1.030760      -1.000355       3.793430       1.164375       4.341133      2.3144661E-02   0.000000             110           3           1           4  0.7934299      0.1643748      0.3411326               2
          29 -1.5171642E-03  4.0289122E-03  2.4152363E-03           2           2

--restart
          29 -1.8644669E-03 -6.8034087E-03  2.6758902E-03           2           3
 Reinjected          29  0.5868658      -4.671297       1.682282      -1.054011       1.133722     -0.2721342       3.793430       1.164375       4.341133      2.3144661E-02   0.000000             110           3           1           4  0.7934299      0.1643748      0.3411326               2
          29 -1.5171642E-03  4.0289122E-03  2.4152363E-03           2           2

The differences are in the 4-6: velocities reinject is giving the particle
different velocities, although the same positions. The velocities are got 
from gasdev.

GASDEV in randf.f was calling ran0 not ran1. Thus it was giving different
velocities. This problem arose because I did a grep on ran0 not on RAN0.
Argh! So I lower-cased the routine names there to prevent future problems.

Fixing that, I seem to get the right answer. No I am getting x_part 
differences for particle 2 (only). 

restart:
0006  15 iterations. ====== Finished uqcijckeck
           1 -1.2301615E-02 -9.4538052E-03  3.2518448E-03           2           2
           2  1.0301614E-02 -2.0927475E-03 -2.7766897E-04           2           0
 Reinjected           2  -1.689946      0.6363239      -4.662046     -0.2221324      -2.588727      0.2661515       2.655035       3.818159       1.169000    
           2 -2.2906796E-03 -2.6510621E-03  2.6147573E-03           2           2


s8
0006  15 iterations. ====== Finished uqcijckeck
           1 -1.2301615E-02 -9.4538052E-03  3.2518448E-03           2           2
           2  1.0301614E-02 -2.0927475E-03 -2.7766897E-04           2           0
 Reinjected           2  -1.689946      0.6363239      -4.662046      -1.117069      -2.251752      0.6365508       2.655035       3.818159       1.169000    
           2 -2.2906796E-03 -2.6510621E-03  2.6147573E-03           2           2

There's another problem. gasdev has internal state information so the first
calls to it are broken. Make the internal state of gasdev available in 
ran1com and include it in the save and restore. Yes. Now we are getting
identical results.

Summary of Restart Facility and Experience.  4 Jul 09
------------------------------------------- 

Code now restarts and gives exact numerical agreement with runs that
simply had more steps in the first place. The key problems were
associated with random number generators. 

I found that dden was not being initialized properly in
sormpi. Correcting that also reduced the number of iterations. [I
think that Leonardo noticed a problem that might be this.] Another
problem found was that u was not being initialized to zero.

A day-long search tracked down the fact that (1) gasdev was calling
the old ran0, not ran1, and also (2) needed its internal state saved.
With that fixed we have exact agreement.


Had a problem with the cvs commit. Wireless broke in the middle. Hope this
is going to work. But it does not seem to do so. 


7 Jul 09 Restart is still not right. 

We get Getfield no good vertices errors. This is because all fractions
are zero. We ought not to be using these slots somehow.  I think the
reason is that pinit initializes all slots up to 52359 but when run
for long enough, ioc_part drops below this. (That did not happen for
my short test ./check) Consequently when we read back, we don't write
over the slots that have been reinitialized, and so the code thinks it
is ok to use them but it's not.  The fractions are not set in
pinit. Perhaps they should be. But perhaps this is fortuitous because
it has shown me an error that might not otherwise have been obvious.

In partread zeroing the flag of slots higher than ioc_part stops errors.

8 Jul 09

Multiprocess calls don't work. On TP400.
There, I've installed the latest MPICH2 from source. Things then compile.
I've fiddled inside bbdy but that's not the cause (I'm pretty sure).
Yes verified that by getting back the cvs mpibbdy and doing a 2 process
run, which crashes.

         call MPI_BARRIER(MPI_COMM_WORLD,ierrmpi)
         stop
helps to localize where the crash occurs. If it happens before this,
then you don't get much output. If not, then you get all output up
to this. 

The crash appears to occur on the second call to sormpi. This is the first
time that communication is occuring. I've heard that mpd does not work
when the hosts resolve to 127.0.0.1, which is the case for me now.
Tried some other mpd.hosts lists, but they give "failed to handshake".
Can't even get the tp41 working. 

Turns out the result of $hostname must be the same, on either machine.
It does not crash on tp41 with -n 2 and blks=2. So we are looking for 
something tp400 dependent.
Checked out the cvs to tp41. It runs with two processes. Yep it's tp400.

Got the alltoallw.f test program. Compiled. It seems to work. Does not
crash. Yes, it makes sense (after some puzzling). Therefore we seem
not to have an inherent communication problem. It's really a problem
inside sormpi. And in fact the mpd stuff all works with the hostname
set to tp400-64.

SCP the new mpibbdy.f to tp41 make. It runs two processes correctly,
with no relevant code differences to here. It is the tp400. 

The code mpibbdytest.f is also broken. This is probably easier to
debug. I can see that communications are correct till the gather. 
Then crash.

9 Jul 09
Figured out the problem finally by doing some tests on Loki.

The problem lies in the routine MPI_TYPE_CREATE_HVECTOR or the older version
MPI_TYPE_HVECTOR. In many man and web pages MPI_TYPE_HVECTOR is said to be
obsolete and to be replaced with MPI_TYPE_CREATE_HVECTOR. See for example
http://linux.die.net/man/3/mpi_type_hvector and
http://linux.die.net/man/3/mpi_type_create_hvector

However, these calls are not identical. In particular the argument STRIDE is
simply INTEGER in MPI_TYPE_HVECTOR but in MPI_TYPE_CREATE_HVECTOR it is:

INTEGER(KIND=MPI_ADDRESS_KIND) STRIDE

I was not declaring STRIDE explicitly (I called it iSTRIDE). Therefore it was an
INTEGER. On a 32 bit machine, INTEGER is 4 bytes long, the same as
MPI_ADDRESS_KIND. On a 64 bit machine (in linux) an INTEGER is still 4 bytes
long, but MPI_ADDRESS_KIND is 8 bytes, and so CREATE code breaks.

I discovered this by accident because on Loki there is no
MPI_TYPE_CREATE_HVECTOR, for reasons I still don't understand. So we replaced
the call with MPI_TYPE_HVECTOR there. When I did the same on my 64 bit laptop,
my code worked, after which the explanation above became clear. Especially
helpful was the page

http://publib.boulder.ibm.com/infocenter/clresctr/vxrx/index.jsp?topic=/com.ibm.
cluster.pe510.mpisub.doc/am107_itchvec.html

It turns out that gfortran recognizes the above KIND construct (not F77). So I
can obey the advice to use MPI_TYPE_CREATE_HVECTOR. Alternatively I can NOT
define iSTRIDE and use MPI_TYPE_HVECTOR. Either works with the correct
declaration.

Tried replacing the sceptic mpibbdy.f with the new fixed one. It gives
identical results. But perhaps I am not using the right switches to 
exercise it.

Probable next steps. Communication of the particle data through MPI.
Most of what else I had previously planned is done.

12 July

Starting on particle gathering. We need a block structure for exchanging
all the data for the whole used array, for example for psum or q.

It makes sense to use the bbdyblockcreate routines to do this. 

First change the iside(imds,2) to iside(2,imds) so that we don't have
to have imds everywhere, rather we can pass ndims, and use that for the
second (but not first) dimension. iside occurs only in mpibbdy.f, as
does bbdyblock. So it appears that's the only file we need to fix.
Now imds is used only in the top level bbdy routine. Not in the block
creation routines.

These and other complications arise because the existing code has
built in two types of block length, bulk and top. Also tuned up the
resetidim code to choose better match to prescribed nprocesses.

Packaged the construction steps for the alltoallw types into routine:
bbdygatherparam. This helps to contain the issues. 
It seems to show that we really only need a call to bbdyblockcreate.
Then the bulk handle for dimension n is at ktype(2**n).
If we put iside(2,id)=0, then we will get a negative count, because
the pass does not include the boundaries, even though the length appears to.
Thus one should presumably pass q(2,2,2) but with iside equal to iuds.
This is confirmed in that iside is calculated from the difference
in origins but with 2 added. 

There's another issue to do with chargetomesh. It puts the charge onto
adjacent nodes without consideration of boundaries. That's not really right.
Checking inside volint, I currently set volumes taking account of the 
mesh boundaries. Vol is set to a large number outside region which will
divide the density down to zero. Vol is set to a corrected number for
a node with intersections. Seems as if this ought to work. So perhaps theres
no problem. If so then we just need the reduce with the correct structure.
There might be a problem with gradient at the boundary. This would mean
the BC is effectively not second order accurate. 

13 July 09

I discover that I have misunderstood the REDUCE capabilities. In order
to operate on a customized MPI datatype, one has to write a customized
operator. So even though I can create a datatype that refers to the 
block iuds(ndims). I can't reduce it unless I have the operator. 
Operator has to be of the form
	 user_function(invec,inoutvec,len,type)

In my case I'd be passing only one dataobject. So I'd have to know
internal to the function, the structure of the calculation required,
e.g. add up the active elements.

Alternatively, if I keep it simple and reduce the whole arrays, regardless
of how much is active, there are inefficiencies. E.g. if only half is used
in each dimension, the inefficiency is 2**3 = 8. How does this compare with
the other communications? Well the boundary exchanges take place more 
often: iterations x more. But they are smaller, because they are only 
faces, not volumes. The number of iterations is probably at max a few
times the linear dimension. So the total communications for a solve
is a few times a total volume. If that's right, then we are comparing 
few x volume with (e.g.) 8 x volume. The reduce is going to be more important
than the total solve communications. It is worth trying to minimize it.
Or else just never using subarrays would make life WAY easier.

The user_function has to know by common: ndims, iLs and iuds (or ium2)
It might do an mditerarg, in which case ifull is needed rather than iLs.

Wrote a user_function using mditerarg. It seems to work. There's an 
error using MPI_IN_PLACE with MPI_REDUCE, but not with MPI_ALLREDUCE.
I suspect that this can only be used by the root (for some stupid reason).
It does not do anything for the non-root in a REDUCE call, but IMHO ought,
nevertheless to work, for consistency. It doesn't.

Packaged the function all up into a decently easy call and moved it
into mpibbdy.f. mpibbdytest shows it working.  Commit.

Put ALLREDUCE of nrein and phirein into rhoinfcalc. This should be 
sufficient to satisfy the partcom reduce requirements. We need to
figure out the normalization and defaults for ninjcomp/nrein etc.

Found that I had to do the allreduce on psum so that psumtoq set
the external phi correctly to compensate for the electron density
which it otherwise did not for multiple processes.

Got phiexamine routine going to examine the potential after the run. 

We need a potential interpolation routine that works at the boundary.
We seem only to have a field interpolation routine at the moment.
Potential ought to be easier, but I don't know how it should work off hand.

If I did not set there to be a discontuity in phi at the edge, it might be
easier to do the interpolation sensibly. It is certainly possible to 
prescribe sphere type 1 instead of 257, which then gives a profile that
could be more easily extrapolated.

Deep thought. This is not a trivial issue. 

18 Jul 09

Built a getpotential function that does multilinear interpolation if all
the nodes are present, else falls back to another approach. At present
just uses the closest valid point. Probably not quite as good as using
a continuous phi, but should work without. Need a way to test this. 
Intend to use stencil extrapolation as the better solution, which will
also need testing.

Had an idea that I could calculate the cut volumes a lot quicker by
doing a geometric integration from one vertex (or actually anywhere in
the cell) along lines. Finding the nearest place where these move out
of the cell would be reasonably easy (bisection). This would work
correctly for cell shape that was convex. Concavity might mess us
up. For that the monte-carlo approach might be more convincing. But I
doubt if concave surfaces are usual. And we probably could avoid the
issues by specifying a particular direction for lines.

We currently use 10000 MC points which requires ndims*10000 random
numbers and 10000 insidealls. I could imagine getting decent results
from a 10x10 grid of lines each of which has to have its ends found to
(say) 1% accuracy. 1% accuracy by bisection requires about 8
insidealls (times two ends). But probably one could avoid a lot of
those calls by starting the end search at the boundary wall, which
will usually be correct. I'd guess, then that each line integration 
will cost on average more like 4 isidealls. In which case our costs are
going to be in the vicinity of n1 x n2 x 4 insidealls. Thus even if the 
insidealls are the main cost and rand is not (which is optimistic), there's
the opportunity for perhaps a factor of 10 win by going to n1=n2=15 which
ought to be fully satisfactory. In the very long run, an improvement of this
order is required if one is to contemplate a moving boundary that needs
its volume recalculated at each step. Such a situation might also call
for parallelization. 

20 Jul 09

Implemented getpotential multilinear interpolation
with two types of filling in of missing points:
1. Just set equal to the nearest point value.
2. Calculate gradients locally from nearby present points. Then fill in
by extrapolating from the value at the centroid. 
Added a phi plot to -gt

Type 2 is sometimes a lot better than fill type 1. However, it is basically
the same when we are on an axis, because then the type of absent points
is such that one face is absent in total. The gradient in that direction
can't be calculated and is taken by default to be zero. Thus cases with 
one whole face missing are almost as bad as the simple nearest point fill.
In a case like that, we must use more information.

I realize that I could use getfield (or some form of it) to obtain the 
gradient, rather than using the box vertices. This might be a more reliable
way to do it. But I'd still need an average or some other value to 
extrapolate from.

21 Jul 09

Very nearly got something working with a fallback to get the field at
the point if we fail to find the gradient from the existing box points.
However, there seems to be a bug. I think it is that it is possible to
set a box vertex that is actually on the other side of an interface 
as being present in the region, if it does not have a pointer set 
(i.e. it is not itself adjacent to the boundary). That gives a big error
when it happens, at the edge and -at2.1. It is certainly possible for
opposite vertices, even in 2-D, to suffer this problem. Consequently,
my implicit assumption that a vertex with pointer not set is in the right
region, is wrong.

This seems puzzling because cijroutine seems to be written such as to
fill in all vertex pointers that have crossings within any of their
boxes. However, the text3 plot shows that that is not happening.
Actually there's some code in cijroutine that is said to drastically
reduce the number of boxes counted. Perhaps that's ultimately the
cause.  For the standard mesh, it reduces the number of pointers from
14k to 11k. So the storage reduction is definitely not worth it.
Anyway removing that test and using the increased pointer count
fixes the bug, making my assumption correct. A 60x60x60 grid with
about 200k point uses about 50k pointers. Which by the way makes cij
objects almost as important as cij itself.

As anticipated, fillin gets a field value on average less than once
per call. So the cost of extra extrapolation is pretty acceptable. 
There are still some peculiarities at the mesh edge in the outside
region. We are getting the wrong field value. It ought to be zero
but it is not. Probably there's an issue with the cij values for
this type of BC. Actually, I don't see how I can correctly do the
extrapolation from the mesh edge. Because I think the point beyond
will be used. That's an issue. Maybe that's ok because of the cij facts.

Print out the maximum phi and grad phi errors for different grids.

grid  	  at?     solution	   phi      gradphi
16x16x10   .9	  3.62e-2	   -.330    0.376
16x16x10   --	  ''		   .385e-2  0.724
16x16x10   .5     ''               .411     0.272
16x16x16   --	  4.0e-2	   .199	    0.35
	   .5	  		   .223	    0.156
	   .9			   .256	    0.100
32x32x32   --     6.2e-3	   .27e-2   0.163
           .9	  		   9.7e-2   0.453
	   .5			   .105	    0.347
64x64x64   --     2.00e-3	   6.6e-3   1.69e-2
	   .9	  		   2.2e-2   8.8e-3
	   .5			   3.1e-2   3.0e-2
128^3	   --     1.61e-3          4.8e-3   2.68e-2
	   .5	  		   2.1e-3   3.5e-2
	   .9			   5.9e-3   4.6e-3

There are sormpi convergence issues with this the 128^3. Fiddled but not
great. 

It's not terribly obvious from these maximum errors what the convergence
is. The gradphi seems horrendously noisy. Phi does not seem quite so bad.
The domain is 10 in diameter, so the ratio of sphere radius to cell size
is 10/L. To have field errors that are of order a percent when the ratio
of characteristic size to cell is ~10 is consistent with second order
convergence, although things don't blow up as badly as you might expect.
32 is pretty bad. 

23 Jul 09

Convergence is pretty slow for the 128^3 case. Looked into the SOR.
Remember that the k_sor count is half-steps (red/black). So it is not
as bad as it looks. The biggest problem is that the omega gets too big
in the initial Chebychev acceleration calculation. It ought to be a
bit lower but I have not changed it as of now. The convergence can be
tweaked a bit but the omega choice is pretty good. I think the only
realistic way to improve it would be to do dynamic adjustment based on
actual response.  That would need more programming than I care to
devote at present.  When lambda is less than about 1, there is a
dramatic reduction in number of iterations. When lambda is greater
than about 50, there is also a dramatic reducion after the first solve.

Next we need to get the reinjection to a point where we can really compare
some cases with sceptic. At present we just have Gaussian reinjection.
We worked on getpotential in order to be able to calculate the averein
potential. Checked sceptic2 paper to see how good we need to be. 
Looks as if there's going to be an issue at intermediate lambda unless
we adopt the complicated reinjection schemes of sceptic.

The sceptic schemes are contained in

REINJECT=fvinject.o orbitinject.o extint.o maxreinject.o ogeninject.o

Of which the maxreinject has no drift. fvinject is for collisional
distribution functions, ogeninject is for gyrotropic distributions.
orbitinject is the standard form. If we hack the orbitinject file
to remove the fv, max, ogen references, then with
REINJECT=orbitinject.o extint.o
it compiles and runs. Therefore, it appears I could use this.
extint contains the netlib exponential integral function(s).
oreinject(i,dt) is the call to reinject. The main issue is the use
of piccom.f which has a load of stuff in it that I don't want.
I need to cut down those needs and rationalize the parameter passing.
The only thing that needs the reinject.f code is the test routine
fvinjecttest, and it does not actually call it, it just needs it for
advancing.f, I think. 

orbitint program tests the stuff, but it seems also too dependent on
piccom.f and sceptic calls to be useful. 
Made a version of orbitinject with implicit none. Seems to work in
orbitint. This makes clear all the locally defined variables.
Anything else is a reference to the common variables.
Then commenting out the include piccom.f should reveal those.
Ok did that an put in dummy declarations.

oreinject
c We put here the declarations that are needed to fix excluding piccom.
      real xp(6,1)
      real averein,debyelen,fluxrein,pi,spotrein,Ti,vd
      real Vcom
      logical lcic,localinj
      integer nthsize
      parameter (nthsize=2)
      integer nr,nrein,nrfull,nrused,nth,nthused
      integer nrsize
      parameter (nrsize=2)
      integer nvel,ntrapre
      parameter (nvel=2)
      real r(nrsize),th(nthsize),phi(nrsize,nthsize)

      r( is used only for determining the wall radius.
      nr is ditto.
      th( is used only for inverting to find th coord for phihere.
      phi( is used only for phihere.
      nrfull ditto. nrused ditto.
      nrsize is unused.
      nthsize is used only in the common below.

We also find that the following common is dependent on piccom
but only occurs in oreinject routine. 
c Testing
      real vdist(nvel)
      real tdist(nthsize)
      real crdist(nthsize),cidist(nthsize)
      common/rtest/crdist,cidist,tdist,vdist

oinjinit
c To fix exclusion we need
      real Vcom(1),pu1(1),pu2(1)
      real Ti,vd,pi
      integer myid,nvel

pu
c needed from common:
      real averein,Ti,pu1(1),pu2(1)

alphaint
c needed from common:
      real adeficit,averein,debyelen,Ti
      integer nr,ninner,diagrho(1),r(1),rcc(1),diagphi(1)

  diagrho( is used only in
           if(diagrho(1).eq.999.)then

  rcc( and diagphi( and ninner are used only in
            if(ninner.gt.iqsteps)stop 'Alphaint Too many r-cells read.'
            do i=1,ninner
               qp(i+1)=1./rcc(ninner+1-i)
               phibye(i+1)=diagphi(ninner+1-i)
            enddo
So none of the above are important if we are not signaling a special
case using diagrho(1) in a kludgy way. Actually it is a signal that
we read a file in in orbitint. Completely unneeded for most use. 

  r( is used only in 
            call initext(iqs,qp,phibye,phiei,r(nr),xlambda)

Now the plan is: construct a different common declaration that together
with existing ccpic declarations will satisfy the needs and allow
testoij to run properly. plascom.f has 
	common/plascom/debyelen,Ti,vd,rs,phip

averein and adeficit don't exist.

Start reincom.f with the random interpolate data from piccom.
      nvel, nQth
      common /rancom/Gcom,Vcom,Qcom,pu1,pu2,Pc,infdbl,bcphi,bcr

Add averein, adeficit, fluxrein, spotrein, myid, ntrapre

Comment out the 999 region. Replace r(nr) with rs. That satisfies
alphaint, pu, oinjinit.

In oreinject
Comment out rs declaration, and setting of it. It's already set.
Remove r( using rs. Remove nr.

Convert phihere section to a function. Get rid of th, phi, nrfull, nrused.
Define nthsize=201 in reincom.f for the commons same as piccom.f. 

We still need nth, nthused which appear in 
      if(LCIC)then
         icr=(1.+crt)*0.5*(NTHUSED-1) + 1.5
      else
         icr=(1.+crt)*0.5*(nth-1) + 1
      endif
In LCIC cases in orbitint, 
   	      nth=nthsize-1 nthused=nth nthfull=nth+1
In non.LCIC   nth=nthsize, nthused=nth-1 nthfull=nth
In sceptic, nth is set by command line. So how do we fix this?

Made two more parameters depended upon nthsize that fix this to the 
LCIC case.

To get a test going with the testoij code we need to deal with
averein, adeficit, fluxrein, spotrein ...

fluxrein and spotrein are set by orbitinject, but never used by orbitint.
So nothing needs doing.

averein is set by orbitint and not by orbitinject, except in the code that
has been commented out in testoij. So we just need to set it differently.

adeficit is default set to zero by orbitint and only set by commented out
code in orbitinject. So we need to zero it differently. 

ntrapre is not used.

nrein is used by orbitint after orbitinject is called, for normalization.
it is incremented by ilaunch each time, which might be bigger than one.
We need code to get it back.

Summary we need: avereinset(+avdeficit) and nreinget in orbitint.f.

Found rp=rs error in orbitinject.

Got orbitinttest going to the extent of completing although not 
correctly.

With further corrections, got orbitinttest working the same as orbitint.
Got rid of lcic, nthused, nth. Retain nthsize=nQth+1 for size of arrays.

Imported a version to ccpic. Got it to link by adding relevant stuff.
Segfault (as expected). There's a fair amount yet to fix in the initing
and in xp passing averein calculation, etc.

24 July 09


Found some bugs in padvnc.f. Was not correctly handling if_part on
reinjection, since it was not being set in reinject. This probably
accounts for prior particle decay. But in any case it changes
the results.
Rationalized reinject call not to pass particle number.
Corrected the rs handling to keep it equal to the sphere radius,
even though the mesh is a tad larger.

Now seems to run in a similar way with new and old reinject.
But I need a portable way to diagnose the reinjections. If it is built
into padvnc, then it will apply to every injection scheme. Probably 
that's best. 

Implemented some reinjection diagnostics and plot that are independent
of injection scheme and implemented by a simple call in padvnc. Seems
to give similar results from my two schemes: shifted maxwellian and
orbitinject. They ought to be the same here because averein is being
taken as zero at present. 

Printed out the velocity distributions because a simple gaussian comparison
is not correct. With counts peaking at 2k per box, the distributions
are indistinguishable. Conclusion: the orbitinject is working. Commit.

TIME TESTS using sceptic:

Single process 
time ./sceptic -s200 -ni900000 -nr20 -nt20 -g -f
tp400:
real	0m24.868s
unity:  
real	1m21.860s
Thats a pretty big difference. tp400 is only using one core.
It is dominated by particle pushing. 
-ni100000: unity gives 0m20.754s. and tp400: 0m6.555s.

Two identical processes doing this on tp400: 0m7.002s. unity: 0m36.481s
I.e. very little overhead on tp400 maybe more on unity.
But that's not real multiprocessing we need scepticmpi.

scepticmpi -ni100000
unity 0m41.520s, tp400 0m7.199s. Not much difference on tp400. 

sceptimpi -ni300000
unity 1m51.364s  tp400 0m20.609s

four processes -ni300000 tp400 2m10.836s !!! Wow, that's a big hit, nearly
a factor of 10! Both processors showed 50% utilization for each sceptic.
There must have been a monstrous hit from swapping. 

Quick summary:

Single processor speed difference is about 3.3 times faster cf unity head node.
Per cpu speed is about 5.0 times faster on tp400 than unity running mpi.
It is useless to run more than 2 processes on tp400.

Total cpu capacity on unity = 36x1 on tp400 = 2x5. 36 vs 10.
tp400 is about 1/4 of the total cpu capacity of unity in multiprocessing.

[But probably there are some big breakpoints.]

The Loki tests of Feb 2008 showed Loki processors to be 3-4 times faster
than unity head node. It therefore seems that tp400 is getting about the
same per-cpu speed as the Feb2008 Loki cluster. cmodws45 got about 2x slower
in 2008. 

25 July 09

Need for averein to be set.
In sceptic averein is set from diagphi, which is part of the diagnostics
and is averaged over nstepsave. So averien is time-smoothed. We would have
to save it for restart purposes. It does not make sense to average every
injection. We ought to add injection potentials to a counter and then
divide by the number added together when we complete the advancing step.
The addition should be done in diagrein. The computation might be done
at the end of padvnc. Let's use spotrein for the sum of potentials at
rein and fluxrein as the sum of reinjections. That's what sceptic uses
them for. They need to be zeroed at the start of each padvnc.

There's currently a conflict between reincom and partcom, both of which
contain nrein. I'm not sure that nrein is really needed in partcom, although
it is written and read back, and needs to be MPI reduced. As presumably
may the other accumulators. There's also a variable phirein, which is not 
currently used except in rhoinfcalc.

27 July 09

First make reincom just the extra stuff and put explicit inclusions of the
plascom and rancom headers, where needed.

Now need to rationalize averein, phirein, nrein, ninjcomp. There's no
phirein in sceptic. 

For restart purposes, anything that influences the orbits or field must
be saved and restores. nrein is set to zero at the start of padvnc. It
does not need to be saved and restored, I think. However ave/phi-rein
if averaged from step to step, will need to be saved/restored. It is more
of a field quantity than a particle quantity, in a sense.

adeficit is used in orbintinj, and in sceptic set in advancing as part
of fcalc_lambda:
c Boundary slope factor calculations:
      do j=1,NTHUSED
c Current fractional ion deficit due to collection.
c Coefficient of 1/r^2 in modified shielding equation is
c a = deficitj * r_edge^2 / \lambda_De^2
         deficitj=1-phi(NRUSED,j)/Ti -rho(NRUSED,j)
c         write(*,*)rho(NRUSED,j),phi(NRUSED,j),deficitj
         blfac1=(deficitj/debyelen**2) * redge
         adeficit=adeficit+blfac1
c BC modification is (a/r_edge)[exp(EL*r) E_1(El*r)] given by approx.
         blfac=blfac1*expE1
         blfac=alpha*blfac
         phislopeconst(j)=blfac*redge*delredge/
     $        (redge+delredge*rindex*0.5)
         phislopefac(j)=(redge-delredge*rindex*0.5)/
     $        (redge+delredge*rindex*0.5)
      enddo
c Actual a factor averaged over angles:
      adeficit=adeficit*redge/NTHUSED
      if(adeficit.lt.0.)then
c         write(*,*)'Negative adeficit',adeficit,' set to zero'
         adeficit=0.
      endif

This gets used in the boundary condition of the sor solver. I don't think
it makes sense for CCPIC to think that we are going to be changing the 
BC every step (maybe not at all). 

29 Jul 09

Implemented OML adeficit setting (only once in the stencils).
This overrides the boundary condition in the input geometry file,
but seems to give very sensible results.

Implemented infrastructure for getting averein (in reincom) and
phirein (in partcom) from spotrein, which uses getphihere. So far
getphihere gives zero and no changes of result are found. 

There's a problem with the fact that getpotential needs lots of stuff
like u, cij, etc. So it can't readily be called from reinject.
It could be called from padvnc, but padvnc does not know the ilaunch
values etc. If I add ilaunch to the reincom, then its value along with 
nrein, spotrein and fluxrein are available to reincom. Then if we
return to padvnc before doing any advancing; getpotential; and pass it
to a routine to advance; that would work. Clumsy though. 
We could take the nrein passing out of reinject if we did this, because
it would be updated at the same time as averein etc.b

This needs rationalization. 
1. We don't need to update adeficit.
2. We update the potential used by reinject once per cycle.
3. We need to average the phihere to supply this value.
4. nrein and potl needs to be incremented by an amount that depends on the 
   number of launches.

I get rid of the localinj section. Then I don't need phihere inside the 
oreinject. Get rid of the accumulation inside of oreinj. 
Pass ilaunch to reinject. So the accumulation can all be done in padvnc.
We no longer pass nrein, because we do the accumulation in padvnc.

Pass potential and ilaunch to diaginj, and therein increment spotrein.

Remove nrein and ntrapre from reincom.f. Now not needed.
Remove fluxrein and spotrein from reincom.f. Now not needed really. 
The only things that really have to be in reinextra are averein, adefict.
These are to be set (only) by avereinset and adeficitset.

Revert diaginj to not passing or calculating with phi or ilaunch.
Update phirein in padvnc. Call avereinset and adeficitset appropriately.

Seems to be working. phirein seems very stable for l>1 and very small 
for l<1.

At present I am using nrein, phirein in rhoinfcalc for calculating
rhoinfinity. It seems to be correct, but not thoroughly checked.

30 July 09

Implemented saving of rhoinf and dt per step which then allows one to 
get the flux density normalized by rhoinf 

We have a result. The OML value flux density is 1/\sqrt{2\pi}
\sqrt(T_i/T_e) (1+\chi) which for \phi=-2 is 3/\sqrt{2\pi}=1.19. We
get this value within less than 1% with large debyelength (40x40x20)
at T_i=T_e.

At lower -l flux drops, which qualitatively it should. There is some
systematic flux enhancement at the poles. The potential and density
get very noisy, and without multiple processors, it is tough to do
much about that. We also need to look at more-negative potentials. But
first off we have a decent result that verifies the orbit solver,
though not really the potential solver with finite rho. Also verify
that acceleration is working.

31 jul 09

Unfortunately this is not working. At higher probe potential only
moderate enhancement of the flux occurs. Not nearly enough for
OML. E.g. at probe potential of -10. Also there's a major difference
between running 200k particles on one processor and 2x100k on two
processors. Therefore there's a bug in MPI somewhere. The total flux
count from both processors is about 420, and it's about twice that
reported by each. Rhoinf is about 420 (too). With one processor, there
seem to be significantly more flux count, about 460. It's unfortunate that
we need to go to a fairly high step count >100 otherwise things are not
converged. 

2x100k Total flux 440, 1.65, rhoinf=430, nrein=2077
200k   Total flux 440, 2.08, rhoinf=340, nrein=3166

tells the difference. The nrein and rhoinf are printed after padvnc and
fluxreduce but before rhoinfcalc. So I think they refer to just one 
processor, since nrein and phirein are reduced in rhoinfcalc. 

I just realized another issue, although I think not one that is relevant
here, that is that if the MPI_COMM_WORLD had different processes from the
CART_WORLD, then I might end up with doing particle calculations on a
node that does not participate in the information about potential. So it
might be completely bogus. 

I have to decide what to focus on. I'll start with single processor issues.
The flux is way too low. Why? Idea: replace getfield with a coulomb force.

Do this easily in padvnc. Then get

./ccpic -ni100000 -s200 -dt0.05 -da5 -l1000
0200   2 iterations. Phirein=   -1.983818 Total flux   219.00000   2.0631711    
 Total flux   219.00000       2.0631711    
nrein,n_part,ioc_part,rhoinf,dt= 1488 100000 100000   168.939     0.050

 Average flux over steps         100         200  All Positions:   4657.2266    
 rhoinf   164.82874       Average particles collected per step:
  4.9406  4.5545  4.9208  4.8515  4.8020  4.6139  4.9604  5.2970  4.2475  4.5842
  4.2673  4.4356  4.7624  4.6139  4.4752  3.9901  4.4653  4.8119  4.7723  4.4158
  4.9802  4.9109  4.9505  4.5644  4.4851  5.0000  4.4752  4.0990  4.6832  5.0396
  4.7723  4.4653  4.1782  4.7228  4.6436  4.8911  4.9901  4.6931  4.5941  4.0792
  4.7030  4.8020  4.7624  4.7624  5.0099  4.3861  4.8119  4.5743  4.5545  4.4950
 Flux density, normalized to rhoinf   2.2484589    

For comparison, still with the coulomb force, but with -l1. instead of -l1000.

0200  36 iterations. Phirein=   -0.072735 Total flux   339.00000   3.4161470    
 Total flux   339.00000       3.4161470    
nrein,n_part,ioc_part,rhoinf,dt= 1110 100000 100000   157.937     0.050

 Average flux over steps         100         200  All Positions:   7102.1768    
 rhoinf   165.44156       Average particles collected per step:
  6.7327  7.4752  6.8020  6.9703  7.2475  7.1584  7.2475  6.6040  7.3069  7.0297
  7.0495  6.6733  6.9901  7.1881  7.5644  8.0297  7.1980  7.6832  6.6733  7.1188
  7.3663  7.1188  6.4950  7.4257  7.0099  6.4653  7.2574  7.2673  6.3366  7.0891
  6.5743  7.0891  8.0495  7.2079  7.0297  7.0495  7.1089  7.3564  6.9406  7.1584
  7.1287  6.9307  7.1287  7.1683  6.7624  6.9307  6.9109  7.4356  7.9109  6.6634
 Flux density, normalized to rhoinf   3.4161532    

That's pretty strange, since the force is always coulomb. 


Try setting phirein=phip/5 immediately before padvnc. Makes the the flux
value low although I don't understand all the output:

0200  41 iterations. Phirein=   -0.297464 Total flux   335.00000     1.766086
 Total flux   335.00000       1.7660868    
nrein,n_part,ioc_part,rhoinf,dt= 2439 100000 100000   301.893     0.050

 Average flux over steps         100         200  All Positions:   7347.1270    
 rhoinf   309.72659       Average particles collected per step:
  7.7921  7.5149  6.9703  7.0594  7.5743  7.6535  7.6634  7.3762  7.8812  7.1782
  7.4851  7.2574  7.4455  7.3960  7.0594  6.9802  7.5347  6.7030  7.4653  7.8119
  7.2871  7.3069  7.2871  7.0495  7.1881  7.2673  7.0693  7.5743  7.6832  7.4950
  7.5248  6.9406  6.9406  7.8416  7.4752  7.1683  7.5842  7.6634  7.6238  7.4455
  7.3663  7.2376  7.2079  7.3861  7.3069  7.3960  7.3267  7.1584  6.7624  6.9901
 Flux density, normalized to rhoinf   1.8876851    


sceptic run like this:
./sceptic -l1000. -nr50 -nt10 -g -ni1000000 -p-10.

gives with some extra diagnostics:

  Flux, Phiedge, Trap, ReinjFrac
  401:  2 nrein=        3047   averein=  -1.9901596      riest=   325.22070      rhoinf=   328.70486    
  402:  2 nrein=        3059   averein=  -1.9901588      riest=   326.50159      rhoinf=   328.61774    
  403:  2 nrein=        2995   averein=  -1.9901581      riest=   319.67062      rhoinf=   328.56485    
  404:  2 nrein=        3083   averein=  -1.9901574      riest=   329.06339      rhoinf=   328.34250    
  405:  2 nrein=        3083   averein=  -1.9901565      riest=   329.06348      rhoinf=   328.36053    
  4.197 -1.990  37401  0.008

This is with dt=.025, so the nrein is of order 6000 for a .05 step.
Plainly the major difference is in nrein. Actually in ccpic there are
essentially no repeat reinjections. nlost=nrein. So any changes that
are arising from edge potential must be through the velocity
distribution. Arrghh.

1 Aug 09
Found the cause of segfaults in sceptic. It was brcsq=0. Changed the 
c Reject a particle that will not reach boundary.
c Was brcsq.lt.0. which gave segfaults.
      if(brcsq.le.0.) then
         goto 1
      endif
This probably should be fixed in a couple of other places in the sceptic
code too.

Implemented mean speed diagnostic of reinjection. It does not look the 
same for sceptic and ccpic. In particular the speed of ccpic takes a step
at the value of phirein, which is wrong (should be sqrt(phirein)).
No. That was an error. I put -p10 instead of -p-10. With -p-10 we get
speed up to 2 in both. Perhaps there's another factor of 2 floating around.
To the naked eye the distributions look the same. If that is right, then
it says we are injecting correct velocities and positions.

5 Aug 09
Ran equivalent cases in sceptic and ccpic. For ni=200000, dt=.05, -p-10
-l1000
ccpic approximate numbers:
ninner=450. nrein=3100. phirein=-1.9839. rhoinf=330. 

sceptic: 
ninner=450. nrein=3080. phiedge=-1.99. rhoinf=164.

Thus the problem is with rhoinf in ccpic. It is a factor of 2 too high. 

In sceptic rhoinf comes from rhoinfcalc:
c smaxflux returns total flux in units of Ti (not 2Ti)
            riest=(nrein/dt) /
     $           (sqrt(Ti)*
     $           smaxflux(vd/sqrt(2.*Ti),(-averein/Ti))
     $           *r(NRFULL)**2 )

and in ccpic
c Calculate rhoinf from nrein if there are enough.
         chi=min(-phirein/Ti,0.5)
         riest=(nrein/dtin) /
     $        (sqrt(Ti)*
     $        smaxflux(vd/sqrt(2.*Ti),chi)
     $        *rs**2 )
         rhoinf=riest

But the expression for chi is wrong! It needs to be max(-phirein/Ti,-0.5).
We were constraining the effective phirein to be -.5, when it was really
-1.98.

With this fixed, we get flux 4.472 normalized to rhoinf. (Sceptic 4.3916).
11/sqrt(2\pi)=4.3884. So ccpic is a bit high. rhoinf=165.
This is with pure coulomb test potential for acceleration. Running with
-l100000, phirein=-1.9947. flux=4.4755. 

Full ccpic mesh potential. flux=4.4122. 
Using 400 steps instead:   flux=4.3756 (mesh). flux=4.4008 (coulomb).
Using dt=.025 4.4049 (mesh).

These numbers appear to be consistent with a flux count that is for dt=.05
400 per step times 200 steps =80000 total particles => 0.4% accuracy.


6 Aug 09
There's still a bug in the mpi code. For 2 processes we get
[tp400 CCPIC]$ mpiexec -n 2 ./ccpic -ni200000 -s400 -dt0.025 -da5 -l100000
Average flux over steps         200         400  All Positions:   17157.578    
 rhoinf   262.65146       Average particles collected per step:
  8.6119  8.1891  8.6269  8.4826  9.1791  8.4378  8.7761  8.1144  8.9701  8.6119
  8.7811  8.7164  8.0547  8.8060  8.0945  8.3433  8.5373  8.7363  8.3134  8.4179
  8.5970  8.9154  8.3234  8.6617  8.5423  8.7612  8.3831  8.6020  8.3781  8.7164
  8.4876  8.6866  8.6965  8.4925  8.4478  8.9005  8.6269  8.1791  8.6368  8.8259
  8.9900  8.7313  8.9751  8.4826  8.7811  8.4527  8.1841  8.8955  8.5174  8.2687
 Flux density, normalized to rhoinf   5.1983638    
 
 0400   2 iterations. Total flux   441.00000       5.3087544    

And the standard case:
[tp400 CCPIC]$ mpiexec -n 2 ./ccpic -ni200000 -s400 -dt0.05 -da5 -l100000
Phirein=   -1.994918
0400   2 iterations.  nlost=        4186  nrein=        4186  ninner=      419
 Total flux   861.00000       5.1920910    
 Wrote particle data to T1e0v000r05P10L1e5.000
 Wrote potential data to T1e0v000r05P10L1e5.phi
 Wrote flux data to T1e0v000r05P10L1e5.flx
 Average flux over steps         200         400  All Positions:   17160.164    
 rhoinf   264.44150       Average particles collected per step:
 17.3831 17.1393 17.1443 16.6119 18.4229 16.7662 17.6169 16.2786 17.9900 17.0448
 17.1294 17.8358 16.1493 17.7214 16.2338 16.9154 17.2786 17.3831 16.5970 16.9005
 17.1493 17.7811 16.5274 17.2687 16.7512 17.5075 16.6219 17.2488 16.6617 17.7264
 16.9900 17.2289 17.2139 16.9204 16.9154 17.6219 16.8607 16.5174 17.3333 17.7711
 17.6766 17.7413 17.7363 17.2338 17.6716 16.6219 16.5373 18.0348 16.9154 16.6816
 Flux density, normalized to rhoinf   5.1639533    

This appears to show that rhoinf is too low, because nrein is too low. 
With two processors, it ought to be about twice the one-processor value
which was 3100. Or it ought to be the same if I am only seeing one processor's
worth. Actually seems as if we are monitoring each processor separately
and then getting too great nrein?
The flux of 861 is roughly twice the ninner number of 419,
which is about right. 

One problem was that numprocs was not being set in common by the sormpi
initialization, because it is stored in partcom, which is not available.
For now, set it explicitly with an MPI call in ccpic. This seems to 
fix it, but the plotting at the end seems to break when specifying -l
switch to mpiexec.

 nrein,dtin,Ti,vd,phirein,chi,rs,numprocs=        6328  5.00000007E-02  1.00000000       0.0000000      -1.9947799       1.9947799       5.0000000               2
0200   2 iterations. Phirein=   -1.994886  nlost=        3106  nrein=        3106  ninner=         445
 Total flux   905.00000       4.2716675    
 Wrote particle data to T1e0v000r05P10L1e5.000
 Wrote potential data to T1e0v000r05P10L1e5.phi
 Wrote flux data to T1e0v000r05P10L1e5.flx
 Average flux over steps         100         200  All Positions:   18373.264    
 rhoinf   331.71246       Average particles collected per step:
 19.1683 17.9604 18.8515 18.6040 18.6733 18.7921 17.6931 18.7723 17.7525 18.5743
 17.8317 18.3960 18.5049 19.3267 17.5644 17.4158 17.9010 19.0990 18.4455 17.9406
 19.2376 18.4752 18.5941 18.0495 17.5644 18.6040 18.2475 17.8218 17.7624 18.8911
 18.2475 18.7030 18.2475 17.5644 18.2277 19.2079 18.7921 18.8515 17.9901 18.3366
 18.8317 18.9208 18.6733 18.2277 17.9802 18.1287 19.0990 17.9901 18.4752 17.6535
 Flux density, normalized to rhoinf   4.4077301    


08 Aug 09

Rationalized some of the MPI functions into psumreduce to get them out of
main and other functions. 

It appears that the first time that numprocs is required is in pinit. 
This is called before the first psumreduce. So it won't do to set numprocs
in psumreduce. To get the MPI call setting numprocs out of main
requires a plan for this. This and the finalize are the remaining calls.

Move the additional MPI routines into reduce.f. Can't cvs add because of
bad nameserver in Venice. Do it later. 

How to fix possible mismatch between cartesian communicator and particle
(MPI_world). We don't want every gather in solver to be to all the world. 
However, 

10 Aug 09
gprof shows that field interpolation dominates in particle-dominated
runs. The interp is for partlocate. Gradinterp is for getfield.

  %   cumulative   self              self     total           
 time   seconds   seconds    calls   s/call   s/call  name    
 22.96      3.44     3.44 148586924     0.00     0.00  gradlocalregion_
 15.75      5.80     2.36 37146731     0.00     0.00  getfield_
 13.82      7.87     2.07 36890568     0.00     0.00  interp_
 11.35      9.57     1.70       30     0.06     0.15  chargetomesh_
 10.21     11.10     1.53 38305999     0.00     0.00  gradinterp_
  5.64     11.95     0.85 50755040     0.00     0.00  inside_geom_
  3.14     12.42     0.47     1121     0.00     0.00  sorrelaxgen_

So the getfield is more than half of the cpu time:
-----------------------------------------------
                0.02    0.04  256163/37146731     fillinlin_ [25]
                2.34    5.34 36890568/37146731     padvnc_ [3]
[4]     51.6    2.36    5.38 37146731         getfield_ [4]
                3.44    1.53 148586924/148586924     gradlocalregion_ [5]
                0.08    0.33 37146731/37146731     boxinterp_ [14]
-----------------------------------------------
                3.44    1.53 148586924/148586924     getfield_ [4]
[5]     33.2    3.44    1.53 148586924         gradlocalregion_ [5]
                1.53    0.00 38305999/38305999     gradinterp_ [9]

By implementing a jump out of the loop in getfield when ii1=0, reduces
the time in that routine from 2.36 to 2.00, a 15% saving. 

  %   cumulative   self              self     total           
 time   seconds   seconds    calls   s/call   s/call  name    
 23.44      3.41     3.41 148586924     0.00     0.00  gradlocalregion_
 15.22      5.63     2.22 36890568     0.00     0.00  interp_
 13.71      7.62     2.00 37146731     0.00     0.00  getfield_
 12.78      9.48     1.86       30     0.06     0.16  chargetomesh_
  8.49     10.72     1.24 38305999     0.00     0.00  gradinterp_

c Probably most of the time is spent in this calculation.
            uprime= ((2.*xm+1.) * (up-u0)
     $           +(1.-2.*xm) * (u0-um))/(dx0+dx1)

This is (2xm.up-2xm.u0 -2xm.u0+2xm.um + up-u0+u0-um)/(dx0+dx1)
= (2*xm*(up-2*u0+um) +up -um)/(dx0+dx1). This might be quicker.
But it isn't. 

Summarizing the computational costs for particle mover, they are
dominated by the getfield, gradlocal, and gradinterp region costs,
which is finding the field. It does not seem possible to make 
substantial savings in this area. 


12 Aug 09

Systematic comparison of sceptic and CCPIC with:
  dt    vd     Ti     steps. -c5  -ni200000 -nproc 2 -nr50 -nt10. -p-10. 
 0.0250 0.0000  1.000  500 
								ni700000
 		Flux:		-da3		-da5		60x60x60
debylen		Sceptic:	CCPIC
.1		1.6146		1.7244405			1.6327829
.2		2.2890		2.3339274			2.2894616
.3		2.7719		2.7993238			2.7707448
.5		3.3643		3.3790596 			3.3812742
.7		3.7056		3.7201011			3.7148750
1.		3.9970		4.0019903			3.9962780
2.		4.3465		4.2363796	4.2306852	4.2459707
5.		4.3954		4.3413186			4.3627992
10		4.3775		4.3506932	4.3558187	4.3643198
		
[Note theoretical OML: 11/sqrt(2\pi)=4.3884]

It appears there might be slight systematic discrepancies at the OML
end and at the small debyelen end. The debyelen is not being properly
resolved at -l.1 because of -nr50 and 40x40x20 resolutions. Therefore
discrepancies are to be expected. In the intermediate regime the
discrepancies are ~1% or less.  This seems very encouraging.

26 Aug 09

Fix the mismatch between numprocs and the cartesian geometry by using
the MPI_DIMS_CREATE function that fits the cartesian geometry to the 
exact number of processes available, in an optimal way. It is not called
if the number of processes is equal to the idims product already. So 
by setting idims and numprocs right, one can override the function choice.

Known weaknesses/needs
1. Possibility of undefined potential (/field) when in a concave region with
no adjacent points in the region.

30 Aug 09

Filled in the 60x60x60 column above. This shows much better agreement
with sceptic at low debyelen. Better than 0.5% for debyelen<=1.  But
also seems to indicate there's a problem at large debyelen. It's not
100% certain that the problem is CCPIC. It might be sceptic (also).
mpiexec -n 2 ./ccpic -dt.01 -da10 -s500 -ni700000 -l10
gives 4.3776546, which suggests that shorter timesteps increase the 
apparent flux. This might suggest a going-through-sphere process. 
Sceptic avoids that, but CCPIC does not. 
mpiexec -n 2 ./ccpic -dt.05 -da3 -s500 -ni700000 -l10
gives 4.3439922, which definitely confirms there's a timestep effect.

mpiexec -n 2 ./ccpic -dt.025 -da5 -s500 -ni700000 -l100
gives 4.4045405, which is perhaps surprising. 

Using -p-25 we get from
mpiexec -n 2 ../ccpic -dt.025 -da5 -s500 -ni700000 -l100
10.382284. Should be compared with 26/sqrt(2pi)=10.3725. Pretty good!

Conclusion

The symmetric results seem to indicate excellent agreement at 60^3 mesh
size with OML and with SCEPTIC, down to l=.1. 

Things to do next.

1. Add momentum and energy to the flux quantities monitored and displayed
by fluxdatatest.

2. Add ability to account for flux of momentum across a surface (presumably
rectangular) including the Maxwell stress.

3. These things might be incorporated into a new way of handling events
consisting of surface crossings: movement from one region to another. 
To do this we would have to keep account of what the region is that a
particle is in. Then we compare new with old before updating it. 
This seems to be already available since tallyexit will be called crossing
any boundary. We need to add the ability not to reinject if this is not
a solid boundary, and the ability to have variable oldregion (if we continue
to use insideall). 

4. Develop the ability to do more general object construction, e.g. sums
and differences of regions. This requires a way to mask insideall. Insideall
returns an integer whose bits are set for all the objects that the particle
is inside. If the object is the (inclusive) OR of two others, then the test
we want to apply to it is whether both bits are zero, in which case we are
outside. This would call for masking the others. Our current test is that
we want the particle region to be outside sphere 1 but inside 2. This works
without current problems because it is a single region.

Cases: I1 and I2 = ~(O1 or O2), O1 and O2 = ~(I1 or I2), 
       I1 and O2 = ~(O1 or I2), O1 and I2 = ~(I1 or O2).
This shows that every or can be rewritten as the negation of an and.

The values to define a region might be numerically this:
1: inside region, 0: outside region. [As currently with iregion].
A mask would say pay attention only to certain bits, not others. 
But it would have also to accommodate boolean combinations of regions.

Since we have the inside_geom function for individual objects, which is 
a faster call, probably we ought to rely on that. 

5. We need the ability to ignore the boundary of certain objects when
setting up cij. Those are objects that don't affect the poisson solution
but are there for other purposes. One way to do that would be that if
all of ABC for the object are zero, then no BC is set: so ignore in potlsect.
Did that. Seems to work correctly. 

6. Then there's an issue of volume setting. This ought to be affected only
by the particle region. That's actually true except for the tests for the
region to calculate and the volintegrate routine, which just use insideall
tests. Thus we basically need a more general insideall test that can handle
unions etc. Then it should work. 

At present compare the result of insideall with a number to tell the active
region. This is not general enough. Probably the simplest solution is to
have a logical function instead, that says: am I in this generalized region?
Then I just need a way to specify the generalized region. I don't think
it is worth the effort to devise a way to specify an arbitrary logical
operation. So instead, code possibly more than one directly and make the call
logical linregion(irg,ndims,x) where irg specifies the generalized region
numerically, but it doesn't generally do so just by its bits. For example
we could make negative irg point to a list of logical combinations, while
positive irg could be bits. 

A general way would be to have a 2-d array of numbers bool(ni,nj) such 
that the logical expression is   Prod_1^nj Sigma_1^ni inside(bool(ni,nj))c
where inside is true if we are inside object bool(ni,nj) (and negative values
of bool means true if outside). To simplify passing, one can make this
a self describing array: n1, n1*values, n2, n2*values, ... , 0

Implemented the self-describing boolean array ibool_part and gave it
a standard default. Eliminated insideall from 
pinit.f
volint.f
test in padvnc

Unfortunately there are rather a lot of places where iregion is used 
in the code. It's not so obvious how to disentangle the presumption that
iregion carries the region information. 


1 Sep 09
Installed ftnchek and made make target ftnchek. 
Started to get rid of warnings. Got most of them gone. 

2 Sep 09
Realize there are multiple types of generalizations of iregion that we need.
1. Particle region, which also defines the charge-density volumes.
2. Potential and field, solution and interpolation.
3. Additional surfaces for force and other calculations.

The particle region is defined by ibool_part. We have purged the use of 
insideall from the volume integration and particle initialization.

We need to purge iregion_part, because that usage no longer makes sense.
At least we need to get it out of being used. Currently it is _set_ in
ccpic and fieldtest, and _used_ only in padvnc. The checkcode tests its
saving, but that's not important.

In padvnc it is used to set iregion, which is passed to getfield, and 
the difference between inewregion and iregion is passed to tallyexit.
In the long run, I want to (possibly) tally all surface crossings. So
the tallyexit must be changed to use the difference between the two regions
of the start and end of step. 
We need to correct the iregion passed to getfield. It's not clear we should
pass anything. Probably getfield ought to decide its interpolation 
separately. 

Ok iregion_part is now unused.

The gradlocalregion code uses
         if(idob_sor(iregion_sor,icp0).ne.iregion)then
for testing the possibility of region crossings. Therefore, provided the
idob_sor region values are correctly adjusted to use the potential region,
it ought to work. There's no reason why getfield needs to be told the 
region. It is told the position. It can figure out the region for itself
from the position. Actually it's not so straightforward, because getfield
is given the mesh position, not the physical coordinates, and what's more
it can be given internally-based values. So really, getfield _can't_ 
currently get the region for itself. However, it could be passed the value
of insidemask and then compare it with idob_sor, which should have been
initialized with insidemask as well. Implemented insidemask and made
the changes in 3dobjects. Seems to work the same. The mask is currently
all 1s.

padvnc needs insideall for the logging information and insidemask for
the getfield information. These might be different. It's inefficient
to call both. Maybe I need a imaskregion function. Yes do it using 
the F95 intrinsic. Works. Now need to set the masking from the readgeom.
Implemented mask setting using F95 intrinsic IBCLR.

Found there's an issue with parallelpiped objects. abc are put at
the end in standard objects, which is at 8=1+1+3*nd. The objects are
described by 2*nd= center, 3 radii/lengths. But a parallelpiped needs
more lengths: an origin and 3 vectors= nd*(nd+1). If abc are at the end
then they would be in a different position. These are 3dobjects of course,
so the total is 1+12+3=16. One approach would be to require the parallelepiped
to put its data in a funny order: type origin vector1 abc vector2 vector3.
That's not totally stupid. Do it for now.

4 Sep 09

Got the code going with masking in input to getfield and getpotential. 
It works provided first two objects define the probe and the boundary, 
which is assumed in ccpic. You can then add on other objects as desired.

The sensible thing is for fluxdatainit and tallyexit to be able to cope
with other types of objects and crossings. They currently do the required
tests to see if the object is mapped or not. However, the interface left
open is the grid on the object which we are asking flux etc to be mapped
into. Currently this is programmed. 

Perhaps what is most needed is to specify what is programming API and what
is input configurable. And to separate definitively the specific aspects
from the other code.

Got xoopic (and got it going after fiddling). It is the same code base as
techx oopic, but uses the xgrafix graphics library, which restricts it to
unix-like machines. Oopic uses Qt and runs on mac and windows. It implements
most things in input files, although I think extra diagnostics can be
added on through code modifications. Its boundaries appear to be made
of individual line segments, each of which can do accumulation. There's
a load of code for parsing input and presumably setup. Boundary ends are
said to be moved to the nearest grid point. I don't know how the interpolation
is done if the lines are diagonal. 

I think the xoopic approaches are not all applicable to 3D. It's just
too cumbersome to have generalized facets and use them to construct
all objects. That was the conclusion I came to early on. I think it was right.
But I still haven't really defined the API.

7 Sep 09.

We need a way to specify the flux collection for an object in the input
file. This specification should be along the lines of the code in fluxdata.
For a sphere, we might wish to collect in angles cos\theta and \psi, with
a variable number of angles for each. In general, then we need to specify
a type of gridding and the number of bins in each of up to ndims-1 dimensions.
This would still be true if we specified the assignment in some kind of 
fourier space, such as spherical harmonics. Therefore, we ought to write
a general flux initialization code and objsect code that can handle this.
What we currently have is only rudimentary. 

The additional information we therefore need for each object is the 
flux collection type, and two integers. 

We currenly have a bit of a problem with       real obj_geom(odata,ngeomobjmax)
because the odata are ordered such that the abc is at 8, which is not enough
data prior to it for the geometric information. It might be better to 
reorder it so that this is more flexible. To do that, I need to make sure
there are no direct references to explicit values of the first index.
Ok purged all direct references to position in obj_geom. Using only
relative to the defined parameters such as otype,ocenter,oradius,oabc, ...

Permuted the order of oabc and other things. Then found a bug. It is not
in what I've just done, but it is that when an extra sphere of radius .5
is added to the objects, a change in one of the flux counts occurs.
The addition of the sphere causes one less particle count for one step
and one position. But making the radius 0.8 instead of 0.5 makes the 
differences much greater. And making it .1 gives no differences. Obviously
the counting is not working properly. This appears to be because tallyexit
chooses to count the crossing as being for the last object crossed.
This is presumably a mistake. One ought to count it for all objects crossed.
Changed tallyexit accordingly. That fixes the difference.

Implement ofluxtype ofn1 ofn2 as the object flux collection parameters.
The problem with my input file is that deciding things by ordering is
very problematic when the number of parameters on a line gets large.
Especially when one wishes to set values above others and the number
of initial values is variable. Perhaps therefore we need to rethink
readgeom routine and input format. This would be a big issue. I don't 
want to do that now, and if it needs to be done, I might write a
code to prepare the input file.

Gradually getting this done. Increase nf_posdim to 2 from 1. Fix the order
as 0 -> 1, -1 -> 1 for the position data. Then fluxdatatest still works.
(But is not yet general.) Save the dimensional structures of the
different quantities (two ints each) as well as the totals. Then we can
in principle use different grids for each quantity although at present
we can't set them to be different through the input file.

Ok. We now seem to have two dimensional accumulation grid setting through
the input file for spheres. But not yet the actual accumulation.
Rewrote the objsect code for spheres to do accumulation. Seems correct.

Try to test with different velocities. Sort of works apparently.
 time ./ccpic -s100 -v1. -ni100000  real    0m15.954s
 time ./ccpic -s100 -v1. -ni200000  real    0m30.662s
 time ./ccpic -s100 -v1. -ni400000  real    0m59.150s
Time scaling is linear in this number of particle range. (Small grid).

Got accumulation of multiple objects working and corrected the mapping
back and forth from flux objects to geom objects. Also got fluxdatatest
to work with multiple object accumulation. 

Conclusion for today
---------------------

We have 2-D accumulation of flux working. We ought to add momentum and
energy. We might want to sign the accumulation, that is, subtract or 
add according to the direction of surface crossing.

8 Sep 09

Made mf_quant equal to the fluxtype. Sensible choices might then be
1 Flux only. 4 Flux plus momentum. 5 Flux, momentum, energy. 

Found a slight problem. mf_quant needs to be set for each object, and
so needs to be an array. At present it is only a scalar. Looks doable,
but needs follow through.

10 Sep 09

This done and the flux reduction also extended to cover all quantities.
At present assuming that the grid is the same for all quantities. 
MPI seems to be working. Commit.

Edit fluxave so that it can be told a particular quantity to average
and plot. Include reporting of average flux in ccpic for objects that
accumulate it.

14 Sep 09

Implementation of maxwell stress calculations.

Having thought about the best way to implement, my idea is that calculation
is best described for a general object by a set of surface elements. 
We regard the object's surface as being approximated as a set of facets.
Each 3-D facet has
     A center position (3 reals) P.
     A normal direction and area (3 reals). A vector in the normal direction 
       with magnitude equal to the surface area: A. 

For general number of dimensions, these just become ndims+ndims.

Then the total force on the body due to Maxwell stress M is 
     Sum_{surfaces i} M_i.S_i,
where M_i is the stress at position P_i.

The nature of the approximation is left open by this datastructure, and
can be chosen appropriately. Obviously, though, the approach will work
best if the areas are approximately equal. 

For a sphere the division into cos\theta and \psi equal spacing will 
give equal areas, but it costs no extra to use the general representation.

If there are n_c cos\theta elements, the end positions are
   c_i=1-(2/N)i, (i=0,N) and the areas to be applied to element d\psi are

A_zi/d\psi = -[c_i^2-c_{i-1}^2]/2
A_yi/d\psi = (1/2)[arccos(c_i)-arccos(c_{i-1})
		-c_i\sqrt{1-c_i^2}+c_{i-1}\sqrt{1-c_{i-1}^2}]
and the optimal position to evaluate the stress for linear dependence on theta
is
	\theta = (1/2)[arccos(c_i)+arccos(c_{i+1})]
	x=r cos\theta, y=r sin\theta
[See notes]

These are referred to a plane of constant \psi, in which lie both A and x.
If there are M psi-positions, then \psi_j= \pi*(-M-1+2j)/M  , j=1,M. 
However, the area to be attributed to large \psi-angles is not given
correctly by r d\psi, because of curvature. Using a grid with cell ends at
	  \psi_j=\pi*(-M+2j)/M
we find that for A_xij and A_yij, the axes perpendicular to the polar axis,
and with \psi measured from the x-axis, the values to be used for d\psi are
cos\psi_j-cos\psi_{j-1} and sin\psi_j-sin\psi_{j-1} respectively. So

A_xij = (cos\psi_j-cos\psi_{j-1})(1/2)[arccos(c_i)-arccos(c_{i-1})
		-c_i\sqrt{1-c_i^2}+c_{i-1}\sqrt{1-c_{i-1}^2}]

A_yij = (sin\psi_j-sin\psi_{j-1})(1/2)[arccos(c_i)-arccos(c_{i-1})
		-c_i\sqrt{1-c_i^2}+c_{i-1}\sqrt{1-c_{i-1}^2}]

A_zij = -(1/2)[c_i^2-c_{i-1}^2] .d\psi

Also, the psi_cj value at which to evaluate the stress is \pi(-M-1+2j)/M.

15 Sep 09

In testing the simple field evaluator needed for the maxwell stress 
calculation, I find that there's an error in padvnc, in that the fractional
position was not always being advanced, which meant that the getfield
call did not always give the correct answer for the current position. 
Put an explicit evaluation of the fractional position into the move loop.
I don't really understand what was happening with the field evaluation
and why the errors seemed only to be in the high positions.

I think this is because partlocate is called in chargetomesh. But I see
now why there's a problem. The iteration of chargetomesh it carried out
only up to n_part. That's an error if there are empty slots. One needs
to go to higher slot numbers. That definitely needs to be fixed.
Change the iteration to up to ioc_part. That fixes it; and the update
to fraction is not needed in padvnc.

This has changed the flux. Therefore this error is significant in prior
code tests. They'll have to be redone. But anyway we now have the 
fieldatpoint code working and checked. Actually this error is not significant
in the scans I did because I used fixed particle number which does not
call this bug up.

17 Sep 09

Created stress.f that includes routines to handle the surface facet 
arrays. Code to create the arrays for a sphere has been implemented and
debugged. Tests include integrating the arrays over the surfaces to obtain
the surface area, and calculating the force on a charge/dipole configuration
in an external field. The convergence appears to be quadratic, as it should
be. However, moving the charge close to, and then outside of the sphere
leads to significant errors in the total force when the charge is close to
(within .1 of) the sphere. I guess that it is becoming the resolution issue.

18 Sept 09

Implement potentialatpoint. Tested in padvnc section. Tests in ccpicplot
are confusing because there the potential is measured outside the region.

Now we need a data structure for storing for each time step:
    maxwellforce(3)
    pressureforce(3)
and maybe other field combinations such as
    charge (integral of normal field over surface).
For each object that we are tracking.

This bears some similarities to the ff_data structure (e.g. maxsteps, obj). 
However, that data structure is rather opaque. It does not seem very 
helpful to load it up even more. Rather, it would seem sensible to 
define some linear arrays of length maxsteps (or small e.g. ndims transverse
dimension arrays).

Currently we write the whole potential phi out at the end of run. To
write it out every step would be rather crazy in terms of size. I can
imagine that we might write out some sample of phi. The maxwellforce and
pressure are more manageable. 

It seems reasonable to assume that we are going to want the field quantities
only for objects for which we are doing flux tracking. That would argue
for putting the field quantities into the ff_data or at least use the
nf_map information. Perhaps that's the compromise: use the nf_map info,
but create different data structure. This might then be
    stressdata(nsdatas,nf_obj,nf_maxsteps)
since nf_obj=5, and ndatas is in the ball park of 10, this is
an array of size 50,000 for maxsteps=1000. This is negligible. 
In the end probably easier to have separate names fieldforce, pressforce
etc in 3dcom.f.

Having made that decision it is natural to put the output into the flux
file. At present, we are just writing the whole fieldforce, pressforce,
and charge traces, regardless of used length. 

In addition we need the surfobj structure for each object being tracked.

Implemented and got to run sphere treatment only. Not yet debugged to
my satisfaction. We need some tests. 

I'm a bit confused about the flux tally type/number. Check reading codes.

19 Sep 09

The first byte of the 3dobject input (otype) is the object type 1: sphere.
Second byte is currently being used for special boundary conditions.
1:256 is used for setting phi to zero outside instead of continuity. 

The input ofluxtype is used to indicate the number of flux quantities
to be tracked, currently <=5. I don't know if this is the best use of it.

Although it is a bit inconsistent to track stresses if we aren't tracking
momentum fluxes (tally 2-4) probably it is reasonable to say that we 
track stresses if obj_geom(ofluxdata,i).ne.0. 

Got worried about the analytic testing on the 0th step. This gives
a big error unless lambda debye is very large. Yet it is a call with laddu
equal to zero. So the solution ought really to be a good vacuum solution,
since charge is zero. Checked out with older version of sorrelaxgen.f.
Same result.

I figured out that the change is in the boundary condition at the
outer sphere. Orbitinjnew.f sets it different from the natural vacuum
condition in geominit(myid). So this is actually a vacuum solution,
but with effectively a potential offset that allows the outer BC to be
satisfied.  The analytic comparison does not account for this
potential offset at infinity. To make a proper analytic comparison, I
would have to pass also the value of phi_infinity. The boundary
condition is a phi + b phi' + c =0. The values of abc are available in
obj_geom(oabc,2). A vacuum solution phi= p0/r + pi satisfies this BC if
a (p0/r+pi) + b (-p0/r^2) + c =0. Which means that the solution for pi 
must be 
     pi = -p0/r - (c-b.p0/r^2)/a 
but then p0=(phip-pi)*rc. So really
    a ((phip-pi)*rc/r +pi) - b(phip-pi)*rc/r^2 + c =0.
So 
   pi( -a rc/r + a + b rc/r^2) = -(a phi*rc/r - b phip*rc/r^2 +c)
It works.

Found that fillinlin works slightly better if we always use the field
at point to determine the gradient of the fit, rather than doing a
full gradient fit to adjacent points. Sort of makes sense.


25 Sep 09

Implemented calling of tallyexit for all boundary crossings. Only those
that are being tracked actually do anything. But now we can put a surface
in the particle region and track the momentum flux across it. Therefore
we ought to be able to compare the force components for different objects.

7 Oct 09

Getting back to this. I had updated fluxdatatest to examine the force 
integrals. It shows that for l=1, the total force is very similar at 
r=2, 4, but not the same at r=1. So there's a discrepancy not yet sorted
out. 

Run case with -l10. in which the dominant force is particles. We do
not appear to have momentum conservation. The different radii give
very different force totals. Hmm. Also, there are some Tallyexit no
intersection errors. The force at r=4 is almost twice that at r=1.  We
ought to have a delicate balancing problem with field force and
particle force. This is not working properly.

Case with (nearly) zero potential on sphere and -l200. Gives a small force
that is equal for all radii, as expected. About 3.68 for partforce. So 
momentum accounting is working when there are zero fields. 

At present, partforce is in units that are normalized momentum per sec
per rhoinf. Fieldforce is in units that are grad(\phi)^2 times
area. pressforce is in units of nT times area (I think).

Multiplying fieldforce by debyelen**2 there is a balance. But it seems
to be closer to 0.5 times the fieldforce. I need to check the coefficients. 
There is also some 9% discrepancy at the inner boundary, which needs to be
investigated. It sure looks as if 0.5 is required.

8 Oct 09

Partforce was not being reduced. Therefore for two processes, the partforce
was half of what it should have been. I think this probably explains this
factor of two discrepancy. But we still need to sort out the comparison
with sceptic.

The particle force for ccpic is  90.14934, 119.96814, 175.66409, 195.43149
for radii, 1., 2, 4, 4.9.
The particle force for sceptic is 87.89902, 196.57240 for radii, 1., 5. 
Thus, we basically have agreement in the particle forces.

The field forces are quite different for the two codes. However, this is
probably because of the different phi boundary conditions at the outer
boundary. For large debyelength like this the cutoff is outside the boundary
so the code is not really determining the force, it is being imposed by 
the BC. The fact that the totals for ccpic 224.91051, 208.83559, 206.43303
197.83185, more or less agree shows that we are pretty much conserving
momentum in the code region. But these are very different values from the
ones being given by sceptic: 540.51074, 545.98761. 

It hardly seems worth the effort of trying to sort out what the
differences are. More useful might be to impose an outer boundary
condition that corresponds to a known field (e.g. uniform) for which
the EM force is therefore known. And verify that we get the right 
field force.

To do this through the BC would require conditions whose normal component
varies over the surface. Might be tricky. Perhaps we could simply add
a uniform electric field to the field calculation. It will still satisfy
the same poisson equation. It won't then have the total potential constant
on any surface (including the inner). Probably does not matter. 

Implemented this external field. The inner surface error remains. 
The error on the inner surface with our current mesh (32^3) is about
halved at a position r=1.05, and does seem to make a finite transition
to the conserved region.

Probe potential 10. extfield.01
Field,part,press forces: -2937.02954     4.97746     0.00000 -2932.05200
================== End of Object 2 ->  3
Field,part,press forces: -2996.83130     3.71953     0.00000 -2993.11182
================== End of Object 3 ->  4
Field,part,press forces: -3087.99609     1.97120     0.00001 -3086.02490
================== End of Object 4 ->  5
Field,part,press forces: -3081.75708    -3.63166    -0.01428 -3085.40283

Probe potential 1. 
================== End of Object 1 ->  1
Field,part,press forces:  -294.28650     0.67058     0.00337  -293.61252
================== End of Object 2 ->  3
Field,part,press forces:  -300.21112     0.44164     0.00290  -299.76657
================== End of Object 3 ->  4
Field,part,press forces:  -308.82449     0.03669     0.00001  -308.78781
================== End of Object 4 ->  5
Field,part,press forces:  -307.54568     0.12403    -0.07416  -307.49582

I find that the charge is unequal in exact proportion to the fieldforce.
In other words, the main force error is arising from the integral of the
normal field over the surface, which is not giving exactly the right answer.
This is understandable, given the approximations that are made close to 
the sphere.

Thus I think we have the force calculations working, but they are showing
that positions very close to the sphere suffer from the approximations that
are involved in the field extrapolation. The charge on the inner sphere
is systematically underestimated by the extrapolation. 

Since field is accurate only to a linear approximation, and we are
using a mesh that has 16 across a radius of 5, we have a mesh spacing
of about 0.31.  The inner sphere has radius 1, so the spacing is about
1/3 of the sphere radius. It is perhaps not surprising that there are
substantial symmetric errors in the field that affect the force.
I'm not sure there's really anything one can do about it.

Fiddling with fluxdata to see what I should do about tallyexit errors.
They seem to be rounding. The tests I have don't make much sense.
Changed to a more sensible test and crossing direction calculation.


15 Oct 09

Playing with comparing ccpic with sceptic, I realize that I still
haven't really got good solutions for storing and analysing the ccpic results.
Sceptic postproc has contour plots of n, \phi. Line-outs, flux density plots
etc. ccpic has phiexamine which can do plots of \phi, but n is not well served.
Also, fluxdatatest has some plots but as yet no output useful for comparing
with other data. 

Do we need density to be output separately from the particle data? We can
certainly calculate density (instantaneously) by reading in all the particle
files. But it might be pretty cumbersome doing the accumulation. Actually
chargetomesh seems just to accumulate, not to initialize. Therefore we
just need to call it for as many particle files as exist. However, the particle
data files do not contain the mesh structure.

Easier route just reproduce the phiexamine structure as
denexamine. Including writing out the density. This works. But there's
a serious problem in that the noise level of the density is very high,
at least on 32x32x32 with default particles. The noise level is of
order 1. This is, of course, no different from what one might observe
with graphics during the run.  The files for density (and phi) are
about 15 times smaller than one file of particle data (for 52k
initialized particles). So it might be worth considering outputing multiple
steps. One might also do time averaging. Implemented averaging and output
the decaying average with 100steps default. Even so with the standard 
settings the density plot is pretty noisy, and with 2 processors, added
steps and 200k particles there's still a considerable noise level.

16 Oct 09

I've plotted the density as a function of r for individual mesh
points. It would reduce the scatter if the 2-D case were gathered together
into bins that correspond to theta angles.

20 Oct 09
Completed additions to phiexamine, denexamine, which bin things together
and then print them out. 

Created a general purpose plotting routine that can read the print out from
ccpic tools and from the sceptic tools. Compared the -p-10, -l1. cases.
Excellent agreement with potential and density right out to the edge.
See in src/plotting/.

Conclusion.
__________

I declare success with the comparisons between ccpic and sceptic in
moderate debyelength cases. They have excellent agreement on flux,
potential, and density for v=0 cases. Forces for flowing cases need
more study because of the finite-domain effects. Also for -l.1 there
is a significant discrepancy at the boundary. The potential is too
small in ccpic and the density is rapidly trending up towards 1. It
looks as if there's a problem in the boundary condition. However, it
is hard to be sure because the discrepancies in average values are
several times smaller than the instantaneous fluctuation levels.


Next Steps   30 Oct 09
---------- 

I think we need to move away from the spherical region and use cartesian
region out to the boundary. We need to work on such facilities as periodic
boundary conditions, or at any rate rectangular BCs at the computational edge.


-----
Did some housekeeping to clean some testing code out to the subdirectory.
Also rename fluxdatatest to fluxexamine. 


11 Nov 09

Renamed bbdyroutine.f to bdyroutine.f. Cleaned it up so that it is easier
to understand the boundary setting routine. 

'Free' rectangular boundary conditions. Might consist of putting the
logarithmic derivative equal to 1 or 2 or something. But on a
rectangle the logarithmic derivative is not so obvious. If r.grad(u)/u
is the radial logarithmic derivative D, then the component in a
particular cartesian direction x is grad(u).x = D (r.x) u/r^2, so
presumably we should set grad(u).x accordingly. Notice the 1/r^2
dependence involves the orthogonal coordinate values. Consequently, one
needs ways within the bdy routine of knowing what the mesh positions are,
or else some other more direct way of passing the setting information. 

Implemented that in fieldtest and bdyroutine bdysetfree. But needs a
different geomtest file: geomtest2.

Also geomtest3 has two separate spheres.

24 Nov

We need now to deal with the outer boundary for particles, when there
is no explicit bounding object. Particles contribute their weight to
the second row/column until they reach the exact edge node
position. Consequently, we ought to consider the particle region to
extend all the way to the edge nodes, if we wish to get the second row
density correct. The contributions to the edge nodes is half what it
ought to be because there are no particles outside. However, the edge
node density is never used, and indeed the volumes of the edge nodes
are not currently set. There will never be an overflow of index for
charge assignment beyond the edge, provided that we never go past the
edge nodes.

In summary: the default particle boundary is the edge nodes. 

The routine insideall is used by padvnc. This approach needs to be 
extended to include the default outer boundary. Currently the mesh is
set using xmstart/end, but there are no switches to change those. They are
+-5. Also rs is in the common, and used for a few things. We need to change
to using the real mesh size, which is set by meshconstruct. 

Relevant routines.  

	 readgeom reads in objects

	 geominit sets rc from first, rs and rsmesh from the second object
	 assumed to be a sphere, and overrides some BC, from orbitinjnew.f.
	 This will be changed when we are doing away with outer sphere. 

	 meshconstruct accepts xmstart(3) xmend(3). 

Implemented command line switches to change xmstart/end. A problem emerges
that stored3geom is not changed, even though the mesh has changed. Because
of that ccpic thinks the volumes are what they used to be, and does not
recalculate. I think we need to store the mesh data in the stored file. 
Implemented storing and checking of ixnp. Fixed.

Crunch of geominit prevented by softening the oabcset error. 
Then pinit hangs. So we can fix this. We need to fix linregion and
then fill up between mesh limits. Actually I don't think there's any
need to edit linregion. I find that the issue is that rs is apparently 
zero. Anyway, dispense with using it by making pinit fill the whole mesh
region with particles in so far as linregion is true.

Now we get as far as stepping and find Relaunch NaNs. 
Found that problem due to rs being reset to zero, and fixed.

It seems as if the best approach to keeping particles within the mesh is to
use the partlocate call. This is tricky because at present partlocate
is called by chargetomesh. But this separation just makes the code harder
to understand. The best thing is to get all the partlocates into padvnc
and ensure after every move, we do the partlocate so the position is in
fact known. To remove partlocate from chargetomesh, we perhaps need
to call it for all the particles when they are initialized. We do, but
somehow this is still not working.

30 Nov 09
Was not setting the pointer iinc properly. Fixed, but now the calculation
of the pointer to position (iu) in partlocate appears to be redundant.
It is not used in padvnc, only in chargetomesh, where it had to be 
done separately. So purge out of partlocate the calculation of the 
pointer. Simplifies and reduces argument count.

Now we need to implement some convenient way to detect when partlocate
overflows the interp. Perhaps a specific large iregion. No. Add a logical
argument. Then test this. Implemented.

1 Dec 09

Putting Mesh information into the geometry file.

It becomes clear that we ought to put the mesh information together with
the object geometry information. Presumably in the same input file.
A general way to specify the mesh information would be to provide for 
each dimension two flexible length arrays:
imeshstep: 1, ms2, ms3, ..., msN
xmeshpos:  xm1,xm2,xm3, ..., xmN
which determine the mesh number corresponding to mesh position.
Thus imeshstep=1,32; xmeshpos=-5.,5. corresponds to a uniform 31-step 
mesh 1-32 over domain -5. to 5.
imeshstep=1,12,20,32; xmeshpos=-5.,-1.,1.,5. is mesh with 3 domains 
1-12 covers -5. to -1.; 12-20 covers -1. to 1.; 20-32 covers 1. to 5.
An imeshstep value of 0 indicates no more array. 

To convert this to the meshcom ixnp and xn values requires that the 
dimensions be known in the correct order (x,y,z,...) so one has to set 
them all (perhaps to defaults) and then do the conversion. 

Implemented structure and default initialization for imeshstep and xmeshpos.
Implemented new meshcontruct to use that default. Gives same answer.

Format in the geometry file will be for dimensions 1,2,3:

91/2/3, ms1,ms2,...,msN,0,xm1,...,xmN,0

Seems to work.

Now trying to rationalize things, we find a logical problem. 
iuds is now set by meshconstruct. That has to be done after we 
have determined myid. But the initialization of sormpi discovers myid,
and it needs to be passed the value of iuds. 

I think this requires me to get rid of the early initialization, which 
was anyway very artificial. Instead I can use a routine getmyid to
get it. This does not seem to need any changes to sormpi. Yes it works
nicely and easily. This is a much better approach. Also got rid of
xir the point assumed to be in the region. Now just use linregion.

Don't seem to have gotten to the bottom of the difference with or
without stored geom. There is a call to ran1 that ought to reset it.
Thus it's not clear that the difference arises from the random
numbers.  However there did seem to be a difference in pinit: 100776
when set, vs ntries= 100414 when read back. So if the random number
generator is not this issue what is? 

Ahh! It WAS the random number generator, because pinit was erroneously
using the old rand instead of ran1. I use rand() _only_ in volintegrate
because of the big cost there of number generation. So now fixed. 
We get the same answer whether or not the stored3 read is successful.

Today
_____
We implemented mesh specification in the geometry file.
Rationalized some of ccpic, notably the mpi initialization. 
Fixed a bug in the pinit call and stored geometry differences. 

Discovered a bug in slicegweb to do with non-equal side domains. Actually
it's a feature. Hidweb is set not to rescale until done explicitly.

2 Dec 09

Now we need sensible reinjection for the whole rectangular domain.
Issues include
1. How to account for any non-zero potential at the domain edge?
2. Are there many interesting situations where periodic reinjection
   would be useful?
3. How to deal with situations where some part of the boundary is outside
   the particle region. 

One thought about 1. is that one might use reinjection at infinity followed
by integration in an approximated spherically symmetric potential. The same
approach as in orbitinject. One would have to use an impact parameter range
that extends to the boundary of the cuboid region. This is substantially 
larger than the largest sphere that fits within the region. Radius is larger
by sqrt(3), so impact parameter area is larger by 3. Consequently there
would be substantial (factor 3) inefficiency in the process. It might also
be tricky to determine whether the orbit intersects the sphere. 

Probably the use of the Green's function solution for this purpose is
overkill. Certainly it will not arise naturally for the actual boundary
conditions of a cube. Other issues arise if the domain is a very elongated
cuboid. Then the spherical symmetry is a bad approximation (perhaps). But 
in any case the injection inefficiency will rapidly increase. 

Overall, it seems unlikely that an orbitinjection approximation will have
sufficient accuracy to warrant the trouble. A straight approximation of
cartesian reinjection of the infinite-distance distribution function ought
to be pretty easy, but will not account for external acceleration. 

Options for ad hoc adjustment of the injection to correct for external
acceleration include 
1. Adding normal energy. (Enhanced normal inward velocity).
2. Adding total energy. (Keeping the velocity direction fixed.) 
3. Some other directional assumption.

Should one change the distribution across the boundary? If one were
in a quasineutral regime, then perhaps the density ought to be weighted
by the Boltzmann factor. One way to do that would be to roll the dice
and reject the particle if above exp(\phi), where phi is the local
reinjection potential. This would appropriately contour the weight. 
However, it would not be correct for a long-Debye-length situation. 

 Periodic Reinjection. Even if periodic reinjection were used for
particles escaping the domain, one would still have to deal with particles
lost to the objects, and their reinjection. Therefore periodic reinjection
does not actually solve the problems of reinjection. It could be an
option for some (or all) of the external boundary surfaces. It could be used
whether or not the potential boundary conditions were periodic. 

The first thing to do is to implement a shifted maxwellian calculation.
If 
   f(v) = C exp[-m( v - v_d)^2/2T]
then the total number with positive v-direction is
   N_+ = C \int_0^\infty exp[-m( v - v_d)^2/2T] dv
       = C \int_{-v_d/\sqrt{2T/m}}^\infty exp[-t^2] dt \sqrt{2T/m} 
       = C \sqrt{\pi}/2 \sqrt{2T/m} erfc(-v_d/\sqrt{2T/m})

Since erfc(0)=1, erfc(-\in+fty)=2, the normalization is such that \int f dv
= 1 if C \sqrt(\pi}/2 \sqrt{2T/m} = 1/2.

The flux density above velocity v_0 is 
    F(v_0)  =  C \int_v_0^\infty exp[-m( v - v_d)^2/2T] v dv
       =  C \int_{t_0-t_d}^\infty exp[-t^2] (t+t_d) dt {2T/m} 
(putting\ t=(v-v_d)/sqrt{2T/m})
       =  C2T/m [-exp(-t^2)/2 - t_d \sqrt{\pi}/2 erfc(-t) ]_{t_0-t_d}^\infty
       =  C T/m [exp(-(t_0-t_d)^2) + t_d \sqrt{\pi} erfc(t_0-t_d)]
       = \sqrt{T/2m} [exp(-(t_0-t_d^2))/\sqrt{\pi}  +  t_d erfc(t_0-t_d)] 

Approach: 

Decide the face to be injected at, by random choice weighted by F x Area. 
Decide the position on face by random 2D rectangular.
Decide the tangential velocities using gasdev (drawing from maxwellian).
Decide the normal velocity by drawing from the above distribution:
       how do we do that distribution draw?

We need to be able to invert F(t_0). 
That's not obviously feasible analytically.
Seems as if we need to form a numerical table of F(t_0) and interpolate
the inverse lookups. This would require six tables: one for each surface.
But at least they only have to be one-dimensional. 

7 Dec 09

Created integrator code. Programmed a fairly smart integrator
subroutine cumprob. 

8 Dec 09

Programmed reinjection code.
Programmed testing/creintest testing code.
I find that the histogram reinjection distribution shows up on the 
testing distributions. Thoughts about fixing that include the idea
of some higher-order interpolation. However that is not straightforward
at the edges, where it most matters, because the inverse cummulative
probability curve has infinite slope there, so how does one fit that? 
It's not clear that a parabolic interpolation will work. If I add a 
further node outside the real range, which has a value much (by some
chosen factor) beyond the bottom. This might work. On the other hand it
probably won't be faster than simply using a finer mesh for the inverse
cumulative curve. Using 1000 arrays seems to give a pretty good result.

9 Dec 09

Constructed (finally) a four point cubic interpolation routine. Checked
numerically. However, when used in creinject, it makes things worse. There
are spikes near the last but one point. Don't know why. Perhaps it means
that the interpolation is non-monotonic (or nearly so). This is not 
completely crazy, since a parabolic interpolation of a sharp corner from
nearly flat to nearly vertical will produce overshoot. This may be
the reason. At any rate it appears to confirm that it is very tricky to
improve the inverse-cumulative approach by higher order interpolation. 
I think it comes down to needing either to use a decently high number
of nodes, or not using the inverse cumulative approach. The old finvtfunc
approach was not bad. Settled for 1000 array for now. Linear interpolation.

10 Dec 09

Finished creintest.f to include testing of position. Seems correct.
Now we need the cartesian versions/equivalent of nreincalc, geominit,
rhoinfcalc.

Implemented nreincalc and rhoinfcalc using 
flux = ninfty sqrt(2Ti/pi)[A_1+A_2
       	      +A_3{exp(-td**2)+sqrt(pi)/2 t_d[erfc(-t_d)-erfc(t_d)]}]
and including a phirein correction factor of
         cfactor=smaxflux(vd/sqrt(2.*Ti),chi)/smaxflux(vd/sqrt(2.*Ti),0.)

(Not yet particle energy correction.)

geominit is specific boundary setting for this geometry. Null for now.
It compiles.

It would be nice to have geometry files ccpicgeom.dat that were automatically
invoked for the different geometry assumptions. Might be possible to 
replace ccpicgeom within ccpic with a value that depends on the REINJECT
case under consideration. Better to build a link into the make structure
and change it according to whatever reinject we are using. Did that and
cvs added geomsphere.dat and geomcubic.dat. Cartesian reinjection is
actually not yet working. We aren't reinjecting anything. 

Made some progress on getting things to run. But not to the bottom of it
yet. denexamine and phiexamine don't run quite right.

11 Dec 09

Ok the main problem seems to be that the sign of the normal velocity
is wrong and so particles leave immediately. Fixing this, there is still
a small problem with sign of particles because the inverse cumulative
probability extends slightly across zero, presumably because of the
integration and interpolation. Consequently there are a few wrong signs
still. I suppose we could fix this artificially. Did that.

Now things run with sensible particle numbers but there are some very 
strange peaks in the density that make no sense. And seem to be in straight
diagonal lines for some number of cells. This was because I used idum as
the argument for ran1. It needs to be >=0 for standard call. And idum
was uninitialized.

Finally got some runs that look plausible, although currently with
potential set to zero at the boundary (the current default). Actually
no. The potential is set to zero on two dimensions and something else
in the third.

Cartesian boundary flux tests. 

32^2 grid ./ccpic -ni200000 -dt.025 -da5. -s500 we get rhoinf 212.67125
Flux density*r^2, normalized to rhoinf   3.9655340. This is to be compared
with 3.997 with sceptic or with 60^2 grid. Broadly we are getting the 
right answer. Close to OML (4.388 for -p-10).

With -l.3 we get: 2.8120570 c.f. 2.77 with sceptic etc. All pretty promising.
I see that the printout from the run is different from what fluxexamine
prints out 2.8006816, but close. Not quite sure why. 

With -l5. I don't expect a good result, because of the BCs. Get 4.795. 4.777.
C.f. 4.395 sceptic. This is 10% in error. Not a great result. 


Summary
-------
Code seems to be working with outer injection boundary at the mesh edge.
Gives promising agreement with prior runs at -p-10, even with relatively
small (32^2) mesh (and inappropriate boundary conditions). 



14 Dec 09

Implement boundary setting of slope to -(1.+r/\lambda_s) with lambda_s
currently set as the linearized value. This works reasonably well to
maintain the phi contours circular at intermediate values of debyelen,
and presumably is close to the right answer for large debyelen, although
arguably it ought to be closer to -2, not -1 in the far distance. 
At small debyelen, the value of the slope doesn't much matter.

Then ./ccpic -ni200000 -dt.025 -da5. -s500 gives
3.9668663. Essentially no change. -l5: 5.2031398 yikes! Too large by
big factor (rhoinf=135.45, 8818). Try logarithmic slope of -1.: 5.3926058
(rhoinf=122.93, 8330) . Also very large. Repeat old bdy run: 4.7955675 (this
forces phi to nearly zero at the boundary rhoinf=161.79, 9714).

This is rather worrying. It looks as if one gets a very bad result if
the boundary potential is non-zero. I think this is because I don't
yet correct the energy of the reinjected particle by phirein (or some
other local potential correction). Scale velocity by
(v^2-2.averein)/v^2. Initially I used phirein and got problems with
overrunning the diagv. This is because phirein is not equal to the
averein of the prior step in the middle of the particle advance.

Now with -l5 get 3.6797631, rhoinf 149, 6897. Too small by a comparable
amount. -l1 3.8620880 214.66699 10418.302
-l.3 2.8107293 219.31303 7746.276

What I think this experience shows is that the cartesian geometry is
not very good for treating an isolated spherical object. Not because
of the cell shape but because there are no convenient approximations
for the boundary condition that allow one to take credit for the 
isolation of the object, and not have to go as far out with the domain. 

If the domain were part of an array, so that periodic boundary conditions
apply, then this weakness is turned into a strength. Probably periodic
conditions ought to be investigated. 

To get into a physics problem soon, we could address the wake issue. This
would also lead to exercising different meshes and perhaps even differently
spaced meshes.

For now, commit. Getting back to sceptic machine we don't get the correct
answer. This proves not to be a function of the cvs version because getting
it to ihhutch gives ok result. I notice that although there's a gfortran
on sceptic.psfc mpif77 appears to be using g77 instead. This might
conceivably be the cause of the major problem. It is that face of the
grid is being significantly depleted of density. Perhaps there's a problem
with reinjection, e.g. that no reinjection is occurring there. In any
case it is probably worth pursuing the problem on sceptic.psfc to figure
out the portability of the program. 

Prove that it really is the mpif77 version by using the -f77=gfortran 
switch. That gives correct answers on sceptic.psfc, although not _exactly_
the same as on ihhutch. At least the major error has gone away. The version
of gfortran is not the same. It seems there are very small differences. 

It is by the way noticeable that the g77 version runs very substantially
faster on sceptic.psfc. Maybe as much as a factor of 2! That's pretty 
significant. 

The depletion of the g77 version is at the -5 end of dimension 3. It appears
gradually with steps, which suggests that perhaps it is associated with 
reinjection; but it could be other things. Since we don't have obvious 
code errors, it is not completely obvious how to proceed. There is no
evidence that creintest gives any difference. 

A force shows up in the faulty version. And eventually a significant
flux asymmetry. There's definitely a particle flux towards negative z,
which is consistent with a reinjection deficit there.

The faulty version shows substantial n_part degradation after 100 steps:
0096  53 3.758| 0097  54 3.228| 0098  53 3.565| 0099  53 3.767| 0100  54 4.023| 
nrein,n_part,ioc_part,rhoinf,dt= 2394  76849  99993    94.749     0.100
compared with
0096  53 3.882| 0097  53 4.035| 0098  53 3.875| 0099  53 4.144| 0100  53 3.888| 
nrein,n_part,ioc_part,rhoinf,dt= 2394  91379  99984    97.415     0.100

Thus there really does seem to be a loss of particles. Got g77 installed
on tp400 and then I can reproduce the g77 problems there (and its speed!)

16 Dec 09

The reinjection diagnostics do not show any asymmetry problem in cos\theta.
Nor are any warnings given about reinjections outside of region. Therefore
there's a bit of a puzzle as to where to look for the error. 

Perhaps we need a way to examine the particle data and plot
distribution functions etc. This would involve reading back the
partdata and then binning it by velocity, for some selection of
cells. One ought to allow the possibility of reading more than one
file worth of data. But it is not straightforward to provide
sufficient storage for the actual data of many particle files. Each is
potentially quite a lot of megabytes. Also the file reading apparatus
might break unless we shuffle stuff. It might be better to reread the
files if a new selection were required. That would be reasonably quick
if the files were in disk cache memory. So the process would be
1. Read a file .00N 2. Bin its particle data. 3. Repeat till all done.
This could be done on a selective basis (perhaps). 

Created partexamine code to do this. It shows that in the depleted region
at the bottom z end, the distribution function is (approximately) 
one-sided. Also, density goes to zero at the edge, not half, because
the particle bins can be chosen finer than the charge mesh. And starts to
drop at about -4 (which is about 3 cells from boundary) after 5 steps.
This effect can be seen propagating inward as one increases the number of steps.
If we set the number of steps to zero, there is no depletion. 
Setting dt to a small number we can see more clearly the depletion region.
It is the bottom _half_ of the bottom cell. 

Putting a test hack padvnc call outside the main loop and using -s0,
I find that it is definitely padvnc that is causing the depletion.
It seems that x_part(3) is corrupted on entry to partlocate.
Also xpart(6) is corrupted and seems to be the source of the corruption.
Yes this is coming from the getfield, and is corruption of the value.

I think the problem is in field interpolation at the boundary. We are
not (it seems) setting the boundary pointers in such a way as to
prevent trying to get potential from past the end of the mesh for
interpolation.  Actually I can now see the problem from within -gt
tests.  I think this problem would be fixed if the boundary points
(which all have pointers to data) all returned a region that was
unique. getfield would then never allow itself to use those boundary
points as the centers of extrapolation. As a result, it won't grab
data beyond the boundary.

Implemented this different boundary value as -1. Also fixed the
text3graphs to cope. Then found a logic bug in the getfield code that
did not treat the whole thing appropriately. Found a bug in the
solu3plot routine that overran the mesh domain at certain angles and
hence gave nans returned for the field. Have not made getfield
completely bullet proof against incorrect fractions in call, because I
was nervous about slowing it down. But now things ought to work.
Ok. No obvious evidence now that we are broken.

There's an inconsistency in rhoinf, n_part, and nrein. With multiple
processors, we initialize 100000/numprocs particles by default. Which
is probably a bad choice, but then also we seem to be injecting more
particles. The defaults are as follows:

17 Dec 09

c Fixed number of particles rather than fixed injections.
      ninjcomp=0
      n_part=0
c Default to constant rhoinf not n_part.
      rhoinf=100.
Then in initialization we do
c      write(*,*)'Doing nreincalc',n_part,rhoinf,dt
      if(n_part.ne.0)rhoinf=0.
c Set ninjcomp if we are using rhoinf
c This does not work until after we've set mesh in cartesian.
      if(rhoinf.ne.0)call nreincalc(dt)
Nreincalc does:
c Correct approximately for edge potential depression (OML).
      chi=min(-phirein/Ti,0.5)
      cfactor=smaxflux(vd/sqrt(2.*Ti),chi)/smaxflux(vd/sqrt(2.*Ti),0.)
      ninjcomp=nint(rhoinf*dtin*cfactor*flux)
      nrein=ninjcomp
      if(ninjcomp.le.0)ninjcomp=1
      n_part=rhoinf*volume/numprocs
where flux is normalized flux-density times area. 
Then we call pinit which does:
      n_part initializations (per proc) and then
c Initialize rhoinf:
      if(rhoinf.eq.0.)rhoinf=numprocs*n_part/(4.*pi*rs**3/3.)
which should be changed, for cartesian volume, incidentally. 
When nrein is reduced, it is **summed**. 
rhoinfcalc is called each particle advance cycle. nreincalc is not.
rhoinfcalc does:
      if(nrein.ne.0)then
c Calculate rhoinf from nrein if there are enough.
c Correct approximately for edge potential depression (OML).
         chi=min(-phirein/Ti,0.5)
         cfactor=smaxflux(vd/sqrt(2.*Ti),chi)
     $        /smaxflux(vd/sqrt(2.*Ti),0.)
         rhoinf=(nrein/(dtin*cfactor*flux))
      else
         if(rhoinf.lt.1.e-4)then
c Approximate initialization
            rhoinf=numprocs*n_part/(volume)
            write(*,*)'Rhoinf in rhoinfcalc approximated as',rhoinf
     $           ,numprocs,n_part
         endif

Thus nrein during iteration refers to the total reinjections, and
rhoinf to the total density including all processors, but ninjcomp
refers to one processor's injections, as (of course) does n_part.
I want the default injections to be rhoinf=100*numprocs really. 
But in any case, the initializations should be such that things are
consistent. ninjcomp is only ever set in nreincalc, i.e. only ever
during initialization.

Need to make a decision about how defaults are command lines work with
different processor numbers. I want adding processors to add particles,
which means that rhoinf goes up. Thus if I specify rhoinf on command line
or default the actual rhoinf is not this value it is this value times
numprocs. This is correctly handled within rhoinf at present if we 
change the meaning of rhoinf from being per processor to total when
we call nreincalc. Hence we change it to 
      ninjcomp=nint(rhoinf*dtin*cfactor*flux)  ; per processor
      nrein=ninjcomp*numprocs		       ; total
      if(ninjcomp.le.0)ninjcomp=1
      n_part=rhoinf*volume		       ; per processor
      rhoinf=rhoinf*numprocs		       ; total
This appears to work consistently. But ought also to be changed
in orbitinject. Did that.


Discovered a problem with testing/fieldtest. Points on mesh edge are
having their phi values changed, apparently to match the derivative
across the object mesh boundary. This presumably is not really needed.
Actually this happens also in the orbitinj version of ccpic. Although
you need to look harder to see it. Basically this is a problem with 
the bdyset routine, which ought not to do gradient setting in these
situations. 

Timing. 

G77       ./ccpic -s200 real    0m31.220s
Gfortran  ./ccpic -s200 real    1m10.779s

This is a really quite amazing discrepancy. Compare with 

./ccpic.gfortran -s50 -ni500 (negligible number of particles) 0m16.886s
 time ./ccpic.g77 -s50 -ni500  0m5.003s

 time ./ccpic.gfortran -s50 -ni500000 0m50.750s
 time ./ccpic.g77 -s50 -ni500000  0m45.196s
Hardly any discrepancy here.

Conclusion: Gfortran is taking 3x longer for the poisson solve, whereas
the particle mover is pretty similar for both compilers. (These are all
on concentric spheres.) The number of iterations is pretty similar.

Profiling gfortran shows time is dominantly in sorelaxgen mditerate,
bdyslopescreen, r2indi. There are 20 Million calls to bdyslopescreen.
Seems probably too much. For g77 bdyslopescreen and r2indi are also
the major players (though not quite so dominant. 19M calls. 

Is this reasonable? bdyset is called for each sor iteration (I think),
average 60. For each step 50. For all boundary points 32x32x6faces.
Total roughly: 18M. Yes this is a reasonable number. Seems as if this
boundary setting is not negligible, as I thought it would be. 
For 32^3 mesh the number of boundary points is only roughly 1/5th of total
number of points.

First thing. I don't think we should be calling the bdyset routine 
every step, only every _other_ step for the red-black routine. 
Second, one might argue that for the PIC the boundary routine ought
to be called only each particle step. 
Third, this is really scary because bdyset does not get shared out
between all the processes. So parallelism won't help. Scaling will
be terrible. 

I notice that the total number of seconds isn't adding to the elapsed
time. And actually the amount of time (about 2 seconds) isn't much
different. Maybe I'm not really profiling where the real time is being
spent. If that's so, then I'm probably not really getting to the bottom
of what is taking the time. I think I ought to be getting library routines
that allow profiling but perhaps I'm not.

On sceptic results are similar except the seconds count is 7 not 2.
Still the calls are more or less in the same place. Same with g77 and
with gfortran. It might be nice if one could compile without MPI, 
which might be some of the problem. 

Installed valgrind. The picture it draws is rather similar, but it detects
that there's a significant amount of time spent in the exp function.

There's no question that g77 is way faster -- up to a factor of 3 --
than gfortran, no matter what optimizations I use. Both are 64
bit. They use the same libraries except for libgfortran vs libg2c.
A real puzzle.

Profiling does not track libc libm without the profiling libs.
Installed them libc_p etc. But then can't get a program that does
not crash. This is a known bug:
https://bugs.launchpad.net/ubuntu/+source/glibc/+bug/193487

In general the flags required are  (-g) -pg -static-libgcc -lc_p
Installed gcc-4.1 which is said to work with these. But that does
not seem to propagate to gfortran. So I haven't really made progress.

Found a small time saving in sorrelaxgen where the parity change is done.
Saves about 10% of time in this routine, but not much overall. 

21 Dec 09

Made a unique version of accis and added to the repository as a subdirectory.

Found that even g77 version runs slower on cmodws48 etc. 
These machines have 
cache size      : 2048 KB
whereas both sceptic and ihhutch have
cache size      : 6144 KB
This is probably the reason. Cache misses may be the secret to understanding
the performance.

Tried reducing Li in ccpic. No significant difference. Reduce partcom
n_partmax to 100000: no significant difference. Well I haven't proved
that it is cache misses.

Ran some cases to see if I can get wake structures similar to those
in sceptic. With +-10 mesh extent, and -l3, -t.1, -v1., p-2, a trailing peak
can be clearly seen (but trough seems past the end. 
Generally the results seem qualitatively consistent with what I saw
with sceptic. 

Ran some rectangle cases. Also don't show negative dip in wake.
Ran an asymmetric case -5,+15. It shows a tiny dip in the wake. 
All this is consistent with sceptic. Now in 3-d.

23 Dec 09

Found problem with cumprob. There are cases when the integral is zero.
We need ways to cope with those, and also with cases where the integral
is too small to be significant, but still appears non-zero. Implemented.

30 Dec 09

Ran a number of cases designed for wake studies on sceptic.psfc.
These show quite oscillatory wakes (unlike the sceptic case). Using 
-l10 and -l20 one gets quite similar scaling with domain size up to 
400 long (50 wide). However, it is very clear that the size of the first
potential peak never gets much larger than .15, no matter how strong the
perturbation (charge \propto \phi x r scaled to lamda_debye). It seems 
as if I can actually reproduce the oscillatory wakes of the linearized
cases, but only with correct amplitude if that amplitude is less than
0.1(Te). It would be nice to do some really quantitative comparisons. 

1 Jan 2010

Investigating partial edge error with zero-derivative boundary condition.
I find this emerges only with multiple processes, but then with standard
cubic boundary conditions. Must be a bug in the communications logic. 
I believe that the edges are not communicated on principle. Why is that
a problem? 

The error seems to be arising in the final boundary setting after the
allgather. It appears to go away if the final boundary setting is
done twice. I think the logic of this is that, after the allgather,
boundary conditions are not set, so boundaries don't satisfy the condition.
Then calling bdyset implements the setting. However, at edges/corners, the
setting might be from an incorrect boundary value along a face to the edge.
In that case the edge value is incorrect after one setting. If a second
setting is done, then the extrapolation is from a value that is correct.
So the whole is then done properly. In general, I think this shows that
when a corner is set prior to the face value that it is extrapolated from,
it is incorrect. I believe this incorrect setting probably does not affect
things in normal operation, because the edge values ought not to be used.
But I'm not sure about that. In any case, we ought to fix the error. 
I've assumed that we can set an edge value from either of its faces.
That's not so. In fact sometimes we can't correctly set it from either,
because they have not themselves yet been set. This problem would not
exist if we could start from the middle of faces instead of the edges.
Maybe, but maybe not, I think we'd need to start at position 2s. 
Do the non-edge faces first, then do the edge faces. It doesn't look 
easy. 

Actually it is not very difficult to implement a more correct
algorithm, based on extrapolating from the nearest body (not face)
point. (Still iterating in the natural order).  It appears to be just
as fast and direct although recoding was necessary.

The run of the v10 cases was done with -t.01. I confused this with timestep
and did comparable runs with -t.1. These are much higher temperatures.
They'll be interesting. The old runs with -dt.1 are fine as far as
timestep is concerned. We've now got two temperatures.

2 Jan 2010

Working on padvnc to incorporate collisions in a compact way.
First clean out the testing code. 
Now reorganize the logic to make it more readable and comprehensible.
Localize the reinjection in one place (which requires logic adjustment).
This is all working. 

Now there appears to be a bug in that dtaccel=0.5(dt+dtprec). This is not
correct. It ought to be dtaccel=0.5(dtpos+dtprec). On reinjection, dtpos
is put to a random fraction of dtpos; and dtprec is put to zero. This means
that we are treating the prior step as of zero length, so that the position
and velocity are simultaneous. For the next step, we therefore want to
advance the velocity by half of the timestep (with nothing for the prior
timestep). But dtpos is the timestep, not dt. Correct this small error.

Then we can use dtpos as the remaining timestep in a subcycling associated
with collisions. 

Implemented both collisions and subcycling.

24 May 10
Got a problem with NANs in case with very large number of particles.
The Field corruption test was triggered. 
This turned out to be caused by initialization to a position exactly
on the mesh boundary, where the field was obtained incorrectly. 
Fixed partlocate to declare NAN position to be out of mesh.
Fixed pinit to ensure exactly on boundary is rejected.
Also fixed interp to reject NANs. 

28 May 10

Thinking about names. One idea was Cartesian Orthogonal Particles and
Thermals in Cell: COPTIC. It would be better if the O could refer to
the object boundary representation. (Since sceptic has orthogonal
coordinates.) Ideas: Oblique (referring to the boundary). I can't
think of anything better

Cartesian[-cell] Oblique[-boundary] Particles and Thermals In Cell: COPTIC 

Changed the makefile and the main filename.
Added to the CVS modules file the alias coptic CCPIC.
Committed. Now one does a cvs checkout coptic and gets the file with
the program name changed to coptic. There are some residual files with
ccpic as part of their name. Don't change them because it is too much
trouble.

Implemented a make feature that saves in REINJECT.f a single fortran
line that sets a character variable equal to the REINJECT configuration.
Then added a feature into coptic that checks if the particle region 
boolean has any objects that it is _inside_. If so, this is a 'bounded'
region. (Even if there is an unbounded region outside as well as the
bounded region inside.) If the particle region is 'bounded' and the 
injection scheme starts 'cart' (cartreinject.o), then this is an incompatible
scheme, and the code stops with an error message on initialization. 
This will probably be tripped only if the objectfile is explicitly 
specified and incompatible.

30 May 2010
Implementing more satisfactory potential BC. We want the potential to
be constant along a direction d which is a vector radial in x,y and
has a z-length v_d times the radial length. Implement this as a
boundary condition that set the boundary value to satisfy this
condition. One is probably safest deriving the tangential derivatives
from the values on the active mesh, one in from the boundary. The
normal derivative from the boundary point and its neighbor in the
active mesh. Should the tangential derivatives be centered? If so one
requires to go forward and backward from the point in the tangential
direction. This will work ok except for the edges. (Faces are ok.) At
the edges, one could just use an uncentered difference, but it might
be better to extrapolate from the 1,1 point diagonally in from the
edge point. If one does that, then an uncentered difference might not
be required.

We first analyse the current slopeD routine:
ccSet the cumulative registers
      r2=0.
      fac=0.
      ipd=ipoint
      do n=1,ndims
c BC is du/dr=D u/r     in the form   (ub-u0)=  D*(ub+u0)*f/(1-f)
c where f = Sum_j[(xb_j+x0_j)dx_j]/(2*rm^2), dx=xb-x0
c Thus ub=u0(1-f-D.f)/(1-f+D.f)
c Here we are using radii from position (0,0,..)
         x=xn(ixnp(n)+1+indi(n))
         r2=r2+x*x
         if(indi(n).eq.0)then
c On lower boundary face
            dx=xn(ixnp(n)+1)-xn(ixnp(n)+2)
cc ixnp(n)+1 and ixnp(n)+2 are the point and its adjacent.
            ipd=ipd+iLs(n)
cc ipd is the index of the point adjacent in the n direction.
            fac=fac+x*dx
cc accumulate dx*x, the projection of the step along the r-direction.
            if(n.eq.1)then
c The exception in step. Do not change!
               inc=iused(1)-1
cc This will remain the inc only if we are not on a 2, or 3 boundary.
            else
               inc=1
            endif
         elseif(indi(n).eq.iused(n)-1)then
c On upper boundary face
            dx=xn(ixnp(n)+1+indi(n))-xn(ixnp(n)+indi(n))
            ipd=ipd-iLs(n)
cc On upper ipd is the point adjacent in the -(n) direction.
            fac=fac+x*dx
            inc=1
         endif
      enddo
cc When we reach here, we have ipd equal to an index to the point
cc adjacent in each of the dimensions, positive or negative according
cc as that direction was lower or upper face, zero if neither.
cc This is the point from which we are going to extrapolate. 
cc We have fac equal to x.dx where dx is the delta-x to position ipd.
cc And r2 is r^2. fac is r.dr 
      if(ipd.eq.ipoint)then
         write(*,*)'BDY function error; we should not be here'
         stop
      else
         fac=fac/(2.*r2)
cc scale fac by r^2
         u(ipoint+1)=u(ipd+1) *(1.-fac+D*fac)/(1.-fac-D*fac)
cc The boundary point is equal to the adjacent times 
cc (1-f+D*f)/(1-f-D*f). Multiplying this out gives:
cc U_p(1-f-Df) = U_d(1-f+Df). I.e. (U_p-U_d)(1-f)=(U_p+U_d)Df
cc The radius to the half-way point is rh2=(x-dx)^2 = r^2-2x.dx=r2(1-f)
cc So rh2*(U_p-U_d)/rh2.dr = D(U_p+U_d)/2 
      endif
c^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

So that more or less makes sense. I want to use the same sort of
algorithm if possible. I can use exactly this algorithm to determine
the point from which we extrapolate. Then when we find a non-zero dx
for a particular direction, if it is in the z-direction we need to 
use the M scaling. The criterion we need to satisfy is: 
  du/dr + M du/dz =0
So we can use the x,y difference to estimate the extrapolated value
as: u_p = u_d + (dx.r + dy.r)/r (du/dr) + dz (du/dz) 
with du/dr = -M du/dz. 
So u_p=u_d + (-M*(dx.x + dy.y)/r + dz).(du/dz)
If the du/dz is measured at u_d we can do it centered.
On the z-ends, dx=dy=0. This is then an identity and we can't do it
centered, but if we do it uncentered, it puts d^2u/dz^2 = 0. 
Why can't we do that for everything? Might be better. Sounds
unstable. 

We must do the substitution for du/dz at the z-ends instead.
   u_p=u_d + (-1/M)(du/dr)dz and du/dr estimated from mesh.
This looks as if it might get problematic at low M. 

Things don't look very implicit and I am not convinced that I've got
the right sort of approach. To do it right we need at least one
tangential and one normal step at each point. And the tangential step
must be in the z-direction unless we are on the z-face. If we are on
the z-face, we could use two tangential steps. A simpler fall-back
would be to make du/dz=0 on the z-faces.

Implemented that: mach on x,y-faces, zero-slope on z-faces.

It's pretty hard to test on the big domains.  Perhaps I ought to do
something on a smaller domain.  It is not obviously broken, but small
numbers of steps give a potential that is rather offset.
Anyway it is now easy to swap between the old and the new boundary 
conditions. 

Removed randc.o from the objects.

Ran for 650 steps after a restart. Still seemed to have a rather
negative average potential. 

Doing tests on a small 5x5 domain. The direction of contours is about
right. So it seems the boundary condition is doing its job. However,
there are cases where the potential is very depressed, so it seems 
as if that's an issue that needs to be resolved. Actually this effect
exists even with the old BCs. It gets really bad at 500 steps for
-v.5 and the small domain. This is not unique to the new version. The
problem's been there for a long time. Probably I need to fix it. 
I see that there's a big particle pump-out from a starting number of
90k down to about 30k. That's a symptom. How is rhoinfinity being
calculated? is the question. This is badly broken. Setting constant
number of particles does not fix it.

First it seems the sign of slpD was incorrect. But still that's not
the main problem. Seems to be worst at low temperature. Bad also with
zero velocity. Problem seems absent at -t1. This must be a bug with 
rhoinfinity calculation associated with temperature. At -t.1 -l1. it
asymptotes to -.13Te. At -t.01 to -1.75Te.

Seems as if the problem arises from phirein, which is exceeding its
capped value because of the low temperature. This corresponds to a 
value of chi equal to 0.5. But no, just setting chi to zero does not
fix it. 

I think there's a major problem with the way that rhoinf is being 
calculated from flux. If phirein is substantially bigger than Ti, 
which it is for pretty much all small domains when Ti is small, then
we are going to get very big swings in cfactor. This is going to be
unstable, probably. But the whole approach is highly dubious, for
a rectangular domain anyway. If the presheath drop to the edge of 
the boundary is significant, then we don't really have a decent
way of determining the rhoinf from the flux, because we don't have
a decent way (for a rectangular domain) of calculating how much flux
is going to enter. The cfactor is mostly just a kludge. If I turn off 
the chi/cfactor, then we just get the maxwellian flux calculated
across the boundary. But in reality there is a faster flux to the 
object than that. This means that at constant density there are many
more injections, which the rhoinf calculation takes as a sign that the
rhoinf is actually much higher than reality. But the electron density 
can't really be that high, because if it were, the whole box would be 
very negatively charged. So the potential everywhere in the box has 
a tendency to drop so as to reduce the electron density.

The flux at zero drift velocity is area x sqrt(2T_i/\pi). This seems
like a factor of 2 too much because it is for both faces.

The low potentials are mostly a sign that the rhoinf is being taken
too large. 

31 May 2010

Fundamentally, there has to be a way to allow the phirein to adjust
the rhoinf. The instability I have been observing is a problem but
just putting it against a rail is not the solution. 

Tried putting a relaxation parameter. It needs to be very small
to bring any degree of stability. Also, It seems that phirein
is quite often exactly zero. This is because phirein is capped in
padvnc never to be above 0. Also the energy of the injection is 
adjusted using averein, which is set to the prior value of phirein at
the start of a step. However, energy is never reduced, only enhanced.

Set the phirein cap to 2.Ti instead of zero. This works ok. It lowers
the edge potential a little, but it still tends on average to be
slightly above zero. It bounces around somewhat, which seems to be 
mostly statistics.

So what we have implemented is that the correction factor for edge
potential is relaxed by only by Ti/(1.+Ti). This is small at small
edge temperature and never goes above 1/2. These values appear to be
stable (experimentally). Also we have raised the phirein cap to 2Ti,
which helps prevent excessively positive edge potentials by allowing
the positive excursions to push down the chi value somewhat. 

We still find that the corners often rise above zero potential, but
this is probably caused by the incorrect boundary
conditions. Generally seems to work for a range of temperatures and
drift velocities.

Commit.

2 June

Had discovered that the wake has a problem in this new configuration.
There appear to be long wavelength waves along the box. This seems likely
to be caused by the release of the BC constraint. Since this case is 
poorly represented by the OML enhancement factor, one really ought not
to rely on that with such a big and asymmetric box. So make the 
leading face have zero potential. This ought to stabilize things.
This was a small modification to the bdymach routine. 

There's an issue about self force. I probably ought to make sure that
I avoid it. This is so only if the electric field interpolation and the 
charge assignment are done "consistently". It is not clear that I am
doing this. In principle either the charge or the field interpolation could
be adjusted to make things consistent. 

4 June Self Force.
Adjusting the charge assignment to zero the self force. 
This can be done. It is probably best done by assigning some of the charge
in a cell that is narrower than its neighbor to the adjacent cell further
away from the wider neighbour. (But not subtracting charge on the wide 
side). This can be considered to minimize the size of the charge. 

When the charge is at the half-step position, which is where the field
is defined, and can be considered the cell-boundary, the charge is
always shared equally between the two adjacent cells. When the charge
is exactly at the node position x_0, between nodes x_m and x_p, which
are distant from it by m, p, with m < p, the fractional charges are
q_p=0, q_0=0.5(1+m/p), q_m=0.5(1-m/p). Charge assignment varies
linearly between these values in the intermediate positions.

For uniform spacing this is exactly CIC. For non-uniform, it can be
considered to be an assignment by volume, of a rectangular charge
shape whose width changes in an area-conserving manner. When it is in
cell 0, its width is equal to the larger of m or p. And the area which
extends beyond the distances x_m/2 and x_p/2 (i.e. past this cell
boundary) are allocated to the adjacent nodes (not to non-adjacent nodes
even if the width extends far enough). 

Another way to think of it is that the particle weight at the nodes,
considered on a uniform mesh-fraction grid, when at a node that has
different step sizes on either side, takes an extended form. The
form morphs linearly in step-fraction from that form to the form of the
adjacent node, when the particle is at an intermediate point. The form
is S(x) m: 0.5(1-m/p), 0: 0.5(1+m/p), p: 0 (and linearly in between those). 
The linear interpolation distance is half a cell step (I think). 
Haven't got a good handle on this description. 

17 June 10
Found that the oscillations that I got stuck with were caused by using
fixed particle number -ni, instead of fixed injection rate -ri. 

30 June 10

Also found that even using fixed injection rate at lower M=0.5 there
are oscillations. The plan is therefore to implement a switch that
totally simplifies the BCs so that reinjection is at a specified rate
calculated as the rate corresponding to distant fluxes regardless of
potential, and the potential boundary condition is simply on the
gradients, possibly using the diagonal expression. This ought to be
sufficient to give a stable scheme. It might lead to some slight edge
screening effects, but that ought not to be a problem. 

Try to sort out the reinjection number logic and the way they are
controlled for the following parameters:

rhoinf the density at infinity summed over processors.
calculated in rhoinfcalc each iteration
if nrein!=0, rhoinf=nrein/(dtin*cfactor*flux), else guess from n_part.
cfactor is the adjustment for non-zero edge potential. 

nrein
the actual number of reinjections in the step summed over all processors.

ninjcomp refers during iterations to one processor's complement.
At initialization, if rhoinf !=0, set in nreincalc.

n_part one processor's number of particles. Set by -ninnn switch. 
If n_part!=0 during initialization, then set rhoinf=0, so nreincalc is not 
called. 

-ri sets value of rhoinf __per node__, which then is converted into ninjcomp
by nreincalc, at which time, rhoinf is multiplied by nproc to make it total.

-ni sets n_part.

To reduce confusion, rename nreincalc to ninjcalc. ninjcomp is the most 
important thing set by ninjcalc. Also introduce new initial variable
ripernode which is what rhoinf was prior to being multiplied by 
numprocs. In other words we no longer overload rhoinf during the initalization.

Introduce new switch -rx determining the relaxation rate of chi, the
edge potential correction (and of averein). Default 1 immediate.
Set to zero there is no edge potential correction of the flux or of the
injection energy.

Corrected dtprec to store for each particle, as per sceptic. This is
the only way to get subcycling etc to work properly. Also set it to
zero in the pinit, do not set it to dt at the start of padvnc. This
latter change makes a big difference to the value after 5 steps. Amounts
to supposing that the particle load occurs with perfect reinjection 
form of distribution function. 
Need then to set dtprec(i)=dtpos at the standard end of each cycle.
That restores much of the difference in flux. 

Working on subcycling. If we calculate the square of the field, f2. 
then perhaps we can determine automatically whether the step is 
small enough. A criterion might be field*dt (= delta v) > some typical
velocity. The parameter subcycle could perhaps be used to set the
criterion, e.g. sqrt(f2)*dt > subcycle (normalized). Another way to
say the same thing is that dt should be shortened until f.dt < subcycle.
An integer subcycle criterion would then be
   dtc = dt/max(1, int(f*dt/subcycle))

This seems to work. Arrange to print out the number of particles subcycled,
if non-zero. 

13 July 2010

Tried out v.5 -rx0. case on 8-processor machine. Its oscillations are
substantially reduced c.f. the previous problems, but not gone away.

Found a bug in fluxdata reading and writing. This is because we need
the address of the next (unused) ff_data slot, to determine how much
ff_data to read/write. I had hacked it to omit the last. Now I need to
make it right. Trouble is with old data flux data files. Do I want to
be able to read them? I think it's not worth it. Assume for now that I don't. 
But put comments in about how to recover the old data files.

15 July 2010

The -rx0 without holding the leading edge potential fixed has rather low
edge oscillation problems and seems to run reasonably convincingly. 

However even at -p0.2 we have not apparently reached a linear regime.

Set going the -v0.8 cases.

Some code clean-up, especially of the main program. 

-------------------------------------------------------------------

31 July 2010

In view of the discrepancies with Lampe's plot of the wake, I am
considering a way to represent a point charge as a pseudo-object of
zero extent. It could be added to the object file as a new type.

There seem to be two ways to implement this. 

1. Simply put extra point charge(s) of specified magnitude greater
than unity into the region at arbitrary position in the grid, and
place the charge onto the grid via the standard chargetomesh process.
This is essentially trivial, but it suffers from accuracy challenges
close to the charge. In effect, accurate field representation in the
vicinity of the charge will be limited to the size of the mesh there. 

2. Put into the field a coulomb representation of the point charge(s).
This has the benefit of NOT requiring fine mesh to represent the point
charge influence. However, we need to solve the rest of the shielding
problem properly. It becomes a sort of PPPM approach. If we add to the
field the force of the coulomb field, and solve the Poisson equation
for the residual potential (with the coulomb potential removed), then
we will have a good solution provided we resolve the shielding. This
will permit us to do without a fine scale near the charge, and resolve
only the shielding (Debye) scale length. This would be a serious benefit.
(There will be some display issues in that we will want to add back the
coulomb parts of the potential for display purposes. Perhaps we can do
it right at the end so the output files are at least consistent.) 

Since we solve the electron shielded Poisson problem, 

      \nabla^2 \phi + \phi/\lambda^2 = \rho  ,

we need to account for the coulomb field in that solution. We will be
tracking only   u  =  \phi - \phi_c  where \phi_c is the coulomb part
of the potential. Consequently the correct equation for u is

      \nabla^2 u + u/\lambda^2 = \rho + \phi_c/\lambda^2  ,

Thus we simply need to add \phi_c/\lambda^2 to the charge. This could
perhaps be done in psumtoq.

It might therefore be convenient to store \phi_c for use both in obtaining
q and in plotting the total \phi = u + \phi_c. It is fixed (but the charge
could perhaps be permitted to change in time). In any case it ought to be
in a separate common block that contains the information about the additional
charges. (There might be a cache inefficiency in it being far from u).

There will then be an issue with boundary conditions on u. The
boundary values of \phi_c will have to be subtracted from the BCs for
\phi.  This looks like the tricky part. For example bdymach would have
to have access to \phi_c. Since it will have the same structure as u,
it ought not to be too problematic to access its elements appropriately.
Looking through bdyroutine.f for u(, it does not look too difficult
to change those all to be u+\phi_c. In the long run it might be a bit
cumbersome to do this, but probably not more confusing than the logic
of those iteration routines anyway. 

The problem with the above is that it is only true for the linearized
approximation to the Boltzmann factor. Really we have to solve

      \lambda^2 \nabla^2 \phi = n - \exp{\phi}  ,

and the non-linearity of the additional operator prevents us exactly
translating from u to phi by the addition of constant phi_c into the
equation.  It has to be in the exponent. If we recalculate the
contribution of phi_c nonlinearly at each step, then we will get an
accurate result. But the contribution is then changing. One probably
does not want to update phi_c every sor iteration step, which might be
expensive, but if one wants a nonlinear solution, that update is needed. 
 
Part of the awkwardness arises from the fact that faddu is written so
that it just takes the exponential of its argument. Because the index
of the u array is not passed, it is not possible for the function to
correct for a phi_c offset. It seems that it ought to be unproblematic
to have additional arguments for faddu such as the array index. Faddu
is called only in sorrelaxgen. One could add the index as the last
argument without breaking the current call (I think). Yes. Did that in
the call without changing the function. It still works. (Might be a
slight time cost).

Thus we would have to change faddu to compensate for phi_c, instead
of doing something to psumtoq. That in itself may be more compact and
manageable. 

To summarize: we could implement PPPM equivalent by introducing point
charge(s), calculating their potential on the grid phi_c, changing
faddu to use u+phi_c, changing the BC so that it applies to u+phi_c,
and solving for u alone, which is the shielding potential with the
phi_c subtracted off. As far as the potential calculation is concerned,
only bdyroutine.f (which contains faddu) would have to be reprogrammed.
(Now sorrelaxgen has been editted to add the index). Force also has
to add the analytic (point-charge) force. This sounds as if it is fairly
low cost.

--------------------------------------------------------------------
Found a problem with restarting. Ugh. With the check script we see
x_part differences, but not uqcij. Moved the checkx before the padvnc
and it goes away. So padvnc is the problem. This was the case when
I last worked on restarting. However, this time, EVERY particle is 
wrong after one padvnc step, and so is the particle count.

It dawns on me why this is. I have to save and restore the dtprec for
each particle, otherwise it is not initialized properly for padvnc.
Yes that is it. Fixed.

Removed the necessity for extra storage to be supplied in the main
by limiting the check code to its own maximum size. This makes it
completely self-contained. And we can use only modest size of storage
so that the impact is negligible.
--------------------------------------------------------------------

The size of the phi_c array must be such as to accommodate the mesh,
presumably the same as u, etc. Those dimensions are currently set in
the main program (and u etc are not in common), so that's a bit of a
problem.

Start by eliminating Li from coptic. Use na_ijk.
coptic still works. There's a problem with phiexamine. It assumes
(defines) its arrays to have equal length in each dimension.
When I change that in coptic, the phiexamine is not correct.
It's not completely obvious why. array3read just reads the used 
dimensions and appears to store accordingly. It defines the size
of its full dimensions by its input parameter (so they must be 
passed correctly). Needs investigation.

1 Aug 2010

Can't make the phiexamine misbehave this morning! Maybe it was just
too late last night. No, it is flaky. the geombigcube.dat file caused
problems. This is going to be hard. It appears to be the second entry
ifull(2) that causes the problem. No sometimes the third does it.

After considerable thrashing, I still can't get to the bottom of it.

Try switching from g77 to gfortran. The link fails in the same way 
that the memory overrun of sceptic caused. Perhaps there's some
problem I don't understand here. phiexamine will not link either.
This was caused by the n_partmax being set to 40M (not 4M) which 
overflows the 2G virtual (though not resident).

With this problem fixed, g77 compiles do not seem to give the phiexamine
problem. Also the gfortan succeeds in compiling. 

CONCLUSION: the phiexamine/coptic misbehaviour was triggered by erroneously
specifying a 40M partmax that caused the (virtual) size of both coptic and
phiexamine to exceed machine memory. In g77 this gave the strange errors
that I could not diagnose. In gfortran it cause it to fail to compile.
Both of these behaviours could be considered to be inadequacies in the 
compiler. But anyway...

Today was an excursion that mostly just discovered a typing error in 
n_partmax. But I now have dimension setting in the main program more
rational. It is done crucially with the following statement:
      parameter (na_m=100,na_i=40,na_j=40,na_k=100)

I could presumably separate out this declaration into a file that was
called by appropriate programs to define the array dimensions, in particular
phi_c could be sized by it.

Observe that phiexamine occupies more virtual memory than coptic itself.
This appears to be because of just dumping all the objects into it. 
It is probably better to create a library and link against that. 
So change the makefile to do that for both coptic and other mains.
This does not drop the virtual size of phiexamine well below that of coptic,
so it does not have the desired effect. It is not clear why phiexamine
is so big. It makes not difference whether the lib is simply put on the 
command line or search using -L. -lcoptic.

3 Aug 2010

Rationalized array size declarations into mini-file griddecl.f
Checked out phiexamine, denexamine, partexamine to bring them into
conformity with the new declarations and to get rid of warnings.

Implemented additional force, with the magnitude corresponding to a
coulomb potential at the specified radius represented by the
2nd extra data argument. Of course, the actual extra potential at that
radius is zero because of the charge shaping/cutoff. This does not 
perturb the plotted potential very much. But we have not yet implemented
the effect of faddu.

4 Aug 2010

Changed the order of arguments in mditerate to be equivalent to that
of mditerarg. This is to reduce potential confusion.

Implemented setadfield to insert the uc and rhoc. It also plots slices
if -gs is set, so we can examine the results. 

When we write out the data to disk, we either need to add on the ptch
potential, or we need to write it out. Implemented the addition to both
phi and pha.

Implemented faddu modifications.
Ought now to be working.

Try some cases with nothing but a point charge. Found a few problems
with fluxwriting. Rationalized that. Found that there was a problem that
arose from accidentally using the point-charge object to define the
particle region. That does not work, because volint breaks when it can't
rely on intersections being set at the boundary. They are not set with
an object that is masked out by having all its abc equal to zero. Such
an object can't be part of the structure that defines the particle region.
Implement a trap to prevent that happening.

Now we seem to have cases with nothing but a point charge working.
The u potential (initially) seems to take the peak value approximately
equal to the charge value specified (phi at r_o). I don't know if that
makes sense. It drops a bit after some evolution and ion shielding.

However, the simple test develops an overall instability in the average
of potential! Annoying but perhaps not totally unexpected, since we
now have no potential set anywhere. It's basically an instability
in the rhoinf. It is suppressed by using -rx.5, or -rx0, or even -rx.8.
Good!

5 Aug 2010

Exercising some benchmarks with pure point charge. 
With large debyelen there is a problem if the bdymach condition is used.
There is simply a big offset in potential. That's bad. 
Instead one must switch back to bdyslpd and set the radial derivative
to -1 (logarithmic). Then convergence is very rapid to a potential that
agrees almost perfectly with Coulomb. (After some mods to phiexamine to
make the plots sensible). This shows that the potential solver and 
compensation is working correctly. At large lambda, it is dominated by
the additional charge.

Testing orbits. Used  ./coptic -s100 -go1 -l100. -ri1. 
but did not get a closed orbit. Strong precession. That's not right
it ought to be at least elliptical or even circular if the energy 
is correct.

Switching back to sphere potential, we get a circular orbit, potential
-2 at r=1. 

Worked on this problem and found and fixed some bugs in the getadfield
calculation. Now we get a good circular orbit and a decent elliptical one
affected by step size. The step size issues are puzzling. Using quite big
steps for the circle does not produce large cumulative errors, but turning
on subcycling produces substantial errors. Subcycling had seemed to help
with the ellipse. Subcycling is not working as expected. The effect of
subcycling gets worse and worse, as we force more steps with a lower limit.

Logic of padvnc needs reexamining. We have the following timestep parameters

dt
dtprec(i)
dtremain
dtc
dtaccel

At top of each particle cycle, dtpos is set to dt and dtremain to 0.
100 continue
Fields are calculated at the current position.
Subcycle evaluates dtc based on the field, and if it is .lt. dtpos,
then it calls for a substep, setting
       dtremain=dtpos+dtremain-dtc              dtpos=dtc
Seems to be the same for collisions.
Then            dtaccel=0.5*(dtpos+dtprec(i))
Here we have a bit of a puzzle at startup as to what ought to be taken
for dtprec(i). If we took a previous step, dtprec ought to be the duration
of that previous step. 
Then acceleration by dtaccel is done
Then moving by dtpos is done.
Then set     dtprec(i)=dtpos. 
And          if(dtremain.gt.0.)then   
	     dtpos=dtremain  dtremain=0. goto 100 
In other words we try to take a step equal to the remaining time in
this step if it is positive.
Then we save the orbits, and move to next particle. 


The problem is nothing to do with point-charges. Exists with finite
objects. We appear to be seeing a sling-shot effect as the particle
approaches the perigee. Too much acceleration when close, too little
far away. The problem is not due to changes in dtc. Setting dtc to
a constant gives the same problem.

Found a bug on the very first getfield call. The xp fractions are not
set correctly, so the field is incorrect. This is for the special particle.
Fixed that bug by moving the special particle to pinit.

Still haven't spotted the problem, which still seems to be present when
dtc is set at a constant value equal to half the dt. The main thing is
that energy conservation is broken badly. But I don't know why.

FOUND IT. The effect is self force or some other related force
error. I was running tests with -l100. Increasing that to -l10000
essentially kills the effect. I had only one particle total. Therefore
it could not be interacting with another particle. It must be
interacting with itself. It is certainly the case that the field was
coming back subtly different, and that seemed to be the cause of the
orbit difference. By contrast, with -l10000 the differences in the
field are greatly reduced. It is extremely interesting that the 
case that is not subcycled appears to have a self-force such that it
keeps it in orbit, while the subcycled case seems to be forced off
the orbit; in particular, its energy is reduced. (I'm using subcycle
directly in the pinit special orbit.)

With -l lowered to say 20, we can see the potential perturbation (for the
first steps when the particle is on the midplane). But obviously it is
there even when -l is larger.

Conclusion: the difference between subcycling and not subcycling is
that in subcycling the potential is not updated between
(sub)steps. The result is that the particle moves in a static
self-force well, whereas without subcycling, it always experiences a
self-potential that is centered on the current particle position. 

Presumably the particle is self-repelling, so this effect ought to 
accelerate the particle in the direction of velocity. Since it is always
a bit ahead of its influence. That does not seem to have been what
happens to the particle in well, but perhaps that's some interaction 
between the well and self-force. Presumably similar spatially localized
effects exist in unequal spacing meshes. But they are not likely to be
progressive the way the subcycling force is.

Specifying one ion but finite lambda effectively assigns a very great
charge to the ion. Remember \lambda_D = sqrt{\epsilon_o T_e/n_e e^2}
rhoinf is approximately the inverse of the box size. So e^2/epsilon_0 
= 1/(lambda^2 n_e) ~ (2r_s)^3/lambda^2.N_i .

6 Aug 2010

One can confirm this is the problem by running different numbers of
particles. setting -ni16 rather than -ni1 drops the orbit error a lot.
The self force ought to be inversely proportional to -ni. But actually
it looks as if rhoinf is actually behaving proportional to (-ni)^2 as
we go from -ni1 to -ni2. This seems to be because the nrein is becoming
non-zero. When nrein is zero, rhoinf is not changed. Therefore, if any
particle escapes, setting rhoinf from the nrein, it remains set to this
larger value thereafter. This is obviously a bias. rhoinf is used in
chargetomesh.f to set rho from psum. That's how it determines the effective
size of the ion charge. Arguably, if nrein is a small number it is
unreliable to use it, and probably it ought to be ignored. Use nrein=10
as a tolerable minimum, to avoid this bias at low -ni. 

Reducing the step size (keeping subcycle half of it) does reduce the
error, seems roughly proportional. Error increases slightly decreasing
subcycle, but quickly saturates. In this case the step size is <~ mesh
size. Which means we are inside the charge cloud, and roughly the
self-force will rise linearly with (subcycle) step size. This
expectation seems consistent with observations. If it is bigger than
the cloud size, it falls inversely with step size. So taking steps
about the size of the mesh is the worst self-force regime. 

7 Aug 10

Implementing automatic boundary conditions. Plan is to use bdyslopeDH
if debyelen is >~ size of mesh, and bdymach if it is << size of mesh.
In order to do this, we need to know the size of mesh. plascom.f has
rs which is the radius of the domain for circular domains but at present
is not set properly otherwise. 

Put into meshconstruct() code that sets rs to half the length of the 
maximum side-length. Move geominit to a position after meshconstruct.
That way if a special circular case is called, rs will be reset in it.

Implemented in bdysetfree a choice between a logarithmic slope if
debyelen.gt.0.19rs, or mach bdy condition if debyelen smaller. 
Also changed crelax default to be Ti/(1+Ti). This stabilizes the 
bdyslopeDH case but probably one still needs to set -rx0 for mach
cases.

Fiddled with phiexamine to improve radial comparisons and to plot
yukawa potential. 

Improve -gs switch to allow setting ipstep. 

Start to try on big domains. Something is badly wrong with the
potential. Get back onto t400 the wakedomain.dat so we can test
locally. Shrink to wakehalf.dat so it's more managable. Seems as if
the point charge is appearing to be in the wrong place for these runs.
Might be better to get an even smaller x25 case. Let's do that.
Ok now we have wake25.dat which runs at a decent pace. It seems clear
that the point charge is in the wrong place. Or some part of it is.

Actually the plots of uc and rhoc show the charge in the right place.
But the potential solutions have features that don't belong.

The strange features appear only to be present with multiple cpu
mpiexec runs. Seems unaffected by changing griddecl na_i, na_j etc.

Is still present with the bdyroutine hacked to use bdyslope rather than
bdymach. It's different, but still pretty much bad. Can increase -l and
still see it very clearly. Probably it must be in faddu. This seems confirmed
by making the faddu test .true. so we just use an unmodified faddu.

uci seems to plot correctly from main coptic. so does rhoci.

Tired. This problem is caused by faddu, but it is hard to see anything
wrong with faddu. 

8 Aug 10

After sleeping on this, it dawns on me that the pointer being passed
is relative to the origin of the particular block, rather than the
whole array. This must be the problem. It cannot be avoided by locating
the changes in psumtoq, because faddu needs to have an effective argument
that includes the uc (even if rhoc were in q already). So probably my
plan to use faddu was reasonable, but ran into this unforseen problem 
that sorrelaxgen is passed an array whose origin is already at myorig,
and is not at present aware of where in the total array it is.

It looks as if the natural way to fix this is to pass myorig to sorrelaxgen
so it knows where it is in the total array. 

Seems to work.

9 Aug 10

Having run some big cases on sceptic machine overnight, I find the
agreement with the small-object equivalent runs to be excellent. This
is using the same mesh as the object runs. Therefore, start cases with
uniform mesh and point-charge. Basically I can already conclude that
the wavelength discrepancy with lampe is NOT caused by the finite
charge size.  Notice, though, that the discrepancy in wavelength is
worse at T_i=.01, while the damping discrepancy is better (smaller)
there. 

Made significant upgrades to partexamine to be able to choose xlimits
and vlimits, and to specify a particular partfile by giving a 3-character
extension. One can see a slow-down of the z-velocity at the top of the
first potential hill, which is in the right ball-park to be explained
by simply the energy reduction there. Evidently this is a non-linear
effect. However, it acts in the wrong direction to be an explanation of
the observed longer wavelength. 

17 Aug 10

Planning for diagnostics etc. 
We need information output about particle parameters. 
Thought about ways of representing the distribution function in a 
compact enough way to document its variation with position. Hard.

Instead implement standard moment diagnostics: velocity, temperatures.
May as well do it on the potential mesh (otherwise confusion!)
Process is relatively simple. At the time we form the psum, form also
sums for v(3) and v^2(3) moments. We require 6 more quantities on the
mesh. This is not a big fraction of memory increase, since we already have
cij, u, uave, q, qave etc. It is probably best to regard the moments as
separate quantities on the grid, so we can slice plot them if desired.
Also has the merit of giving the same structure as (e.g.) psum, for the 
purposes of mpireduction. For ease of passing, we probably ought to make
the diags an array whose trailing dimension enumerates the diagnostics.
c Diagnostics
      integer ndiagmax
      parameter (ndiagmax=6)
      real diags(na_i,na_j,na_k,ndiagmax)

For speed during particle pushing, we just want to assign the moment the
same way we assign the charge in chargetomesh. It is not necessary in 
principle to mpigather the moment information until something is required
to be done with it. And we should not. In the case of psum, it is zeroed
at each step prior to chargetomesh. However, there we probably should not
zero the moment diagnostics if we are averaging them. Instead they should
just be accumulated. If we are doing trailing averages, we might need to
multiply by (nave-1) before tomesh, then divide by nave after. But if we
did a pure box-car average, it would be unnecessary to do that. In any
case those decisions can be made separate from the chargetomesh process.

Have to generalize array3write/read to allow an extra dimension. Did that
and implemented detection of old version written files through the first
two characters of charout. Fixed phiexamine, philineread, denexamine.

Implemented writing in coptic. I wonder if we ought to write out the
box addition number of steps? (array3write does not).

18 Aug 10

Continue working on diagnostics. I think I want to use the whole of the 
diag iuds arrays. Psum does not. It does not use the boundaries. And it
does not reduce the boundaries. Therefore I need a reduction code that
reduces the whole block including boundaries. 

Start by adding the used dimensions to mditcom.f. Also it was mistakenly
omitted from the headers list. Add it. Add code to set iasuds in 
mpisubopcreate.

Found a prior bug in mpisubopcreate that the initialization call was
occuring every time, because lfirst was not set to .false. Fixing does
not seem to change anything.

Seem to have it working for diags reduction. One can use denexamine or
phiexamine on one diagnostic (which is the equivalent of psum). It
appears to have face values about half of the core values, and edge
values about one quarter. That is what is expected.

It is not obvious that I want to do any additional processing within
coptic main program. 

Time costs (for particle-dominated cases):

time ./coptic -s10 -a5 -ni1000000
real    0m12.361s

time ./coptic -m1 -s10 -a5 -ni1000000
real    0m12.485s

time ./coptic -m4 -s10 -a5 -ni1000000
real    0m14.347s

time ./coptic -m7 -s10 -a5 -ni1000000
real    0m16.837s

Looks as if there's a non-trivial extra cost for a substantial number
of diagnostics. 34% extra for 7. Not too bad.

Test of solver at low step size shows that using steps of ~ -dt0.00005
or less reduces the sor iterations to 2. But that if we increase it to
maybe -dt0.0001 we get more typically 11 iterations. (These are
half-iterations actually). -dt0.0005 gives about 25. -dt0.001 31.
-dt0.005 43. -dt0.05 60. Basically I think this shows the sor solver
works sensibly to converge to its tolerance of eps_sor=1.e-5. 
It presumably also means that if the particle numbers are large
enough that rho fluctuations are small, then the sor solver will be
fast. (These are 32^3 meshes).

Implement minimalistic diagexamine that reads all the arrays and 
displays them. 

21 Aug 2010
Fixed some bugs in the restart code that put time step data into the
wrong array elements.

25 Aug 2010

Remove the obsolete ran0 function from randf.f.

Discovered that randc.o is needed on loki. But earlier this year I 
removed it from the objects. volint still uses it. I don't understand
how it is still working! The answer seems to be that gfortran and g77
have an intrinsic random generator. The float version is called rand()
and gives numbers in the 0-1 range, which is correct. This does not
seem to exist for pathscale compiler, which is why randc is necessary.
It probably does not hurt to list randc in the objects. With gfortran,
etc, it will not be used. The intrinsic fortran call will be used 
instead. Add back.

I find that the Loki problem seems to be with MPI_IN_PLACE. 
Some searching on the net shows that this is an MPI-2 spec only.
Not in MPI-1. That presumably explains the problem. 
I find that sceptic has a piece of code that removes the IN_PLACE
saying that it does not work on Loki. The usage in sormpi is easily
fixed, but the usage in reduce.f is less easy to fix.

By the way, mpibbdytest is the easiest way to get to the bottom of these
communications issues.

Found that it is possible to compile a coptic with MPICH2 on gigabit
on loki. But the flush has to be removed from the main program because
it causes segfaults. Then in works, but one needs to run multiple processes
under:
/opt/mvapich2.gbe/path/bin/mpiexec


26 Aug 2010
Fixed some phiexamine things.
Implemented -w<iwstep> switch to determine how often one saves the code
state to disk.

29 Aug 2010
Fixed some things in the makefile. It is now possible to use override
option settings in the make command. Also make vecx will use the nonglx
driver and remove the glu libs.

4 Sep 10
Established that short domains z11 have indistinguishable results with
-dt0.05 -s3000 and -dt0.1 -s1500. So we can save on computation, and be
confident there is not a timestep convergence issue.

Had a problem with cumprob on loki because tiny seemed to be a bit too
small and we experienced interp equal points. Lowered to 1e-14 from 1e-15.
Probably ought to do some more checking with this. It arises with a big
shift of the distribution function when calculating reinjection for the
downstream face (I presume). 

There are various problems on loki with communications etc. It appears
to take several minutes for the particle files to be written to disk
at the end of a run. Also there are unterminated runs and mpds that
hang around and screw things up.

11 Sep 10
Succeeded in runs on the cmodws cluster but only by removing the 
-machinefile argument. 

partexamine modified to save the distibution information, since it takes
so long to acquire. For 4M particles, it takes several seconds to read
the data per file. So a multiprocessor postprocessing takes a very long time.

In point of fact, postprocessing all the particles from a particular
run takes roughly as long as advancing them. This means that it is not
totally crazy if one requires the average over many steps, to think that
one should run the multiprocessing code over again to acquire the averaged
distribution function as one goes. This has the advantage of doing the 
computing over many nodes. The hard part is deciding the domains in which
to do the accumulation. (And specifying that). 

Right now I have accumulation in ndiag=200 mdims=3 x 4 quantities: fv,
px, diagv, diagx. 200x3x4 = 2400 points in cartdiag for partexamine
accumulation. But actually the diagv and diagx could reasonably be the
same everywhere. And arguably all I need is f(v). So that's 3
quantities.  For these quantities, it would not be excessive to
accumulate into an array equal in size to the cell array. Then at each
step one would be able to communicate it without excessive cost.
Alternatively, if the communication is only at the end, we would not
need to worry about it, and one could consider an array of size equal
to the number of particles or bigger. (E.g. 4M) In fact it does not
make sense to accumulate into an array bigger than the number of
particles for one processor, because there would be less than one
particle per cell. Of course if we are averaging over time and
processors, that might give a decent count even with close to 1 per
cell. Consequently, the spatial array size we could consider
collecting distributions over would be no larger than 4M/200 =
20k. Notice that this not quite the size of the spatial array 32x32x96
~ 100k. But if we have only fv to accumulate, then the extra factor of
10 is compensated by each particle having 9 reals. We could use a
resolution in 3-D equal to the cell-mesh; but probably reduce the
storage to an unimportant cost at about twice the cell-size (factor of
8=2^3).

Put it another way. We use typically 50 times as many particles as cells.
Each particle costs 9 reals. We can therefore afford fv 450 reals per cell
and only double the storage cost. 

Notice that we need a computationally efficient way to assign the
particle weight or else we'll die from the extra particle calculation
cost. For example, suppose we had 450 moments, each of which had to be
calculated when a particle was accumulated. This would undoubtedly far
outweigh the moving cost for that particle. We already know that
forming the sums for the first two velocity moments is a significant
expense (+35%).

Alternatively, if the process is simply assigning to a cell in 3
dimensions, this requires roughly 3 multiplications, which is going to
be cheap.

What's more if we use the cell mesh at the spatial basis, we already
have the fractional mesh position, so that is free. On the other hand
we might well do CIC-style assignment, in which case we have to assign
6 adjacent fractions for each charge and velocity. That means we can't
use integers for accumulation. 

Adding an extra array ndistmax,na_i,na_j,na_k, with ndistmax=300,
(50,50,100) takes the virtual memory from 485M to 772MB. Thus if 
we can keep the distribution to 3x100 we'll be ok.

Then presumably we'll have to write this array: 300MB. If it were 
reasonably sparse, we might be able to save a lot by not writing 
out those velocities that were never present. This could perhaps
be done by keeping track of a total count for each velocity (which
is then the total over the whole domain which might be useful in its
own right). Any zero of that is not an fv required to be written. 
However, we would have to consider the order of indices for optimal
access. The gain from this complication might be a compression of a
factor of 3 or 4 in typical cases. But it would probably be better 
to keep from being sparse. 

To ensure that we have a well chosen set of velocities (assuming this
is possible for the entire domain) we could use analysis of the actual
particles after a tolerably small number of steps. Choose velocity
ranges that cover the substantive range(s) before we start
accumulating. We probably would want to be able to choose a range that
covered all but a tiny number of particles (not all but zero particles).
That would be a bit more elaborate than just finding the min and max
of velocity components.

Specification (q, nq, nmia, rmi, rma) for array q find the range rmi, rma
such that there are nmia values greater than rma and less than rmi.
Actually we can't pass a 1-d array so it's got to be more complicated.
Also we need to do in 3-d. Have to retain the nmia smallest and largest
values (or pointers to them). 

Slot new values into the sort, in order, with overflow out of the end.
Initial values ought to be chosen to be overflowed out. 
Wrote code to do that. Took longer than planned. Seems to work when
used in partexamine.

13 Sep 10

Idea: why use equal spaced array of bins? That's inefficient. If one
had a bin array with (roughly) equal numbers of particles in each,
that's efficient.  One could probably do the same sort of adaptive
approach. Use the first step or file to decide on the bins. Thereafter
accumulate in the fixed bins. This needs a good algorithm to decide on
the bin placement. That's basically integrating the distribution
function to get the cumulative dist function. Then using equal spacing
on it (or something like that). How does one get the cumulative dist
function or its approximate equivalent efficiently? One way would be
to do the accumulation on a uniform grid with a large number of bins
and then add it up.  One also then needs an indirection step so that
one can assign particles to non-uniform bins by a single
division. This process would in any case require the larger _uniform_
bin array and its mapping to the non-uniform.  The mapping would be
universal, so no big storage is required. We also need ways of dealing
with non-uniform binning. 

Implemented in partexamine. I find that using a non-linear curve to
choose the non-uniform bins enables one to get a tolerable
representation with only 16 bins. 32 is really quite good.  Using this
technique, which will port readily to coptic itself, one does not need
more than 100 times the cell mesh, and possibly 50 times would be
enough. Then accumulation on the entire mesh will require only about
25M reals: less than the particle storage (11 reals times 4M). This
mesh based accumulation is highly appropriate for multiprocessing and
averaging over a decent number of steps, because then the numbers per
cell can become substantial. 

2 Oct 10
Reorganization of analysis routines into subdirectory analysis.

We need a more general potential variation on specified objects.  One
idea for such an object might be the general cuboid (actually general
parallelopiped) which is defined by three vectors vj from an origin
corner. The corners are then defined by the triple n=(n1,n2,n3) where nj
is either 0 or 1, and the position is \sum nj.vj. If we consider potential
to be given at (0,0,0) and that \delta\phi is equal to \sum nj,dj, with
dj being the potential difference at the end of vector vj, then we will
have a consistent and linear potential progression across the object.

One could do the same for a tetrahedron but that is not as likely to be
useful.

4 Oct 10 

About to try to implement some of this. But first want to understand why
with orbitinject etc we are getting Getfield No good vertices errors.
Find that we get a few such errors with the test field case. We used not 
to do so. 

The change seems to have been in the transition to revision 1.19. Where
the edge region was set to -1 to prevent reaching beyond the arrays. 
Now, when a sphere cuts between the penultimate node and the edge, the 
region outside it is 0, but the edge nodes are region -1, so for outside
points there are no good vertices. This is actually correct in one sense. 
When there is no object boundary nearby, this won't cause any problems. 
But when there is, we get these nogood regions.

Actually the error in the test routine only shows up when I shrink the
sphere radius to 4.9999. Otherwise the only drawn points are inside the
object. Basically this is working correctly.

With particles, however, we are getting nogoods. These are all reported
with fracs 1.0 or 32.0. Region 0. We ought not to have particles there.
Guessing this is a reinjection problem.

Seems to have something to do with the makefile not compiling the 
reinjection routine. But now that I've cleared it I can't reproduce
the error.

Ahh. I think I see the problem. ar -rs is inserting with replacement
into the libcoptic.a archive (library). When switching from one
reinject to another, the old reinject is left in the library. Then
when linking is done, the linker finds the old reinject code before
the new code, and it is using the wrong reinjection code. If the
makefile is changed we ought to delete the old libcoptic.a for
sure. But in any case this explains why running make clean fixed
things because it deleted libcoptic.

For now just always delete libcoptic before inserting into it.

Now things are all working correctly.

5 Oct 2010

Improving field interpolation near boundaries.  Implemented a wider
search along the direction of field for valid points.  This helps a
lot to improve the interpolation in cases where the point would
otherwise be omitted. Especially when there is only one point of four
(of a 2-D box) in the correct region. However, when the two missing
points along the field direction are opposite a leg with two points in
the region, then the extrapolation is not so good. A discontinuity
occurs between the extrapolated value and that for a case where the
upper gradient cannot be extrapolated from anywhere and so it is
omitted.  The latter (omission) can give a _more_ accurate result than
a long extrapolation. Maybe we should have a ramped weighting of the
certainty of this field value, rather than a sudden omission. How to
implement is a bit tricky. Perhaps the flags become weights?

6 Oct 2010

Fixed the above problem by introducing weights rather than flags for
missing nodes. Instead of simply dropping a missing node, we try to
interpolate it from further away. When we do this, its accuracy is
substantially reduced compared with the accuracy of legs that are all
in the region. So the weight of a leg that is extrapolated is reduced
and the box interpolation is modified (actually simplified) to account
for a continuous weight rather than a discontinuous flag indicating
absence. 

For lattice legs that have both points out of region, call this
"overextrapolation", the weight is reduced linearly from 1 to zero as
we move from the outermost point to the innermost along the
extrapolation direction. This allows the adjacent point (if any) in
the region to control the value towards the inner end. This does not
much affect squares with three points outside. But it helps very much
for a row with four nodes outside adjacent to a row entirely
inside. In such a case box overextrapolation from either side is
possible for the two outer boxes of the row. They are linearly
unweighted, reaching zero at the boundary of the middle box. for which
the nodes are entirely missing. As a result, discontinuities that
otherwise occur in the extrapolated values are avoided. All this is coded
in. 

However, there remains a big error in Ez when in the z-constant plane, at
oblique angles. It is associated with boxes that have a single missing
node. It abruptly reverses as z varies across the midplane. This must be
arising because of unidirectional extrapolation whose direction reverses
as we go from one half of the box to the other. What we need for this is
to have weighted bi-directional extrapolation to ensure continuity when
it is possible to overextrapolate from either direction. 

There appear to be other problems that are tickled by moving the mesh
around, and are real bugs. We need better diagnostics that really know
where the sphere center is and do proper analytic calculations.

7 Oct 10 Coded in proper analytic calculation. Looks sensible.  Now
starting to figure out the major bug(s), using shifted mesh.  

Implementing some internal writing diagnostics, we find that the
easiest way to test this is by using a specific test on the value of u
and the gradient to flag an error. Then we find that the bug consists
of a single point with offset 33363 whose cij pointer is 3276 which
has both boa and coa set to zero, but fraction equal to
0.602049947. This is incorrect for a point inside the fixed potential
sphere. But why is it wrong? Perhaps I need a sanity check on the entire
cij set and its pointers during initialization. It could hardly be general.
It is specific to this particular problem, since boa=coa=o amounts to setting
potential to zero. 

8 Oct 10
Found the source of this bug in boxedge:
                     f1=1./(sign(max(abs(fn(i)),tiny),fn(i)))
c Warn if any strange crossings found. Removed not necessary.
                     if(f1.lt.1. .and. f1.ge.0)then 

This warning and removal of the strange crossings was commented out.
The results was that strange crossings were being allowed in the
boxedge calculation whose purpose is to set some fractions determined
by planes of intersections with nearby lattice legs. That information
is basically unused in the present incarnation, since I found that it
was infeasible to embed the object geometry into the cij information,
and I currently don't.  Therefore this complex area of code ought
probably to be cleaned up and the complexity removed. For now make
sure the strange crossings are in fact removed. This was the real
bug. Now there remains the poor interpolation problems. Actually this
does not seem to be the only bug affecting the flat potential
region. There remain some strange results but not as large in
error. They are associated with uprime=-1.850757 and many (maybe all)
of them with icp0=3276 (as was the case with the old bug). So perhaps
I have not really fixed this bug.

Using the stick plot and wiremesh to diagnose this more fully. The reason
we are getting the Warning of strange crossing is not that the strange
crossing is wrong. It is that there really is a crossing there, but for
some unknown reason it is not being found or documented in the original
pass. Thus the bug is not in the warning code but earlier. USE the stick
plot (with outer sticks removed by removing outer object).

Finally tracked this to an error in the algorithm of spheresect. PHEW!

So now we really seem to have the bugs fixed. And we can move on to the
problem of whether the interpolation can be improved. Right now with the
spherical test case (shifted) we are getting max tangential field errors
in the ball-park of 1, and lower radial errors.

12 Oct 10
Work on encapsulating the potential solution and the pic code separately.
Created sortest which does not depend upon particles at all. It is in
the makefile but still depends on a lot of routines. 

Also get all the MPI routines into just mpibbdy.f and reduce.f
No direct reference to anything mpi is outside those routines.
A non-mpi version would need to have a certain number of dummy routines
for the abstracted mpicalls. But would not need mpi. 

Got a version of sortest going without mpi, using nonmpibbdy.

Thus I've accomplished a couple of things. (1) Separated out the potential
solver. (2) In the separated potential solver, separated out the mpi into
a single file, and developed a dummy file. This is a demonstration of the 
effective encapsulation of the mpi within the solver. 

Because I can't access cvs at the moment, I note that phisoluplot.f replaces
ccpicplot.f ; we add sortest.f nonmpibbdy.f. 

How to cement the encapsulation? 

13 Oct 10
Start to get the include files sorted out of the sortest objects.
Make faddu a separate file. Make a different bdyroutine bdysetsol.f.
Get rid of svdsol.f by putting it into cijroutine which is the only
place it is used.
Get orbit3plot out of phisoluplot and make its own file.
Clean out the includes of plascom and partcom from the solobjects.
Then the only includes are meshcom, objcom, 3dcom, bbdydecl, accis/world3.h
It's pretty much impossible to purge out the accis/world3.h reference from
slicesect. 

14 Oct 10 Thinking more about improving field extrapolation. If just
one point is absent from region, it would in some cases be better to
extrapolate based on both directions. Then when we reach the point
itself, we would balance out errors, rather than them making an
instantaneous transition from one to the other direction of
extrapolation. There's a puzzle about a situation where the object
simply is only one point thick. In that case one probably should NOT
extrapolate from the other side. In fact the field might be totally
discontinuous from one side to the other. Therefore any improvement
obtained by smoothing is inappropriate except when the region itself
continues from one side of the point to the other. I.e. when there's
an unintercepted leg to the region somewhere from the point.

The switch in this case is done within gradlocalregion. So there's a bit
of a puzzle with how to set weights if one wants to. But in any case it
is not really obvious that there is much gain to be had in further 
refinement of the extrapolation. 
 
Ran a series of cases with different resolution. 

N	16	32		64		128
Er	1.71	0.80		.167		.081
Et	1.86	1.74		.337		.100
Pm	1.01	.296		.056		.016
Ey	1.90	0.82		.342		.076
SDEy	0.33	0.11		.030		.008

The Ey error really needs to be explored over the whole volume. When it is,
the maximum converges rather more slowly. 128 max is about 0.25. But probably
64 is also larger. It might be that the maximum error is only linear. But the
SD is quadratic. 

15 Oct 10

Implemented the integration/search over 3-D in ereplot to find the worst
error and plot at that level. 

Implemented over-extrapolation from both directions being counted when
possible. That occurs when there are just two points outside the region.
This may be less dangerous than the case where there is just one. But maybe
not. Still this is easy to reverse if necessary. It produces roughly a
20% reduction in the maximum error, so arguably not a big win.

Values with this new implementation and searching over the volume:

N   	  16  	  24     32       48       64 	       128
Ey	  1.67	  1.57   0.91     0.68     0.50	       0.223
SDEy	  0.201	  0.17   0.089    0.060    0.022       0.007
Nodal	  0.200	  0.04   0.031    0.014    0.008       0.002

Plainly the maximum error is (only) linear with mesh size. SD is faster
than linear, but not obviously quadratic (till convergence care is taken). 

This is perhaps disappointing, but I think explicable on the basis of
the fact that there are places where tangential-field is evaluated using
no derivative information because of omitting points on the other side
of the boundary. 

If the boundary has continuous potential, it might be a better approximation
_not_ to omit the points, but always to be sure that we never take a 
derivative based upon potential values on either side of the interface.
This would help only for tangential derivative. It would make things
worse for normal derivative. 

Another maybe better view might be that when multilinear interpolation
fails because a point is outside the region, instead of omitting it,
we should extrapolate from the other side. This ought to have the
merit of quadratic accuracy, rather than the linear accuracy of the
above scheme.  The danger is that it might over-reach into other
regions. But that's probably unlikely. I think that the omission of ML
points leads to linear convergence only if two (or three) points (in
3D) are missing, so that no information on a gradient is obtained.

There are issues to do with whether one reaches too far and thus out of the
domain. Perhaps it is not a problem. 

19 Oct 10
Plan for validation. Get a UROP student who will then run both coptic and
sceptic for equivalent cases. 

27 Oct 10
Getting back to the code development. Particle distribution diagnostics.

Note that we track the fractional mesh position of each particle. This
makes it trivial to assign the particle to a bin that is based upon
the (potential) mesh. Consequently, if we generalize the partexamine
data structure to add ndims extra suffixes to it, we will easily be
able to assign. In fact we can pass the x_part, and that's what we
should do.  Addressing might be a bit tricky in general
dimensions. There's another issue that although the total number of
assignment memory slots is less than the particle data (11 reals times
4M) when the mesh is small, it may not be if the mesh is big. I really
need a transparent way to either include storage for the distribution
on the mesh or not.  Or a way to reduce the storage demand by
concatenating cells if desired. This might be done by defining a
concatenation number which is the number of cells (in each direction)
that are counted as one for the purposes of fv-storage. If this is 2,
then there are half as many in each direction. Which cuts the storage
down to 1/8. For bigger numbers there is trickiness in accounting for
unequal numbers concatenated. We could choose to prevent that in the
storage assignment, and perhaps in the actually array assignment at
run time. In fact, though, it may not matter if the concatenation is
unequal, provided we account for it in normalizations etc. But the
idea is that if this number is equal to the whole mesh array, this
reduces the accumulation to the whole domain.

5 Nov 10
Implemented a spatially resolved accumulation in partexamine. The spatial
boxes are based on the x-mesh full structure divided by isubdiv. 
A problem arises that partexamine does not know the mesh spacing structure
because it is not stored in the particle file. Similarly it is hard to
make sense of the numbers in cell because their volume is different, 
and particle file does not know the volume. These would not be so
problematic for coptic itself, since it would know that information. 

Mostly the reason for these issues is the use of the fractional mesh 
information in x_part for position registration. If one had a totally
different, perhaps uniform, mesh for the distribution, then this problem
would not arise. So one should probably reconsider the idea of using 
the x_part fraction. Once the spatial boxes are combined, it is far from
obvious that it is a good idea to base them on the phi-mesh. 

Suppose instead that we use the xlimits structure to define the total
spatial domain, and simply divide it into equal sized boxes. It will be
quite straightforward to assign position to bin, and I imagine not very
much computational cost. 

Changed to this approach. It is much easier to understand and implement.
It works sensibly in partexamine. 

15 Nov 10

Moved most of the serious work of particle accumulation to a new file
coptic/partaccum.f and moved fvgriddecl.f to coptic/. Then recompiled.
This prepares us for incorporating some of the accumulation into
coptic itself. For that purpose we need to consider the parallel 
operation. 

Recall the overall approach: we run the total accumulation first.  It
puts every particle that is within the xlimits into a uniformly spaced
set of velocity bins. In so doing, it produces xnewlimit.  Then
bincalc groups those velocity bins by pointing each one to a specific
sub-bin. With those pointers determined, it is then cheap to calculate
the velocity sub-bin via the uniform bins and the pointer array.  The
routine fvxinit initializes. Then the subsequent call(s) to subaccum
accumulates particles to the spatially resolved sub-bins. Of these
processes, the bin and newlimits etc need to be universal, otherwise
there is no way to reduce the data back consistently if the bins are
different between processors. xnewlimits could presumably be reduced
as maxs and mins. But bincalc needs to be done centrally, after
reduction of the original accumulation. Alternatively, the whole of
the first global step could be done only on the particles of the
master node. In that case no reduction would be required, only a
scatter of the resulting pointer and xnewlimit information. If the bincalc
were to be based on a very localized xlimit, then there might be a risk that
the noise level would be excessive if we use only one node's particles. 

Each process can do (sub)accumulation at each timestep. Then presumably 
a reduce to the master node is required, and the data to be written to
disk. Presumably the sub-bins would then be reinitialized and the process
repeated. If this gather-write is not done too often then it ought not to have
a significant time cost. 

Probably the way to proceed is to package the code in partexamine without the
plotting (routinely). 

17 Nov 10

Having packaged the update and plotting of the subaccumulation into
partdistup and pltsubdist, incorporated them into coptic, and got working
with the appropriate default values. 

The extra cost if both partaccum and vaccum are done each step is
about 10% of the particle cost. Nearly negligible (but not quite).

Now we need to incorporate into the control structures. We have two
step lengths -a which is the iavesteps the averaging length and
diagnostic writing step length and -w which writes out to the (same)
final files. One presumably would use the diagnostic step length,
although at present that is examined only if ndiag is non zero. We
could invert that priority. Ok did the inversion. Now we need a control
for whether we are doing the particle accumulation. And to incorporate 
data write-out. 

Incorporated the switch -pd and display and writing.
Allowed arrow keys to change the cell position in different dimensions.

Basically this is now working in serial but needs mpi reduces for 
parallel. This also needs to worry about the initialization using the
full information.

19 Nov 10

Combined fvgriddecl into ptaccom.f.

Seem to have got the parallel version going with some reduces.

-----------------------------------------------------

What to do next:  
     General objects with potential variation. See Oct 2.
     Magnetic Field.
     Collisions.
     Benchmarking with sceptic (student).
     Self forrce tests(?)
     Space and moons.

22 Nov 10

Discovered that there's a problem with the written out data from the particle
diagnostics. It seems to arise because I've incorrectly assumed that the
velocity bins for the subaccum can be recalculated from the other data. 

Found a bug giving Field corruptions. It appears to arise at the outer
boundary of the cuboidal region. It comes from one of the new getfield
calls that is invoked when there are missing interpolation
points. (Which is presumably why it is a new bug.) Tracing it through
various diagnostic writes is tricky because of the dereferenced origin
and the multidimensional aspects. But my best guess is that this is 
arising because of the way that the extra getfields reach beyond the
immediate vicinty (at least along the gradient direction). 

23 Nov 10

Thus my theory of the problem is that when adjacent to (e.g.) boundary
of 1 (x) when getting gradient component 3 (z), the box interpolation
fails to find acceptable gradient on the outermost-x (boundary) and
starts to look further along it. In doing so, if it is near/at the
z-boundary too, i.e. at the edge, then it may reach outside the iused
limit in the z-direction (gradient direction). It will try to check
the cij region there, but that won't have been set, so it is not
obvious that it will say, don't use this. Also, it is possible that
this might reach beyond the ifull limits, in which case in will
violate the bounds of this array as a whole. 

Although I can't quite verify this theory, I can see that I am
accessing a negative array position for the first corrupt localregion
detected.  Perhaps I should simply ban the calling of getfield from a
general origin and do checks about exceeding bounds. That might
eventually be a handicap if the domain was divided for particles. But
that's only a remote possibility.  In fact, checking the rest of the
code, the only calls to getfield that currently exist pass the bottom
of the u-array. There are no local origin calls. Therefore extended
the tests in the extra getlocalcalls to prevent over- (or
under-)running the array bounds. This prevents the corruption
errors. So it presumably proves that my diagnosis was correct.

There are still some box recut errors. They probably ought to be 
understood. They go away with very small adjustment of the mesh. 
So they seem to be a mesh clash. It doesn't seem very rewarding to 
go chasing after it at the moment. Just tell to adjust the grid.

There's still a bug in pex reading and writing with the fv data.

26 Nov 10 

Working on additional diagnostic plots, especially wrt velocity.
Cleaned up some of phiexamine.  Found that there are some spurious big
velocities in the diagnostic files saved from recent runs for
wakes. These apparent bugs need to be understood if we are going to be
able to use the diagnostics for velocity plotting. It seems likely
they arise (at the edge of low-density regions) because of divide by
(near) zero. Yes this is the problem, because one can look at the 
unnormalized values and they are fine. So we increase the minimum
value required for the division in normalization to 5. rather than 0.
But there's another calibration problem, which is that the value of the
velocity is too large: 40 instead of 5. This must surely be something
to do with the number of processors (8 for these runs). Yes there is
a bug in diagreduce that means the first diag (density) was not 
reduced. That explains the 8 times larger velocities. Fixed.

27 Nov 10

Implemented arrow overplot on phiexamine radial plot. It does not 
show too much on the fast wakes, since the arrows mostly point in the z
direction.

Got puzzled about the fact that density is not zero inside the object.
Realized that this is because of setting it to exactly cancel the electron
density in the object. Thus one way to use the density in the object is
to say this is the electron density at the potential of the object. 

Implemented quasineutral solving for debyelen=0. Seems to work. Needed
to fix cijroutine to avoid divide by zero. However, this may not have
really reached a correct simulation because the potential boundary
condition is a fixed value on the sphere, whereas for a quasineutral
case, the BC ought to be an equivalent of the Bohm condition at the
boundary. According to Leonardo, the way to get a correct condition is
to put the density at the probe edge equal to the flux divided by the
mean normal velocity. Evidently this will require changes of cij. That
sounds very tricky. Despite this problem, the result actually does not
look too bad. How can this be? Well we are actually not using the
boundary condition on the probe, because that is embedded into the
cij, and we are not using the cij for potential solving. No difference
equation is being used. Actually we are using the cij for getting the
acceleration.  All we are doing is evaluating the density and putting
the potential equal to its logarithm. For mesh points whose volume is
set large because they are outside the domain, all that happens is
that psumtoq sets the rho=faddu(u), and then the quasineutral
potential is set to be the inverse. So, within rounding errors,
potential never changes (from zero).  That explains why we are seeing
zero potential inside the sphere.

We never use the potential from inside the sphere. Instead the cijs are
used to apply boundary potential. And quadratic interpolation of the normal
derivative is used. It might be useful to explore the extent that total
flux depends upon specified potential. It ought to be a weak dependence. 

Actually there are substantially worse problems. The flux is strongly 
anisotropic in the quasineutral case. It is also when using lambda_d less
than the spacing, e.g. -l.01. There are massive fluctuations in the 
potential, maybe up to +-.3Te, for these cases. The noise is very large.
An estimate of this is: suppose we have 4M particles on a grid 48^3, which
has about 120k cells. That is about 33 particles per cell. So we would
expect the fluctuations to be about 6 of 33. So perhaps this is explicable. 
In itself, the noise hardly explains fixed anisotropy, which is visible. 
However, perhaps the self forces are strong in these situations with 
few particles per cell, or maybe there is some other mesh-dependent effect.


13 Dec 10

Working on Media type boundary conditions. Developed the interpolation
and difference formulas.

In cijroutine, whether there is an intersection of a lattice leg is determined
by potlsect (in 3dobjects.f). This is probably the first place to work on to
implement such conditions. This is currently implemented for spheres only
through the routine spheresect. Evidently we need routines that implement
other objects. 

It might be appropriate to implement a new object type which consists of
a plane of charge-density. I realize that there is a question as to whether
objects like this should be implemented through boundary conditions or 
through the equivalent of PPPM. The point-charge PPPM is really the addition
of a screened point charge, with a radius of influence specified, over which
it is screened. In principle one could specify a charge plane by the same
sort of approach, but one would have to have a way to calculate the analytic
force quickly.

One might approximate the charge plane in pretty much any way that is
(1) convenient, and (2) capable of being accurately compensated on the grid.
But I don't really know how to do that. Perhaps should think about it. 

Analytic solutions in 2-D for a finite uniform charge strip are easily
calculated. For a strip -a<x<a at y=0, the field at (x,y) is

E_y= \sigma/4\pi\epsilon_0 [arctan(x-a) - arctan(x+a)]
E_x= \sigma/8\pi\epsilon_0 ln|[(x-a)^2+y^2]/[(x+a)^2+y^2)|

(I'm not sure what the potential is.) This could be screened in some
arbitrary way with a function that falls to zero some distance from the strip
and the corresponding screening charge density calculate. 

However, if a finite length in the third dimension (z) is needed, this
solution does not seem to be available. Another might. Actually the Wolfram
integrator gives an analytic form for the integrals required. But they look 
like this:

Integrate[ArcSinh[a/Sqrt[x^2 + 1]], x] ==
x*ArcSinh[a/Sqrt[1 + x^2]] + (Sqrt[1 + x^2]*Sqrt[(1 + a^2 + x^2)/(1 + x^2)]* (2*a*Log[2*(x + Sqrt[1 + a^2 + x^2])] + I*(Log[(4*(I + I*a^2 + x + I*a*Sqrt[1 + a^2 + x^2]))/(a*(I + x))] - Log[(4*I + (4*I)*a^2 - 4*x + (4*I)*a* Sqrt[1 + a^2 + x^2])/(I*a - a*x)])))/ (2*Sqrt[1 + a^2 + x^2])
which is rather overwhelming.

In any case, although we can come up with solutions for uniform plane charge
sheets, that does not help with the ultimate aim of implementing surfaces of
more complex shape with possibly non-uniform charge density on them. E.g.
a sphere with non-uniform charge-density.

Therefore while an analytic test might be useful, in the longer term, I think
my boundary approach is more likely to yield a profitable way forward.
By the way, it might serve my purposes to implement dielectric charged
sphere first. 

27 Dec 10

Implemented general parallelopiped pllelosect and inside_geom.

Geometry seems now to be working. The wireframe rendering is rather lumpy
because it is not optimized to draw the extrema connections. The connections
that are drawn, between intersections common to a node or on adjacent
parallel lattice legs, do not include lines between diagonally connected
cells that might be the best extrema. In particular if diagonally
adjacent nodes have just one intersection, in the same direction, then
it is likely that the whole surface is below those two intersections, 
and thus that they probably ought to be connected. It seems too much trouble
to try to fix this, since it does not actually affect the solution. 
It appears to be solving the equations in a sensible fashion and putting
the potential to the correct value on the object surface. 

Played around with thin slabs. They seem to work fine and the solution 
by eye looks plausible. If they are very thin then periodic gaps appear
for oblique slabs. These appear to be permeable, so I think it is behaving
like a wire (mesh) screen.

I think it would be possible to use this object to introduce a plane
by making the plane a surface of a big object whose other faces were
outside the computational domain. This then divides the domain into two.
I don't know off hand what the boundary condition in the Neumann case
is on the inside of the object. I don't actually think it is automatically
the continuity condition. Perhaps I haven't specified this quite
completely yet. Although I think I did for the sphere within sphere test.
Yes potlsect returns B=negative for the "inactive side". And in that
case continuity only is (normally) used. This isn't really implemented
for anything except spheres. Actually no derivative scheme is.

Implemented coordinate-aligned cuboid. Seems to work.

28 Dec 10

Implemented coordinate-aligned cylinder. Works.
Fix cijplot to be able to rotate the stick only plot. 
Now we have all the originally conceived objects working with 
Dirichlet boundary conditions. 

Needed: Neumann/Robin for other than spheres. Media/Surface-Charge.  

29 Dec 10

Committed to CVS.

Working on different boundary conditions. In the long run we probably want
to link the surface charge to the flux. Therefore when we are using a
surface charge density that varies across an object, we ought to use a
data structure to describe it that is the same as the structure 
describing the flux density. At present this structure assumes that the
number of dimensions needed to describe the flux-position on the object
is nf_posdim=2. For an object with facets, this may be inconvenient. 
For example, for a parallelopiped, the natural way to index the position
would be by face (2 x ndims) and then by (ndims-1) dimensions on the face.
Thus at least nf_posdim=3 would be useful.
Also, nf_posdim is a parameter at present, being used to define the
array sizes of numerous objects in 3dcom. It can't be variable. 
In 3-D then, it seems a reasonable compromise to suppose that objects
might have facets on which 2-d indexing of position is done, but that the
facets themselves are indexed in a single dimension. Thus nf_posdim=3,
and the third dimension is the facet number. For a sphere there would
be just one facet. For a cube or parallelopiped 6= 2x ndims. For a cylinder
there might be 3=ndims (top, bottom, curved-surface). One can imagine 
others such as cone 2, tetrahedron 4, etc. 

Currently the only flux accumulation that is defined is for sphere 
in standard cos\theta and psi. 

Make nf_posdim=3. Then fluxexamine does not plot the new flux data
properly until it is recompiled with new common. Then it appears to work.

Now we need to generalize the flux tracking routine objsect.
There are two main parts to it. The first finds the bin number.
The second adds the contribution to the bins. Only the first needs
to be different for each object. The second should become common.
Split out the sphere-specific part. 

Working on cuboid face and surface indexing. I find that my approach
that uses ofn1,2,3 with ofn3 simply the number of facets (faces) is
limited.  The problem is that it makes sense to use a specific number
of face divisions in each of 3 dimensions. So that face 2 is indexed
with n3 and n1 (for example). This does not quite work with the plan I
have set up, in which ofn1 and ofn2 are the same for all faces. That
essentially forces all face indexing to be the same numbers. What I
really ought to be using is ofn1,2,3 for the three dimensions and offc
for the face number. Then the dimensions of face 1,2,3 are
ofn2*3,ofn3*1,ofn1*2, and the total is
nf_posno=2*(ofn2*3+ofn3*1+ofn1*2) (because of two faces per
dimension). In general I regard the indexing of the surface data as
totally the responsibility of the object code. So I really just need
to correctly calculate the total size of the surface data, nf_posno. 
Any subsequent analysis has to know how the data is organized, but 
coptic doesn't except in the fluxdata accumulation (which is what I am
working on). There's no reason not to include this extra facility, 
I think. Yes there is. The problem is nf_dimlens(nf_quant,nf_obj,nf_posdim)
which is assumed to be a triple (for nf_posdim=3) in the last index,
and hence can't really change. Actually at present, for spheres, I don't
use the third dimension of dimlens. Therefore it would be acceptable 
to make it equal to the third index length for a cube. It would not break
the sphere.

Had to introduce a new nf_faceind array that gives the offsets to the 
different faces within nf_posno, because those offsets are not uniformly
spaced when the faces are not of the same side, which now they aren't
since I'm using constant array length for each coordinate dimension.

Found that using an odd number of mesh points gives faddu singularity
errors only when the total coordinate length is a factor of n_mesh-1.
Not really clear why. Seems that cijroutine is not setting the regions
correctly at the boundary of the cube. Region 0 outside of it is not
being set. Must be because of the mesh clash.

30 Dec 10

I think the problem with meshclash is that on the one hand one can make
a convention that points on the boundary are all (say) outside the object,
but on the other, one has made the convention that fraction=1 is taken
to be a non-intersection. These are inconsistent, when projecting from
inside. This probably needs to be fixed in cubesect, etc. But there's
another inconvenience with those. I should have defined the center to
be really the center and the radii to be the distance from the center
to the faces. I want to change that. 

Changed cubes to be defined by center and radii. 
Fixed the mesh clash by automatically shifting the mesh slightly.
There's a down side to this in that it is asymmetric. 
Maybe I should have shifted and scaled. Try that. Seems better.
Seem to have got the changed cube flux accumulation working.

Cubes are now working apparently consistently. 
Fixed parallelopiped to use centered definition.
Fixed cylinder to use centered definition.

Flux on cylinder. We need 3 facets: two ends and curved surface.
Rationally one should index the angle by the same number on each.
Then the radius number and the axis number could be different. 
Therefore, like the cube, we could have 3 dimlens, which we should take
to be r,\theta,z. 

31 Dec 10

Despite good progress yesterday, I am mired in the cylinder case.
It seems very complicated to determine the intersection point. 
Rationalized the spherical intersection code to some extent by
packaging the calculation. Also including the ability to do a cylinder
in the same code by omiting a particular coordinate direction. 
However, this package needs to pass some stuff back, e.g. C, D, which 
are the radii squared -1 are useful to tell which end is inside etc.
Also, one might as well pass back both intersections, not just the one
that is closest positive to x1. Then we'd have all the useful information.

Organized this all more carefully. Now cylfsect is working. And the 
coordinate-aligned cylinder case appears correct.

Parallelopiped flux accumulation is not yet done, but it ought to be
very similar to the cube.

Created a routine docwrite to convert plain text files into fortran
routines that print out the same text. Have to replace single quotes
with backquotes to make the fortran compile properly. Seems to work. 
Fixed the parameters.txt file to bring it up to date. 

Made a geommany.dat input file that illustrates many of the shapes. 

3 Jan 2011

Remaining to be done: pllelogram flux accumulation. Should be like 
cuboid. 
Implemented. Also packaged the cube to use the same code based
upon a unit cube.
Seems to work but there are some strange inconsistencies
when changing from 1 to 2 bins. The exact numbers don't add up. 
We need some diagnostics of the intersections being found.

4 Jan 11
General idea is to draw in 3-D the flux grid for an object and the
set of intersections in the form of short lines from the prior 
particle position to the intersection point. This should show if we
are finding the intersections approximately correctly. Also it 
should serve as a basis for displaying the flux accumulation. 
Flux accumulation is done for a set of surfaces that lie between
certain surface parameter limits. It makes sense to plot those limits.
One problem is that there is one more boundary than there are regions.
That might make saving in the negative index spaces of the flux data
a problem. Since arrays are uniformly spaced maybe we don't have to 
save the top limit. In any case, we need to
  Calculate and store the flux position array data.
  Develop data structures and code to do 3-D plot of intersections.

Sphere has already implemented values at the center of the intervals,
rather than the boundaries of the intervals. Might be tricky to
change, although it is only in fluxave that it is used. And there in a
non-general way. It is not too difficult to transform between bin
centers and bin boundaries when they are guaranteed to be uniform in
the quantity considered. In fact if we know the bins are uniform we
don't need to use the bin position arrays. Just the algorithm.  The
main problem is that we should not let things get too adhoc.  We
already have some stuff hard wired into the various fsect routines.
We need a programing interface turning flux index into cartesian
position if we are going to be able to draw a mesh of the flux
domains.  At the moment, even the sphere does not have this, because
what is stored in the fluxdata is the angle coordinate. The flux file
does not have enough information in itself to reconstruct the position
of the surface elements in 3-D. In fact perhaps we need to make sure
that the object input data is written back to a file, perhaps the flux
file, otherwise it will be too difficult to do post-run visualization.

Put object data into flux file at end. 

7 Jan 11

Developed plotting of objects and intersections. It shows that the 
intersections do indeed appear to be on the objects. Need a sphere drawing
object. Presumably an approach like the cylinder would do. 

Now we need the flux accumulation grid(s). It might make sense to draw
each surface element separately. Then they could be colored in (e.g.)
with the flux number color. To do this, we need a way of transforming
from the ibin number to the element position on the object, and then
to the extent (edges of) of the element. For curved elements one might
need interpolation along the curved edges. For uniform grids we can
get the edge grid values from the central values. 

8 Jan 11

Corrected algorithms for surface painting of cylinders, spheres and made
consistent with specified flux accumulations. Coloring so far just according
to flux bin.

The ppcom.f structure seems at this stage to be an unnecessary duplication
of what goes on in 3dcom.f. One could simply define the pp_ pointers 
relative to the start of the object, and then pass the object. As far
as I can tell. Got rid of it and just use the objg reference instead.

Finished implementing coloring by bin of sphere, cylinder, cube, pllelopiped.
Sphere has a fall-back to its plotting arcs if there is no flux.

Emacs register can make a register bookmark in a file by 
C-x r m <ret>
Then you can jump back to it by
C-x r b <ret>

Alternatively one can save positions in registers by:
C-x r <SPC> <char>
and jump back to that register position by 
C-x r j <char> (with the same character).

12 Jan 11

Change 3dcom.f to extend flux data addressing to one step more than
the maxsteps. That last step is for averaged data. (We don't write it out).
Fix fluxdatainit.

Change fluxave to do the averaging into that ff_data position.

We have sphereplot plotting the fluxaves but not really tested that
the are perfectly in the correct position.

pllel also plotting. This is somewhat better tested using the expedient
of putting ijbin into the value to ensure consistent ijbin. However, 
even that does not really test that what I am reconstructing as the 
position is the same as the position I started with. This is a real
headache. 

Next we need to devise a way to test that the positions are really being
plotted and tallied equivalently. 

13 Jan 11

Perhaps this is simply a matter of reducing the number of tallies till
they can be spotted individually. Yes that seems to work, and to be 
correct. It is hard to be sure the counting is _exactly_ right, but the
approximate variation of count is correct and that is presumably enough
to show the registration of the facets is correct. It does not prove
the binning of individual intersections is exactly right. And I don't think
they are exactly right for pllel.

The case 
 ./coptic -ofgeompllel.dat -ni5000 -s2 -dt.4
shows points that are plainly not being attributed to the right bin.
All the points are accounted for though. And things appear to be nearly
correct. Just that points are being moved to an adjacent bin. It's extremely
hard to diagnose, though. 

Save the ijbin with the endpoint, then draw the number on the figure.
That shows points are being moved, mostly within a face to wrong bins. 
There does not seem to be a consistent direction of motion. Some points
seem swapped. Judging by the apparent position of the intersection which
is on the face, the fraction is being returned correctly. It is ijbin that
is wrong. Both of these are determined by cubeexplt operating on normalize
xn1/2. Presumably that means that xn1/2 can't be wrong. 
FOUND. The problem was in cubeexplt using the wrong fraction balance
frac vs (1.-frac). I can't now see any errors. Wow that was a slog to
uncover the error I spotted 3 Jan.

Rationalize the facecoloring into facecolor routine.
Implement for spheres and for cylinders. 
Ok. Now all are done and all use facecolor.
Have to turn off the iosw if the plot is called for an object that
does not have flux saved. Also need to pass iobj, not ifobj, since
the latter might be zero/undefined if no flux accumulation. 
Might be better if no flux accumulation to draw a shaded object of 
all one color. There's no real reason to do multicoloring in that case
other than to see the shape of the object. 

14 Jan 11

Add in 3x3vectors after flux bug before odata end to accommodate 
gradients of the coefficients a,b,c, They are in the order c,b,a 
because usually the c will be the only thing varying with space.

Fixed some things that broke for the higher byte=3 case.

Now we are ready to implement gradient of potential. This would be 
considerably easier if the spheresect etc (in 3dobjects) operated
on specified x1,x2 rather than on mesh indices and directions.
If that were the case, we could rationalize the intersection finding
by using the routines that are in fluxdata.f. Then we'd merge the code
bases of these things. I think this probably should be done. But it's
going to take some work. 

Commit present version.

Do the swap to using sphereintersection. Gives identical flux density etc.
Do cubesect swap. Ok.
Do pllelosect swap. Interesting! The old version is actually broken in
some way and does not give the region map correctly. The swapped version
appears to work. Gives sensible region map. And a different flux. Probably
now more correct (though maybe not entirely).
Do cylsect swap. Fixed one fmin problem.
That more or less completes the swap.

Timings. Cubes and Spheres are so quick that it's not worth worrying about
them.

Cylinder shows that the new routine is a lot slower: 1.68s vs 0.99s.
This is substantially reduced to 1.1s by putting an outside-ends test
first in the cylfsect. That's probably as much optimization as is easy.
The old cylsect uses knowledge that either we are seeking 'radially' or
axially (i.e. axial difference is zero or else non-axial distance is zero) 
for field-aligned cylinder and lattice legs. That's not true in general.
But in any case the time is now nearly the same.

Spheresect is not quite right. It is that fraction needs to be set
to 1. if outside the allowable range of intersections, because
sphereinterp returns the actual fractions regardless. Done.

Cut off the obsolete routines. They might have ideas that are worth
keeping, though.

Pllel has box recuts and mesh adjustments with new routine. Not old.
But old takes longer and is wrong. So that's no test, and it's probably
not worth fixing the old. However the box recuts may be themselves caused
by an error. Apparently not by returned fractions outside the ranges.
Perhaps by not returning the minimum intersection correctly. 
There's no sign that the mesh shifting and scaling is reducing the 
box recut problem. I conclude it's not a mesh clash.
I think there's a logic problem. cubexplt returns the fraction closest
to x1, but also requires x1 to be inside the object. 

15 Jan 11

I haven't got to the bottom of this problem. It needs to be solved.
The recut arises from an SVD fit to 6 points cutting a box. Example:
 Warning: Box Recut  6 2 -1 928 18 20 17  0.978108108 Adjust mesh!
 Intersection fraction=  0.753565311 1 1 1 18 20 17 x=  0.753565311  0.  0.
 Intersection fraction=  0.0625383928 3 1 1 18 20 17 x=  0.  0.  0.0625383928
 Intersection fraction=  0.251363158 2 2 2 19 20 17 x=  1.  0.251363158  0.
 Intersection fraction=  0.25136292 3 2 2 19 20 17 x=  1.  0.  0.25136292
 Intersection fraction=  0.919081092 1 2 2 18 20 18 x=  0.919081092  0.  1.
 Intersection fraction=  0.616751194 2 2 2 18 20 18 x=  0.  0.616751194  1.
 0.754 0.000 0.000 0.000 0.000 0.063 1.000 0.251 0.000 1.000 0.000 0.251
 0.919 0.000 1.000 0.000 0.617 1.000
 6 svdsol  -0.312387854 -0.0157415178 -0.432068348 -0.477817297 -0.632711947
 -0.294701457  0.323155403 -0.0396499522  0.363257378  0.269468725 -0.239875913
 -0.794914365 -0.0854396746  0.0455816351 -0.605457604  0.0698276907
  0.624653518 -0.478513867
 1/fn=  1.11668324  0.978108108  3.25422573

These intersections appear to arise from all box points being inside the
object except 0 0 1, and 1 0 0. This is not consistent with there being
a single plane through the box. There must be two planes at different 
oblique angles, slicing out these two points. When the SVD fit is made to 
find a single plane best giving rise to these intersections, there's no
way to do it sensibly, so a stupid result is found that happens to 
cut one of the legs that really isn't cut. That's a recut. It does not
appear to be a coding bug, but rather a logic bug for a situation with
acute angles. That was not encountered before because only a pllel will
give it, and the pllel code was previously not working properly (apparently). 

What to do about it? Well the SVD fitting procedure is really not
useful right now because the whole idea of using the extra fractions
>1 to tell something about the geometry has been abandoned. Is there a
reason to try to avoid such situations by moving the mesh?
Maybe. Actually the case in point is more or less benign, and seems to be
unavoidable. I wonder if the whole boxedge thing is now a waste of time. 
It's far from clear that we should go ahead and set the intersection to 
1.01 as we currently do. Remove that for now.

Hopefully we now have a fully rationalized version that works with 
sphere, cube, cylinder, pllel using the flux intersection codes.

We now need to generalize the condition setting to account for 
coefficient gradients, if present.

This is quite simple (after avoiding a bug from variable name clash).
Implemented and working. 

Ok now we have going
   1. Flux plotting.
   2. Gradients of boundary conditions. 
we also already had:
   3. Collisions

Things still to do. 
       Magnetic Field
       Surface charges
       ...

17 Jan 11

Surface charges. The key question to decide on implementation is
whether we should immediately go to a dereferenced value of surface
charge, or whether we should proceed more simply initially. A
dereferenced charge would perhaps be stored with the flux data. It
might be a new fluxtype, although it is not obvious we necessarily
want to be forced to use all the momentum and energy flux accounting
inevitably if charge accumulation is being done too. In effect, charge
accumulation is simply integrating the charge flux, i.e. adding it
up. There might be an electron flux that also is subtracted. For that
to be calculated, we would need the (mean) potential of the surface
element each step. If we were using specified potential, that would
not be too much of a problem, but also we would then not need to do a
surface charge calculation. So there's actually a substantial
challenge in calculating the electron current to a flux element. We
need to get the potential solution, and then do some sort of
appropriate average over the facet. Since this needs to be done every
step, it must be fast.

We ought not to attempt to get the potential on a finer spatial scale
than the mesh. Therefore it would be reasonable to use every lattice
intersection with a particular facet to document the potential. Taking
all such intersections one could regard the facet electron flux as
given by the sum of the electron flux density for potentials at the
intersections, multiplied by the corresponding area. This would then
require getpotentials at all lattice intersections multiplied by fixed
coefficients representing the geometry (areas). The number of real
lattice intersections is maybe 1/3 of the total number of pointers
used, which might be up to 10% of the mesh points. Compared with a
single SOR iteration, this would therefore be managable. Does the
surface charge update need to be done during SOR iteration? Well, not
really if the charging time is long compared with timestep. But yes, if
it is short. If it is really short, there'll be various problems. 

Charging time is capacitance*potential/current. 

Capacitance (of sphere) is roughly Q/phi = 4 \pi \epsilon_0 /(1/r_p
+1/lambda_s). 

Current is of order 4 \pi r_p^2 e v_te n_e. 

So time scale is\epsilon_0/[r_p e v_te n_e(1+r_p/lambda)]
=  phi_p  \ep_0 /[r_p e n_e(lambda+r_p) omega_p]
=  phi_p  e \lambda^2/[ r_p T_e (lambda + r_p) omega_p] 

So if phi_p is normalized to Te/e the charging time is

phi_p  \lambda^2/[rp(rp+\lambda) v_t/lambda]
=phi_p  \lambda^2/[rp(rp/lambda +1 ) v_t]

In normalized time units, v_cs/r_p = 1. So v_te = smr r_p where smr is
the square root of the mass ratio: sqrt(1870). 

Thus charging time is 
     phi_p (lambda^3/r_p^2 (rp+lambda)) 1/smr 

If lambda<r_p and we are talking about ions so smr=1, then with
phi_p ~ 1 we get just the (lambda^3/r_p^2 (rp+lambda)) factor, which 
could get small rather quickly. For example if lambda=0.1 rp, it is 
about 10^-3, which will be smaller than any sensible ion step. 
What's more, since smr is maybe ~100, 1/smr is itself a small quantity
for electron charging. In order for the time to be of order 1, we need
to have lambda^2/r_p^2 ~ smr. That is lambda/r_p ~ 10.

Therefore, unless the debye length is >~10r_p, the charging time is 
less than 1 normalized unit. And unless it is >~r_p the charging time
is less than 0.01 normalized unit. (Which might be roughly a time step.)

Particles charge instantaneously on the ion timescale if lambda_D <~ r_p. 

Thus an implicit solution for the potential appears essential. The
surface potential usually tracks quite quickly to cause the electron
flux density to equal the ion. This amounts to saying that the ion
surface charge is quickly neutralized by electron currents until it is
consistent with the potential solution. In effect this amounts to what
I implemented in SCEPTIC: that the surface potential becomes equal to
the value required to make j_e=j_i. 
 
The problem is that the ion flux will be very noisy on short time
scales.  Therefore it may have to be averaged over many
steps. (Perhaps non-physical as far as transients are concerned).
Actually the ion flux changes on a relatively slow time scale, even if
the debyelength is short. Roughly on the timescale of one normalized
unit, because it is governed by the presheath, not the sheath. That's
a reasonable time over which to average the ion flux. What's more, if
the potential locally fluctuates faster than this, then it will act
mostly like random kicks: collisions.

Conclusion. To make ion simulations of transients, one needs the ion
flux to vary on roughly a normalized timescale of 1. It is justifiable
to average the flux over approximately this time scale to reduce the
noise. If the potential still fluctuates, this will give rise to an
unphysical effective collisionality. 


Given concerns about noise level of surface charge, it might not be
advisable to make the surface charge structure the same as that of
flux collection. There might be a reason to adopt a coarser grid for
surface charge than for flux collection. If that were necessary, it
would presumably be possible to do appropriate spatial averaging as
well as time averaging. So perhaps the structure does not matter so
long as it is coarser. There might be other approaches to surface 
charge approximation, though. For example one might want a surface
charge (component) on a sphere that was proportional to cos theta. 
Trouble is, low order moments (for example) are a whole different 
approach. They can be linearly mapped from facets, though. Also 
from the point of view of inputting surface charges from the object
file, there is every reason to expect that one might want to do that
with (for example) a simple gradient term. (Cos theta on a sphere). 


Actually object data already contains a reverse pointer to the object
in iinter_sor.  It looks easy to generalize that to include the ijbin
in that object.  In which case one probably has the reverse reference
required. As imagined earlier, the right place to implement new
code is in ddn_sor. At present all this does is add on the diagonal
and bdy contributions e.g.      dnum=dnum+dob_sor(ibdy_sor,ip). 
Presumably it could do far more if necessary. Actually this ibdy 
contribution is the only thing containing coefficient c. It is the sum
of -P.C/A over all directions, for Robin condition. If C were being 
changed dynamically, perhaps we could do a similar sum for adjustments.
[What happens if there is more than one contribution to this sum? 
That is not so unusual for an oblique surface. In that case we'd 
have to add up the new contributions from each intersection. But we'd
also have a different P in each direction, and we don't store it. Tricky.
The calculation of P (coef) is elaborate. Requiring two passes through
the stencil. In effect the whole cijroutine. ]

It seems as if one might need quite a lot of extra stored information
for an object that had variable boundary condition, e.g. potential. 
At the least one needs to store the Ps separately from the cij (in the 
bulk cij=P) so as to be able to adjust their boundary contribution.
Perhaps the way to do this is to implement a new type of object, and
handle the difference equation differently at its boundary. One might
chain extra object data to the end of this cijdata, when needed.

Or possibly just make more space (which would be less complicated):
Currently cijdata count is 24 total. If we add one (P) for each direction,
(x6) that would bring it to 30, a 25% increase, not too desperate. 
But that might not really be enough, since one really needs a pointer
to the variable c-data in each direction. Perhaps that pointer is what
one really needs. It could be a pointer to a total data structure which
has sufficient information to return whatever value is needed for this
adjustment. This could be embedded in ddn_sor through an examination
whether this pointer is non-zero. 

In effect, then, we need the dob_sor to include for each monitored 
point a pointer to extra structures that fully determine the additional
adjustment to be done in ddn_sor. This is more elaborate than what we
have at the moment which is simply for this point add on the denominator
and numerator adjustments, precalculated. 

What is needed in the new structure? 
     a reference to where am I going to get the variable data from
     what that variable data is in the form of
     any coefficients that are needed to add it, such as:
     the P's (i.e. coefs) in relevant directions

Do we need this for 2*ndims directions? One might assume that there 
is never an opposite intersection, so only ndims directions were needed.
But I think this is unnecessarily dangerous. So make it for every direction.
Then the actions called for in this new structure are 
     Look through all directions from point.
     If we need to for this direction
     	 get the variable data
	 multiply it by the relevant weight and add to adjustment
	 calculate the new c/a and b/a and store.

So for each direction we need 
     A flag to say whether we are doing anything here
     A pointer to the place where the variable data is (maybe same as flag). 
     Some coefficient(s). 
     At least 12 storage spaces. 24 ought to be plenty. 
Therefore we can use the same space and pointer arrangements that
are handled by objstart. But we would have a different meaning for
the data in the structure. We would only start this secondary data
in special cases where there was variable boundary value to be accessed. 

There's another issue. interpolations use the fraction, boa, and coa
information. If that is not updated, then the field finding is wrong. 
It has to be updated to include the effective c/a and b/a. (Fraction
does not change if the objects don't move.) ddn_sor could presumably
do that updating. 

Note that if the update consists of putting the boundary potential
equal to some value set by the ion flux, then it does not need to be
done every sor-iteration, only every time-step. Consequently, there
might be a better way of doing this that did the update, including the
update of the ddn_parameters idgs_sor and ibdy_sor and of coa, boa,
and then the iteration just uses those parameters for the entire
solution of this step. Given that the sor iteration is a substantial number
of steps, then doing the update over the entire cij array is a tiny
cost. This does not then accommodate iterative changes within the potential
solution to the boundary data. But it's not clear that's necessary. 
An approach that tracked the ion surface charge and then updated the 
total charge based upon electron flux might benefit from such a scheme,
but probably mostly if the potential is changing a lot in a timestep,
in which case, the potential control/adjustment seems to make more sense. 

Doing the update over the entire array would be wasteful if done every
time step, because of the additional searching for non-zero pointers.
Doing within sor updates is then less expensive (but maybe not by a
big factor). 

What happens if I want a dielectric sphere with surface charge? The two
contributions (polarization and free) to the surface charge might be
incompatible other than in the form of tracking the free. But then
I would not be tracking boundary potential. Seems an abstruse case. 

Summary. We can do the potential/surface-charge variation with a chained
structure of the same size as the existing cij extra data. It might need
to be accessed only once per time step. Quantities in the primary data
such as boa, coa need to be updated. In fact from the iteration's perspective
those parameters (and ibdy, idgs) once updated can be used identically 
to now. 


---------
Distraction. Get rid of old unused fillin. We are using fillinlin. 
Trying to figure out how to make it unnecessary to set pointer 
in all nodes of a box containing a node that really has an intersection. 
Notes say getpotential and fillinlin are broken. But I don't see how
fillinlin is broken by this. Or getpotential, which just uses it. 
They just use getfield. Maybe getfield is broken by it? 
Switching this code section off changes the flux density reported by
make (by a smallish amount so it's not clear why). That seems to 
show that there's something different. getpotential is only called
in padvnc, and setting the value called to zero in both cases does not
give an identical result. Therefore it is not the value returned by
getpotential that is affecting things. The notes of 21 July 09 describe
why I ended up with this test in. I suspect that getfield is where 
the issue actually lies rather than getpotential. Yes it is gradlocalregion
that needs the pointer information. Fix misleading comment. 
---------

Adjust the _sor data structure to include iextra_sor pointer to additional
data. 

Flux dependence encoding of the input data:
ocenter, oradius, ovec, ocontra etc remain with the same meanings.
oabc presumably have different meanings.
ofn1-3 etc remain as before. 
Perhaps ofluxtype changes currently it means the number of fluxes.
Anyway, we currently have 3 parameters (oabc) that can be used to 
specify how the BC depends on flux. They are used in setting some
of the default code a/b ... Perhaps we simply default to the same
and use the otype to tell us what to do. 

The standard thing to do is to set potential to be such that the
electron flux is equal to the ion flux. For this we need the mass
ratio over Z SCEPTIC has

         flogfac=0.5*alog(2.*pi/(rmtoz*1837.))

Actually flogfac depends on magnetization. The above is for zero B. 

           fluxofangle(j)=finthave(j)*(nthused)/
     $              (4.*pi*rhoinf*dt*r(1)**2)

           phi(imin,j)=(alog(fluxofangle(j))+flogfac
     $                    +(ncs-1.)*phi(imin,j))/ncs

So this is averaging phi over something like ncs(50) steps. Maybe finthave
has already been averaged? Yes it has. flogfac is the log of sqrt of
(2.*pi/(rmtoz*1837.)), and the solution is such that
fluxofangle = sqrt(2.*pi/(rmtoz*1837.))exp(phi)

fluxofangle is the number of counts per unit time per unit area normalized
by rhoinf. We need to know the area of our facets to get flux density. 
Fluxes are acquired by all nodes in fluxreduce. 


19 Jan 11

A key question, it is now clear, is whether one should implement new 
routines for the dynamic update of the cij parameters during iteration
(triggered by flags) or whether one should use the same cij setting 
code we currently have, but only once per time step. In effect, calling
the cij routine for a variable-BC object might be what one wanted to do
right from the get go. Doing so once a timestep might not be very 
costly especially if there were a way to skip nodes that were known not
to be affected by variability. If this view were taken, then one would
embed the variability into potlsect setting of conditions. 

The weakness with this approach is that it is wasteful to determine
the intersection on each iteration, since that is not going to change
for stationary objects. So we would definitely not want to do potlsect
exactly as it now exists. We really need a way to store at least the
reference to the flux bin for each direction. That is, the geometry 
is one calculation, the flux is another, the conditions depends on flux,
but the geometry does not change. We might however go through whatever
conditions calculation is implemented at each timestep.

Decision is to set iobj,ijbin and some other stuff for each direction
intersected within a chained code during initial cijroutine, then
reset what is needed separately.

Implemented chaining and iobj,ijbin setting.
Use spherefsect for sphere cij because we now need the ijbin.
Had to move the fluxdatainit before cijroutine otherwise the ijbin
is not calculated correctly.

It seems we perhaps ought to use the 1-nf_posdim locations to store the
facet areas, because it is not straightforward to calculate them on the
fly. I previously increased nf_posdim to 3 because of needing cube positions
in 3-d. But actually, I haven't even implemented the calculation of that.
Instead the positions are being deduced algorithmically. The areas
are significantly harder to deduce algorithmically and probably ought
to be stored.

Ellipse arc length for major and minor axes a,b as a function of angle
parameter theta is sqrt dl= (dx^2+dy^2) = sqrt(a^2 sin^2 + b^2 cos^2) dtheta
= a sqrt(1+ (b^2/a^2-1)sin^2theta)  dtheta
 is  a.E(theta,k) where k=sqrt(1-b^2/a^2),
E(theta,k)= \int_0^theta sqrt(1-k^2sin^2theta) dtheta

It's all too messy. For now just flag that the calculation is not right
if the axes are not identical. There's a calculation in my notes for
a case where two of the axes are the same length. But for now just 
divide 4pi r**2 by the total number of facets (theta and psi). That's 
correct for a sphere. Probably one could get the surface area numerically
using the same sort of scheme as used for the objplot, adding up multiple
quadrilaterals. 

Pulled the area data into the third slot of dob_sor. Seems to work. 

So now the plan is when evaluating at node ij either in the iteration, 
or more likely in a separate coefficient update step:
If cij iextra set
   for 2*Nd directions
      if iobj ne 0 reevaluate potential by
         get flux using ijbin, iobj, and time average
	 divide by area, dt, and coefficients
	 calculate corresponding potential
	 reenter the relevant main cij parameters boa, coa, ...?
      endif	
   endfor

20 Jan 11

For recalculating the cij parameters, it seems it would make sense if
we used some of the same code that is currently in cijroutine. We
don't need to do the boxedge section. And we don't need to do the
potlsect section. But we do need the dplus deff section perhaps. 
After we've recalculated the abc. No. This is too cumbersome. Really 
what we need is to have the coef for all directions stored (this is 
in cutcartesian notation P). Then most of cijroutine is redundant.
Probably we need to store coef in the third slot of dob_sor instead
of the area, and get the area from the flux location directly. If so,
then we will need to know the object number.

Implemented cijupdate (a code for mditeration). It seems to work
setting the potential boundary condition to the local floating potential. 

Time costs with cijupdate 5.423, without 4.759 for 20 steps of
standard settings. These are very fast steps with ni=100k. Costs are
not zero but hardly significant. They probably could be improved by not
iterating over the entire cij mesh, only over the stored data.

Did a test with different initial potential. It converges to the same
answer.

Did some tests with different -l and -v they look qualitatively very
sensible. These are with 5x5 flux facets.

Also tests with different flux facet numbers up to 20x20 and down to 1
seem to work.  Thus we have the ability to do floating spheres as well
as insulating.

Now I have to establish whether the absolute values make sense.

First. Rewrite this so that it does not iterate over the whole of the
mesh, instead it just iterates over the auxiliary data up to oi_sor.
This should take negligible time. It seems to give just the same 
result. Good.

Timing. Old update 5.234. New 5.201. This shows the real cost even of the
mditerate version is very small. I think the differences observed above
had more to do with changes in the sor solving because of different phi
that with the additional code execution. Still it is more elegant
to do the update sensibly rather than looking everywhere. 

With -s500 and 
1025 , 1.,0.,5., .0,.0,.0,  1.,1.,1.,   1, 5, 5, 
we get:
nrein,n_part,ioc_part,rhoinf,dt= 2394    96132    98569   100.014     0.100

 Average flux quant  1, object  1, over steps 250 500, per unit time:  1730.514
 rhoinf:  100.014366 Total: 43436  Average collected per step by posn:
    6.76    7.03    6.70    6.77    6.65    6.96    6.70    6.81    6.98    7.28
    6.93    6.93    6.80    6.96    7.27    6.96    6.95    6.77    7.01    6.90
    7.06    6.96    6.89    6.94    7.08
 Flux density*r^2, normalized to rhoinf  1.37690246

phiexamine shows the probe potential is about -2.6-7

A very profitable day. Thanks be to God!

21 Jan 11

Doing a hand calculation of the floating potential. The flogfac for hydrogen
is -5.678. If area is .5, rhoinf=100., dt=.1, count=5, then fluxdensity=1,
and so the floating potential is 0.5*flogfac=2.84. That's just what the 
phiofcount is getting. I think it's about right.

There's an issue with the whole approach for a floating object. It is
that we might well wish to obtain the angular dependence of the flux
in an object that was a floating equipotential. That could not be done
simultaneously with the present implementation. It could perhaps be done
by introducing an additional ghost object for the flux accounting. But
that's a bit cumbersome. 

Implemented a type 5 to do floating equipotential by summing the flux 
and area over the object.

The the local flux is implemented only for spheres. 
To make things work for other objects, we need to initialize their 
facet areas, which is not done yet.

Now we have going
   1. Flux plotting.
   2. Gradients of boundary conditions. 
   3. Collisions
   4. Insulating surfaces.
   5. Floating objects.

Things still to do. 
       Magnetic Field
But
       Surface charges
does not seem so pressing because of the considerations that have shown
that floating/insulating objects are not well treated by using a surface
charge. Their potential should be controlled by flux density. 

Magnetic field implementation.
I think it is essential to be able to have the magnetic field point
in any direction. The way to prescribe this is to have a vector B(ndims)
and to have switches -Bx -By -Bz (default B=0). ndims is not defined
in plascom. I guess we need to set it: nplasdims.
Ok built code to read in those components and calculate Btotal. 

The cyclonic integrator does rotation of the Larmor radius and velocity 
about the (total) magnetic field by an angle \Omega.dt. We should regard
the B as \Omega in normalized units. It has components and a total. 

For rotation, we simply need to multiply the vector by the rotation matrix.
Wikipedia says that if u is a unit vector in the direction of the axis
the rotation matrix is 
R =[ cos(t)+ux^2(1-cos(t)),ux.uy.(1-cos(t))-uz.sin(t),
					ux.uz.(1-cos(t))-uy.sin(t);
   ...;
   ...]
= uu + cos(t)(I-uu) + sin(t) u_cross = I cos(t)+uu(1-cos(t))+u_cross sin(t)
where 
      u_cross = [ 0, -uz, uy; uz, 0, -ux; -uy, ux, 0]
and this rotation when applied by premultiplication on vector v
is said counterclockwise about the direction of u. 

In sceptic3d Leo allows the B field to have a cosine angle cB so that
the following rotation supposedly makes it in the new z-direction
temp=xp(2,i) xp(2,i)=temp*cB-xp(3,i)*sB xp(3,i)=xp(3,i)*cB+temp*sB the
1-component is unaffected, so it must be being presumed that the B
field has zero component in the 1-direction. I probably don't want to
assume that. It's more straightforward to use the general rotation,
but I'm not sure how bad the extra cost will be. Also, the external
ExB drift is subtracted off before getting the gyro radius. That's
reasonable and might be needed here. 
When should one subtract off ExB drift and when not? Probably only when
it is constant.

Got most of the B-field advancing going (without ExB drift). The ions
appear to be rotating counterclockwise when viewed along the field. 
This decided by using -go and figuring out which way the particles are 
moving along their orbits.
That's correct polarity. Seems to be working. Magnetic field implemented.

Good Day!

Needs: area calculations for non-spheres.

24 Jan 11

Parallelopiped area. Face area is magnitude of vector product of the
two edge vectors. So 4 times the "radius" vector cross product. This
is then divided by the product of the facet numbers in either direction.

Implemented for parallelopiped and cube (pretty much the same).

Cylinder? Ends are each pi r^2. Side is 2pi r 2z. We have issues
with unequal radial radii as with the sphere. Don't account for
ellipses yet. Implemented.

Now we need to increase the nf_posdim again because we need to store
3 position references in addition to the area. 
Reorganized the references to be through mnemonics nf_pr nf_pt nf_pa...

Increase. Ok. Implement cylinder consistently. Done. 

Cube/Pllel positions not yet done. 
Cube done. But I'm not sure that cube is really working right.
There was a nf_posdim logical error when it was increased.

Ran out of steam. Cube may be done, but there may be an error in the 
flux read/write since it looks very different from the in-code plot.

The Average flux quant 1 printout is the same. 
The in-code flux of step plot is crazy.
Found a bug in that, which was old. Correct it.

Found that the problem with cube plots from fluxexamine is that 
nf_faceind is not being written and restored. Actually it is 
now incorrectly defined as dimension
      integer nf_faceind(nf_quant,nf_obj,2*nf_posdim)
nf_posdim is not the right dimension, since it is now detached from 
the dimensionality of the position info.  The same is true of 
nf_dimlens. This confusion needs to be cleared up. In some places 
it needs to become nf_ndims.
Then implement writing and reading back of nf_faceind. 
We get correct display from fluxexamine. 

Hopefully we are now done except for pllel positions.

25 Jan 11

Add some switches to fluxexamine.
Test coptic with two processors. Seems to be working.
Put zoomrotate into objplot to improve viewing.

Commit. 

Still to do: cylinders of arbitrary orientation.
Input: otype, ocenter (3), axisvec (incl length) (3), radius (1)
Do we need more than one radius?
Could have two radius vectors, but that would be a real headache to 
calculate correctly. Also we don't currently handle unequal radii. 

Algorithm for intersections with general cylinder:

	Get fractional distances from center in the axial direction as
a fraction of the axial length (multiply by inverse axial vector?).
Get the distances from the axis divided by the radius. This is then
rendered into unit cylinder coordinates. Implement unit cylinder code.
This could then be used for the other cylfsect too, if
desired. Actually we need both radius and angle in the transverse
plane.  (Or 2-D cartesian position). Where are we going to measure the
angle from? Need a stable and complete specification. Perhaps we 
project the x-axis on the radial plane? If x aligns with axial, this
becomes singular. Use a different axis then? y?

Might be best to construct the contravariant vectors from the input.
Such that v_contra.x = cartesian coordinates in a frame in which z
is along the axial direction and others are normal to it.

26 Jan 11
Implemented cylinit to initialize the contravariant vectors. Needs a
second vector to be prescribed.

Started to implement normalized cylfsect. It's not working. But also,
I've found a bug in the old one with intersection binning.
Fixed the bug. Improved the cylinder end facet labeling.
Found the problem with normalized cylfsect and fixed.

After further corrections I seem to have cylfsect working to the point
where the volumes and cij plots look sensible. Objplot is not yet working.
It needs a reverse transform from the unit cylinder coordinates back
to world coordinates. With pllel this was automatically given by the 
covariant vectors. In our case, the covariant vectors are not automatically
present, because the specification of the cylinder is not done by them. 
Actually the covariant z-vector is present, but in the wrong place. 
We could make the initialization routine fill in the covariant vectors,
but that would overwrite the read-in data. Would that be a problem?
Probably not. Because they are orthogonal, the covariant vectors are equal
to the contravariant vectors divided by their magnitude squared. It 
would therefore be possible to store the three magnitude but pretty 
cumbersome. 

Although installing the contravariant vectors works, it erases the 
radius. That's a problem, I think.  Maybe not.

Positioninit done just in terms of the normalized cylinder (for now?)
Now we have intersections working and giving flux quantity in bins. 
Implement world3contra and contra3world to transform from world to
contravariant coefficients and back. 
Got cylgplot working using these transformations. It is rather
duplicative of cylplot, but never mind for now.

I think that's it. We've got general cylinders implemented.
Add to geommany.dat

We have a Tallyexit error on object 5, a coordinate-aligned cylinder.
It does not happen with the old cylfsect. So there's a bug in the
new one that does transformation to unit cylinder. Must fix.
But for today, I'm tired.

28 Jan 11

That seemed to be an error in the transformation into unit cylinder code.
At least there certainly was an error and now the Tallyexit has gone
away.

The flux plotting in objplot probably would be more useful if it
colored by flux _density_ rather than flux. This is different for
non-spheres, for which areas are not all equal. At the moment, we
store the averages in maxsteps+1. It would be possible to store the
averages divided by areas in maxsteps+2. Implemented.

Actually I've been using maxsteps+1, but that's not what I want. I want
to use nf_step+1 (and nf_step+2) which is the number of actual steps+1/2
because otherwise when writing/reading one does not do so for the relevant
data. Changed that.

Implemented -gw switch to control coloring and annotation of surfaces.

29 Jan 11

Clean up. Commit.

We now need to get back to thinking about forces, I think.  At the
moment forcetrackinit is implemented only for spheres.  Stress
integrations are based upon the surfobj structure which has, for each
facet, position and area vectors. The number of facets in the theta and
phi directions are defined in 3dcom.f Other object types are
ignored. In principle that's all we really need to do for other
surfaces to work. Likely the force evaluation works better somewhat
away from the bounding surface, where the interpolation is more accurate.

Compiled with gfortran and got rid of most warnings but not all. 
Also corrected one bug found thus. It's still a factor of 2 slower.

31 Jan 11

I think my approach to averaging is incorrect. The code assumes that the
data has been initialized to zero in the new slots we are writing in. 
Hence if fluxave has been called, and has written in the current slot
that's a mistake. We need to write it just past the last step, not the
current step. This is a problem because the last step is not kept in 
common. It is determined by nsteps in the main program. Since fluxave
is only called at the end, this will not cause a problem unless we
change that.

Fix addressing bug in sphere positioninit.

Dealing with another bug in flux accumulation that shows up when
using floating sphere. There appear to be random negative flux entries
in the ff_data ahead of where we are. 

Found an error in spherefsect that only mattered when the sphere was
not centered. This is the first time I've collected flux for a non
centered sphere.

1 Feb 11

Running some test cases with fairly big domains and point charges
to try to see something about force. Found a bug in that the 
field calculation did not use the object center. Corrected. 
Trouble is that the forces seem to be down in the noise level.
These are for point charges with -.04 at r=5. Which is pretty small.
Boost that to -.4 at r=5. There there's what looks to be non-zero
field force. Much more on the back particle, which probably makes
sense given the particle repulsion. But still there's very big
fluctuation in the particle momentum component presumably because the
collection crossings are just a few per step.

Does this make sense? there are 400k particles in volume 130x50x50
so density is about 1 (actually 2 because two processes). Reported 
rhoinf 2.35 about right. At velocity 1. timestep .1 there is going
to be about pi*.1*rho ~ 1 crossing in each direction per step for a
radius 1 sphere. So yes, it makes sense. We need a smaller domain
for debugging the forces. 

2 Feb 11

The point charge influence is not currently included in the force
calculation, because addfield is not included in the fieldatpoint.
What does that mean? It means the charge ought to include only the
shielding charge, not the point charge. That's a bit puzzling in
respect of the sign of the apparent charge (negative). Ought to be
positive I think. Actually the charge is calculated using field at
point times surfobj. 


The charge is in general 4 pi r^2 times the gradient at surface
(averaged). If the Yukawa approx holds, it is phi(r) 4pi r^2*(1/r +
1/lambda). So for r=1, phi=-2, lambda=1. it should be -50.3 for an
actual object. For a point charge there's a question.

Here is the result for a series of radii: 1. 1.2 1.5, 2 3
at -l100
    Field,       part,       press,       total,     charge:
================== End of Object 1 ->  1
     0.44564    -0.10514     0.00000     0.34050   -24.09440
================== End of Object 2 ->  2
     0.34017     0.19731     0.00000     0.53748   -25.00404
================== End of Object 3 ->  3
     0.15454     0.37895     0.00000     0.53349   -25.09287
================== End of Object 4 ->  4
    -0.00017     0.09210     0.00001     0.09194   -25.04920
================== End of Object 5 ->  5
     0.09503    -1.49080     0.00002    -1.39575   -25.02007
These are correct to the analytic result 25.1 to within half a percent
except for the first object which is the radius 1 sphere itself,
and we are extrapolating to the boundary for that, so we expect it to
be less accurate, perhaps.
For -l1.
    Field,       part,       press,       total,     charge:
================== End of Object 1 ->  1
     0.40185    -0.81520    -0.00044    -0.41379   -42.66593
================== End of Object 2 ->  2
     0.15460    -0.40849     0.00360    -0.25029   -41.78420
================== End of Object 3 ->  3
     0.01874     0.10315     0.00845     0.13035   -37.08116
================== End of Object 4 ->  4
    -0.04777    -0.45493     0.01364    -0.48906   -28.70999
================== End of Object 5 ->  5
    -0.02963    -0.76805    -0.05416    -0.85184   -15.78975
This is a bit less than the analytic value, but quite likely this is 
a combination of non-linearity and boundary extrapolation error.

Here is what we get with a point charge of -2. at  1. with -l100
    Field,       part,       press,       total,     charge:
================== End of Object 1 ->  2
     0.28349    -5.29697     0.00000    -5.01348   -24.91903
================== End of Object 2 ->  3
     0.29647    -5.88335     0.00000    -5.58688   -24.97408
================== End of Object 3 ->  4
     0.09009    -5.08781     0.00001    -4.99771   -24.91823
================== End of Object 4 ->  5
     0.10640    -4.33492     0.00003    -4.22849   -24.87956
Looks as if the point charge IS counted in the charge. Yet force seems
pretty crazy. This is what we get with -l1. :
    Field,       part,       press,       total,     charge:
================== End of Object 1 ->  2
     0.05172     0.63756    -0.00032     0.68896   -16.00428
================== End of Object 2 ->  3
     0.07639     0.63138     0.02616     0.73394   -12.89612
================== End of Object 3 ->  4
     0.02346     0.17503     0.12118     0.31967    -8.94730
================== End of Object 4 ->  5
    -0.00932     0.89757     0.13519     1.02344    -4.66365
Seems rather a lot lower than I might have expected, yet perhaps
the extra shielding inside r=1 is the reason. As an approximation,
The potential would be exp(-1) smaller from that effect:
at r=1 we have 42.6/16.0 = 2.66 ~ exp(1) so it makes sense.
Looks as if the charge is being correctly calculated including the 
point charge. 

I think this makes sense because the pointcharge I am using extends
only to radius 1. That's the place where the effective pointcharge has
gone to zero. Consequently, my calculation outside of that region should
be correct even though the extra point charge field is not accounted for.

If I were inside that radius, then I would expect the charge to be wrong.
Here's what we get for a ptch radius 2. potl -1.
    Field,       part,       press,       total,     charge:
================== End of Object 1 ->  2
     0.00696     0.62275    -0.01049     0.61922    -2.68375
================== End of Object 2 ->  3
     0.03578     0.47410     0.01403     0.52391    -5.96094
================== End of Object 3 ->  4
     0.02476    -0.06874     0.11036     0.06638    -8.62990
================== End of Object 4 ->  5
    -0.00941     0.87602     0.13223     0.99884    -4.60279
Basically it agrees in charge for r>=2.

Conclusion: Maxwell stresses work only outside the defined size of 
a point charge. 

Here are the force summaries for r=1.2 to 3. for  -v1. -s500 -l5.
    Field,       part,       press,       total,     charge:
================== End of Object 1 ->  2
     4.73587     6.05562     0.02025    10.81173   -24.35178
================== End of Object 2 ->  3
     3.85421     6.99056     0.04728    10.89205   -24.04704
================== End of Object 3 ->  4
     2.68490     8.25658     0.12276    11.06424   -23.23796
================== End of Object 4 ->  5
     1.17217    10.02062     0.35843    11.55121   -21.32471

Looks fairly well converged, and field is significant component
especially close. 

Here's a case with ptch r=.5, -4. with flux collection at .8, 1. 1.2 1.5 2 3
    Field,       part,       press,       total,     charge:
================== End of Object 1 ->  2 Averaged over steps 250 500 :
     5.12855     4.30121     0.00359     9.43335   -22.22561
================== End of Object 2 ->  3 Averaged over steps 250 500 :
     4.40905     5.04523     0.00984     9.46412   -21.94547
================== End of Object 3 ->  4 Averaged over steps 250 500 :
     3.86179     5.66072     0.02109     9.54360   -21.92307
================== End of Object 4 ->  5 Averaged over steps 250 500 :
     3.13189     6.43115     0.04756     9.61061   -21.57910
================== End of Object 5 ->  6 Averaged over steps 250 500 :
     2.17909     7.48203     0.11928     9.78039   -20.83145
================== End of Object 6 ->  7 Averaged over steps 250 500 :
     0.96240     8.96126     0.33762    10.26127   -19.09183
It's a bit lower. Is that reasonable? Not so clear. Mesh spacing is 
10/32 ~ .3. Not much smaller than the point charge size. Might be an 
issue. Try with larger nmesh 10/50
    Field,       part,       press,       total,     charge:
================== End of Object 1 ->  2 Averaged over steps 250 500 :
     6.37185     4.49729     0.00322    10.87236   -24.86204
================== End of Object 2 ->  3 Averaged over steps 250 500 :
     5.47498     5.41855     0.00930    10.90283   -24.67407
================== End of Object 3 ->  4 Averaged over steps 250 500 :
     4.74671     6.18794     0.02015    10.95480   -24.52661
================== End of Object 4 ->  5 Averaged over steps 250 500 :
     3.83298     7.12687     0.04705    11.00690   -24.05890
================== End of Object 5 ->  6 Averaged over steps 250 500 :
     2.66614     8.40752     0.12212    11.19578   -23.31898
================== End of Object 6 ->  7 Averaged over steps 250 500 :
     1.17042    10.17546     0.35528    11.70116   -21.41303
Yep. Looks as if we did not have sufficient resolution.

Evidence is that the force and charge is working pretty well for -l5
out to r ~ 2.5.  Possibly there are influences from the boundaries
outside that, but even so one ought to have momentum conservation,
so there is something to explain in that the force has increased
by about 9%.

Try with -dt.05 half the timestep:
    Field,       part,       press,       total,     charge:
================== End of Object 1 ->  2 Averaged over steps 250 500 :
     6.18294     5.36787     0.00312    11.55393   -24.85392
================== End of Object 2 ->  3 Averaged over steps 250 500 :
     5.33291     6.22361     0.00905    11.56556   -24.66480
================== End of Object 3 ->  4 Averaged over steps 250 500 :
     4.64108     6.94924     0.01963    11.60995   -24.51566
================== End of Object 4 ->  5 Averaged over steps 250 500 :
     3.75168     7.87032     0.04595    11.66795   -24.04460
================== End of Object 5 ->  6 Averaged over steps 250 500 :
     2.61367     9.13149     0.11955    11.86471   -23.29948
================== End of Object 6 ->  7 Averaged over steps 250 500 :
     1.14207    10.87260     0.34823    12.36290   -21.38397
Force looks substantially better converged. But also substantially higher.
Charge is hardly different. I wonder if the electron pressure term has
the right sign. 

If it were opposite, the force conservation would be very good.
Pressure force is: 
            force(i)=force(i)+surfobj(koff+ndims+i)*exp(phi)
Maxwell force is:
         do i=1,ndims
            do j=1,ndims
               stress=field(i)*field(j)
               if(i.eq.j)stress=stress-es/2.
               fieldforce(i)=fieldforce(i)+surfobj(koff+ndims+j)*stress
            enddo
         enddo
Thus the magnetic pressure is applied with a sign -es/2. whereas the 
electron pressure is applied with a sign +. These seem to be opposite.
Which can't be correct. Not sure. Need to think more about this.
The EM force density on a volume is Div.T where T= \epsilon_0[EE - E^2/2]
By contrast the pressure force density is -grad p. Thus the E^2/2 term
ought to be added to the force with the SAME sign as p. 

Try with -dt0.025.
    Field,       part,       press,       total,     charge:
================== End of Object 1 ->  2 Averaged over steps 250 500 :
     6.01259     6.14960     0.00303    12.16523   -24.84747
================== End of Object 2 ->  3 Averaged over steps 250 500 :
     5.22286     7.01671     0.00881    12.24838   -24.65675
================== End of Object 3 ->  4 Averaged over steps 250 500 :
     4.56712     7.71234     0.01917    12.29862   -24.50600
================== End of Object 4 ->  5 Averaged over steps 250 500 :
     3.69267     8.63389     0.04504    12.37160   -24.03256
================== End of Object 5 ->  6 Averaged over steps 250 500 :
     2.57710     9.83186     0.11744    12.52640   -23.28122
================== End of Object 6 ->  7 Averaged over steps 250 500 :
     1.11903    11.51102     0.34266    12.97271   -21.35380
Seemingly no better converged. Modify the code sign and rerun:
    Field,       part,       press,       total,     charge:
================== End of Object 1 ->  2 Averaged over steps 250 500 :
     6.01259     6.14960    -0.00303    12.15916   -24.84747
================== End of Object 2 ->  3 Averaged over steps 250 500 :
     5.22286     7.01671    -0.00881    12.23077   -24.65675
================== End of Object 3 ->  4 Averaged over steps 250 500 :
     4.56712     7.71234    -0.01917    12.26029   -24.50600
================== End of Object 4 ->  5 Averaged over steps 250 500 :
     3.69267     8.63389    -0.04504    12.28153   -24.03256
================== End of Object 5 ->  6 Averaged over steps 250 500 :
     2.57710     9.83186    -0.11744    12.29152   -23.28122
================== End of Object 6 ->  7 Averaged over steps 250 500 :
     1.11903    11.51102    -0.34266    12.28739   -21.35380
This is consistent to better than 1% Clearly the change is correct.

It looks as if the magnitude of the step size does have a strong 
influence on the apparent force. So does the mesh size. 
Basically the force now seems to be working for spherical measuring
surfaces.

The old force paper says that the domain has to be at least twice
lambda_D to get the force right. So these domains aren't big enough.

3 Feb 11

Trying again with floating sphere. Still have a problem with negative
counts, apparently arising from spurious ff_data value entries.
I thought I'd fixed that. Yes I had. I was not excluding the floating
sphere from the region. That was an input file error.

Here's -l5 -t.1 -v1. -dt.05 on a 10x10x10 size 50x50x50 number
 Flux density*r^2, normalized to rhoinf  1.56599867
    Field,       part,       press,       total,     charge, over steps 250 500
========== Object 1 ->  1 radius=  1.000 ========
    52.40706    32.18951     0.00168    84.59825   -36.26878
========== Object 2 ->  2 radius=  1.200 ========
    42.01913    35.39398    -0.02149    77.39163   -36.11956
========== Object 3 ->  3 radius=  1.500 ========
    34.98729    41.46753    -0.10783    76.34700   -35.83485
========== Object 4 ->  4 radius=  2.000 ========
    25.78210    50.83405    -0.44894    76.16722   -34.85485
========== Object 5 ->  5 radius=  3.000 ========
    12.82122    65.19434    -1.89569    76.11987   -31.77205
Sceptic got force 81.8, charge 37.7 fave=1.50
So coptic is getting a slightly smaller force and charge, with a slightly
higher flux, which presumably makes the floating potential a bit less
negative and might explain the force and charge differences. 
According to my calculation the floating potential in coptic is -2.39048
In sceptic it is saved as -2.44023. 

Using instead -dt.025 we get:
 Flux density*r^2, normalized to rhoinf  1.46931636
 Floating potential= -2.45420861
    Field,       part,       press,       total,     charge, over steps 250 500
========== Object 1 ->  1 radius=  1.000 ========
    52.92072    31.25940     0.00173    84.18185   -36.16894
========== Object 2 ->  2 radius=  1.200 ========
    42.38842    34.59715    -0.02204    76.96353   -36.01539
========== Object 3 ->  3 radius=  1.500 ========
    35.25710    40.69879    -0.11040    75.84549   -35.72120
========== Object 4 ->  4 radius=  2.000 ========
    25.90463    50.27276    -0.45853    75.71886   -34.72077
========== Object 5 ->  5 radius=  3.000 ========
    12.78950    64.85056    -1.92698    75.71309   -31.56680
========== Object 6 ->  6 radius=  5.000 ========
     1.84271    80.31524    -6.17617    75.98178   -23.68394
A lower floating potential but no better force agreement. 
Looking more likely that the discrepancy is domain size.
Try instead with -dt.1 :
 Flux density*r^2, normalized to rhoinf  1.56204593
 Floating potential= -2.39300942
    Field,       part,       press,       total,     charge, over steps 250 500
========== Object 1 ->  1 radius=  1.000 ========
    53.72910    33.49794     0.00167    87.22871   -36.49997
========== Object 2 ->  2 radius=  1.200 ========
    43.07132    36.76583    -0.02142    79.81573   -36.35361
========== Object 3 ->  3 radius=  1.500 ========
    35.86864    42.91519    -0.10785    78.67598   -36.07600
========== Object 4 ->  4 radius=  2.000 ========
    26.39515    52.41656    -0.45035    78.36136   -35.10577
========== Object 5 ->  5 radius=  3.000 ========
    13.21550    66.83462    -1.90827    78.14185   -32.04988
========== Object 6 ->  6 radius=  5.000 ========
     1.88130    82.67591    -6.19397    78.36324   -24.44571

Using a domain 15.^3 we get (-s500 -dt.1 -da8 -v1. -l5. -t.1)
 Flux density*r^2, normalized to rhoinf  1.55521822
 Floating potential= -2.39739013
    Field,       part,       press,       total,     charge, over steps 250 500
========== Object 1 ->  1 radius=  1.000 ========
    56.00081    33.54607     0.00416    89.55104   -36.63660
========== Object 2 ->  2 radius=  1.200 ========
    44.98659    36.85350    -0.02179    81.81830   -36.48862
========== Object 3 ->  3 radius=  1.500 ========
    37.71225    43.03333    -0.11243    80.63316   -36.20971
========== Object 4 ->  4 radius=  2.000 ========
    28.08273    52.67813    -0.47333    80.28754   -35.23322
========== Object 5 ->  5 radius=  3.000 ========
    14.53309    67.52608    -2.03984    80.01932   -32.14792
========== Object 6 ->  6 radius=  5.000 ========
     2.86774    84.40074    -6.98550    80.28297   -24.58187
Thus the shortfall is to do with the domain not being big enough. 

Actually, judging by the potential plot the biggest problem is having
enough room on the downstream side. 

Try a different layout:
91,1,15,36,50,0,-10.,-3.,3.,10.
92,1,15,36,50,0,-10.,-3.,3.,10.
93,1,8,29,50,0,-5.,-3.,3.,20.
    Field,       part,       press,       total,     charge, over steps 250 500
========== Object 1 ->  1 radius=  1.000 ========
    49.05901    30.66676     0.00020    79.72597   -39.57825
========== Object 2 ->  2 radius=  1.200 ========
    38.68083    33.98512    -0.01521    72.65074   -39.46384
========== Object 3 ->  3 radius=  1.500 ========
    31.12519    40.37292    -0.07865    71.41946   -39.26323
========== Object 4 ->  4 radius=  2.000 ========
    20.89518    50.49304    -0.33271    71.05552   -38.45021
========== Object 5 ->  5 radius=  3.000 ========
     6.01135    65.98265    -1.30534    70.68866   -35.77306
========== Object 6 ->  6 radius=  5.000 ========
    -7.66802    79.41839    -1.35357    70.39680   -29.23763
Rather unsuccessful. No! I accidentally set the -t to .05.
Here's from -n 8 with  -s500 -dt.1 -da4 -v1. -l5. -t.1 -ri70
91,1,15,36,50,0,-20.,-3.,3.,20.
92,1,15,36,50,0,-20.,-3.,3.,20.
93,1,8,29,50,0,-10.,-3.,3.,20.
 Flux density*r^2, normalized to rhoinf  1.51346183
 Floating potential= -2.42460632
    Field,       part,       press,       total,     charge, over steps 250 500
========== Object 1 ->  1 radius=  1.000 ========
    54.36328    33.12401     0.00478    87.49207   -36.79696
========== Object 2 ->  2 radius=  1.200 ========
    43.79966    36.23051    -0.02060    80.00957   -36.65427
========== Object 3 ->  3 radius=  1.500 ========
    36.79764    42.14454    -0.10813    78.83405   -36.38494
========== Object 4 ->  4 radius=  2.000 ========
    27.46853    51.54213    -0.45674    78.55392   -35.42867
========== Object 5 ->  5 radius=  3.000 ========
    14.17500    66.08906    -1.97374    78.29031   -32.40749
========== Object 6 ->  6 radius=  5.000 ========
     2.64290    82.94704    -6.85779    78.73215   -24.91580
One can see that the leading edge at z=-10 is in fact significant in
the potential solution. It looks as if even the leading edge needs
to be far away. 

With
91,1,15,36,50,0,-20.,-3.,3.,20.
92,1,15,36,50,0,-20.,-3.,3.,20.
93,1,15,36,50,0,-20.,-3.,3.,20.
we get 
 Flux density*r^2, normalized to rhoinf  1.54259562
 Floating potential= -2.40553951
    Field,       part,       press,       total,     charge, over steps 250 500
========== Object 1 ->  1 radius=  1.000 ========
    55.57957    33.41187     0.00557    88.99701   -36.60782
========== Object 2 ->  2 radius=  1.200 ========
    44.81544    36.56225    -0.02148    81.35622   -36.46081
========== Object 3 ->  3 radius=  1.500 ========
    37.71716    42.56567    -0.11317    80.16965   -36.18245
========== Object 4 ->  4 radius=  2.000 ========
    28.22109    52.14386    -0.47762    79.88733   -35.20405
========== Object 5 ->  5 radius=  3.000 ========
    14.76352    66.94378    -2.07169    79.63561   -32.12032
========== Object 6 ->  6 radius=  5.000 ========
     3.12687    84.30839    -7.06419    80.37107   -24.36137
Looks like we've got what we'll get. 

Increase to -l10 (floating. 20.^2)
 Flux density*r^2, normalized to rhoinf  1.58223844
 Floating potential= -2.38016534
    Field,       part,       press,       total,     charge, over steps 250 500
========== Object 1 ->  1 radius=  1.000 ========
    82.47508    34.32501     0.00254   116.80263   -32.96266
========== Object 2 ->  2 radius=  1.200 ========
    69.27230    36.64183    -0.00902   105.90512   -32.95042
========== Object 3 ->  3 radius=  1.500 ========
    62.79748    41.54189    -0.04652   104.29284   -32.96438
========== Object 4 ->  4 radius=  2.000 ========
    54.03246    50.05941    -0.19974   103.89213   -32.73141
========== Object 5 ->  5 radius=  3.000 ========
    39.37125    65.06497    -0.96813   103.46809   -31.79218
========== Object 6 ->  6 radius=  5.000 ========
    20.33151    88.01800    -4.43408   103.91543   -28.86920
Compare with sceptic:
==> T1m1v100r20P02L1e1.dat <==  
 dt       vd       Ti  steps   rhoinf     phiinf   fave  debyelen  Vp / nr/ phi
0.05000 1.00000  0.1000  2000    190.8047    5.25125  1.5857 10.00000  -2.36912
 Charge      E-field       Electrons      Ions     Total
 -34.1119576  0.76158464 -6.75517142E-10  35.8557892  112.014252
 -3.82497454  0.0045971889 -52.9793701  163.565552  111.045898
Agreement is not great. But one can see that we have not incorporated
the whole of the positive potential peak. And not even all rho peak. 

T=.01 -l5 floating: T1m2v100P025L5e0z020x20.flx
 Flux density*r^2, normalized to rhoinf  1.6818924
 Floating potential= -2.31908631
    Field,       part,       press,       total,     charge, over steps 250 500
========== Object 1 ->  1 radius=  1.000 ========
    73.62286    41.47784     0.00801   115.10871   -36.11523
========== Object 2 ->  2 radius=  1.200 ========
    58.49369    45.84773    -0.02990   104.31152   -35.93894
========== Object 3 ->  3 radius=  1.500 ========
    48.03215    54.48299    -0.15543   102.35971   -35.60561
========== Object 4 ->  4 radius=  2.000 ========
    34.47293    68.05175    -0.64462   101.88007   -34.51732
========== Object 5 ->  5 radius=  3.000 ========
    15.89227    88.27644    -2.70578   101.46294   -31.17728
========== Object 6 ->  6 radius=  5.000 ========
     0.27191   110.13190    -8.57866   101.82516   -22.76466
Sceptic equivalent:
==> T1m2v100r20P02L5e0.dat <==
dt       vd       Ti  steps   rhoinf     phiinf   fave  debyelen Vp  / nr/ phi
0.05000 1.00000  0.0100  2000    206.2824    5.32925  1.5893  5.00000  -2.37589
 Charge      E-field       Electrons      Ions     Total
 -37.1715889  2.51376629 -1.71099135E-10  40.6259079  103.470062
 -10.1660223  0.0073696305  47.7507477  52.3004951  100.235481
Flux agreement is not too good, but force is good. 

With the above meshes, the dz is about 6/20 at the probe, i.e. 0.33. 
Thus, the resolution near the probe edge is not great. There is a very 
steep gradient of density near the edge behind the probe. That seems 
probably to be the cause of the momentum apparent nonconservation. 
It seems that one probably needs to be at least one times dx from the
boundary for the force calculation to be reliable. 
Increase the z-mesh density to
91,1,15,36,50,0,-20.,-3.,3.,20.
92,1,15,36,50,0,-20.,-3.,3.,20.
93,1,15,58,72,0,-20.,-3.,3.,20.
Result:
 Flux density*r^2, normalized to rhoinf  1.67777085
 Floating potential= -2.32153988
    Field,       part,       press,       total,     charge, over steps 250 500
========== Object 1 ->  1 radius=  1.000 ========
    65.69753    41.45311     0.00996   107.16059   -35.68893
========== Object 2 ->  2 radius=  1.200 ========
    58.64346    45.80416    -0.03127   104.41637   -35.92920
========== Object 3 ->  3 radius=  1.500 ========
    48.44096    54.44807    -0.15888   102.73014   -35.62333
========== Object 4 ->  4 radius=  2.000 ========
    34.61152    68.05910    -0.65155   102.01907   -34.54094
========== Object 5 ->  5 radius=  3.000 ========
    15.96391    88.41527    -2.73084   101.64834   -31.24056
========== Object 6 ->  6 radius=  5.000 ========
     0.26093   110.37730    -8.62403   102.01421   -22.76320
The error at the sphere is decreased by more than a factor of 2. But there's
little or no change at radius 1.2. Thus the result is not substantially 
better. But it does say that provided we avoid the object itself we
don't need to go to high resolution. 

Try to document the effect of walls. Move the particle to position x=10
in a 20^3 domain. We get
[./coptic test1.dat -s500 -dt.1 -da4 -v1. -l5. -t.1 -ri60]
 Flux density*r^2, normalized to rhoinf  1.53941882
 Floating potential= -2.40760088
    Field,       part,       press,       total,   ave over steps 250 500
========== Object 1 ->  1 radius=  1.000 ========  Charge=  -36.6542
     0.56419    -0.30757     0.00007     0.25669
    -0.11836     0.05651    -0.00001    -0.06186
    55.26081    33.32751     0.00551    88.59383
========== Object 2 ->  2 radius=  1.200 ========  Charge=  -36.5060
     0.50242    -0.29941    -0.00023     0.20277
    -0.10309     0.06281     0.00005    -0.04023
    44.48288    36.54395    -0.02127    81.00556
========== Object 3 ->  3 radius=  1.500 ========  Charge=  -36.2226
     0.46840    -0.23163    -0.00126     0.23551
    -0.12979     0.09512     0.00033    -0.03434
    37.31798    42.62213    -0.11201    79.82810
========== Object 4 ->  4 radius=  2.000 ========  Charge=  -35.2406
     0.41672    -0.22586    -0.00571     0.18516
    -0.18482     0.17877     0.00205    -0.00400
    27.90334    52.16618    -0.47223    79.59729
========== Object 5 ->  5 radius=  3.000 ========  Charge=  -32.1702
     0.37379    -0.15386    -0.03115     0.18878
    -0.17052     0.15924     0.01519     0.00391
    14.49882    66.91687    -2.04443    79.37125
========== Object 6 ->  6 radius=  5.000 ========  Charge=  -25.6056
    -4.11204    17.45297   -12.43861     0.90232
    -0.11563    -0.01325     0.08904    -0.03984
     5.31671    81.13351    -6.06404    80.38618
There is an x-force but in the reliable regions it is only about 0.2
compared with the z-force of 80. This gives a feel for how much the 
proximity of a wall might affect us. About 0.2%.
Not a lot when we are half way out. Incidentally, the last object is
centered at x=12, not on the particle center. That's the reason for the
big field and part components separately (presumably).

12 Feb 11
Tried a long run on sceptic machine. Got tallyexit error with -v1
(not with -v1.5)
0091 149-0.001| 0092 150-0.076| 0093 147-0.094| 0094 145 0.056| 0095 152-0.012| 
0096 148 0.109| 0097 158 sd problem, fraction,type,No  0.  1. 1 6
 Tallyexit error 1 721545 6 113
 xpart,r=  4.45770645 -3.3805542  52.4564285  5.59457731 128
 xp1 -0.0105438232  0.00775885582 -0.114601135  0.013090915
rank 7 in job 54  sceptic.psfc.mit.edu_59249   caused collective abort of all ra
nks
  exit status of rank 7: killed by signal 9 

Tracked this down after a great deal of effort to a problem in spherefsect.
We are getting fraction =1+ from the sphereinterp, but this is actually 
a situation that inside_geom has decided is a crossing because a point
is exactly on the boundary. Then I was not setting sd to 0 but I was 
returning without calculating ijbin. The unset value of ijbin was being
used for an address with an overrun. The simple fix is to put sd=0.
Then tallyexit decides there's an error and I've now made it discard the
particle if such an apparent error arises. This works to prevent crashes
but with 8 processes 30M particles total, the number of such tallyexit
errors is annoying. One every 20 steps maybe. I'd prefer a better solution
that did not seem like an error. It seems that the rounding differences
arise because of the differences in inside_geom and sphereinterp 
calculations. 

Got some cases to run. There seems to be more y-force coming from the 
particles than is easily accounted for. It ought to be zero by symmetry.
On the second run this doesn't happen. Hmmm...

Made adjustments to spherefsect so that it permits fraction up to 1+tiny,
but only if sd is not equal to zero. Thus it errs on the side of allowing
a slight over-run of fraction when deciding whether to calculate the ijbin.
It is in principle possible for calling programs to reject cases that
are allowed through on that basis, because fraction is left slightly 
higher than 1, even though ijbin is calculated. This is not debugged
because it is only on major runs that this problem arises.

29 Mar 2011

Idea is to calculate the force on point charge object(s) from the field
(and any non-local contribution perhaps if any) within coptic and put it
out in the flux file.
 
At present obj_geom(ofluxtype) is zero for point charges. And we do 
no flux accumulation, which means that they are not accounted for in
the mf_obj list. One would wish to leave the point charge masked out
of the region calculation, but have it listed in the mf_obj list.
Also ns_flags needs to be set to something. 
When read back, we need to set the address so this is null:
      read(23)(ff_data(i),i=1,nf_address(1,1,nf_step+2)-1)
Now the obj_geom(ofluxtype) doubles as the number of flux quantities
mf_quant. A point charge for which we wish to calculate the force 
should have mf_quant = 0 so there is no flux accumulation, but a value
of ns_flags that tells it to get the field force and multiply by the
charge. In order to leave the point charge out of the calculation, we
retain the idea that ofluxtype=0 simply turns it off. However we implement
a test in the flux initialization that detects a point charge and if the
ofluxtype is 1 we turn on force calculation but still set mf_quant to zero.
Implemented in fluxdata.f. 

Seem to have got this going including writing and reading back. However,
although the force is in the right ballpark, it seems too low, by up
to a factor of 2 in some minimal runs on small domains with T=1.
At lower T, results look better. Probably need some big-domain runs
to check consistency more thoroughly. I find that the point field force
estimate is in fact much less susceptible to noise than the others.
That's a good sign. It isn't enough to run 100 steps with 3M particles.
This is neither steady nor sufficiently overcoming the noise. 

We need to do major tests on sceptic machine.

Here's the v=1 result for r_p=2, r_p\phi_p=1 on a 110x25 size mesh.

    Field,       part,       press,       total,   ave over steps 750 1500
========== Object 1 ->  1 radius=  2.000 ========  Charge=  -12.5664
    -0.13769     0.00000     0.00000    -0.13769
     0.00107     0.00000     0.00000     0.00107
    20.98632     0.00000     0.00000    20.98632
========== Object 2 ->  2 radius=  5.000 ========  Charge=  -10.7190
    -0.06480     1.21881     0.02813     1.18214
     0.00021    -4.02067    -0.00296    -4.02342
     4.49055    27.66972    -3.72163    28.43864
========== Object 3 ->  3 radius=  4.000 ========  Charge=  -11.0850
    -0.07555     1.25581     0.01572     1.19599
     0.00655    -4.03701    -0.00099    -4.03144
     6.79924    24.02388    -2.30635    28.51677
========== Object 4 ->  4 radius=  3.000 ========  Charge=  -11.9277
    -0.08995     1.28759     0.00703     1.20467
     0.00827    -4.05602     0.00034    -4.04740
     7.48934    20.37780    -0.95641    26.91072
========== Object 5 ->  5 radius=  2.000 ========  Charge=  -11.9285
    -0.09585     1.31110     0.00195     1.21720
    -0.01691    -4.00246     0.00022    -4.01915
    10.30984    16.43853    -0.25337    26.49500
========== Object 6 ->  6 radius=  1.000 ========  Charge=   -3.7371
    -0.03220     1.30913     0.00019     1.27712
    -0.00306    -4.01847     0.00001    -4.02153
     4.50617    11.54300    -0.02968    16.01949

This seems to show that the local field calculation is very significantly
lower than the integration over spheres. I'm not terribly happy with the
agreement across the radii: from 26.49 to 28.51 either. We know that 
r=1 is going to give a wrong answer, and maybe r=2 is suspicious, but the
others ought to agree. 

Looks like the mesh spacing outside the inner region is as great as 
1.6 or so. This might be a bit big. But in any case the inner region is
better resolved. I don't understand. There are comparable discrepancies 
at v=1.5.

Went back to laptop and using geomforce and geomforce2 which has twice
the resolution, I seem to get agreement between the point measurement
and the surface measurements at -v1. -t.1 -l10 that improves with resolution. 
At 64^3 we get (200 steps on a +-5^3 domain).
========== Object 1 ->  1 radius=  1.000 ========  Charge=  -12.5664
     0.23548     0.00000     0.00000     0.23548
    -0.12161     0.00000     0.00000    -0.12161
    14.90358     0.00000     0.00000    14.90358
========== Object 2 ->  2 radius=  1.000 ========  Charge=  -12.2314
     0.12728     0.33798    -0.00012     0.46514
    -0.15539    -0.46727     0.00017    -0.62249
     7.63848     7.31926    -0.01387    14.94387
========== Object 3 ->  3 radius=  0.500 ========  Charge=   -3.8205
     0.01426     0.46699     0.00000     0.48125
    -0.01897    -0.54087     0.00001    -0.55983
     3.31843     4.22225    -0.00121     7.53946
========== Object 4 ->  4 radius=  2.000 ========  Charge=  -11.9920
     0.11133     0.40623    -0.00188     0.51568
    -0.02513    -0.51600     0.00148    -0.53965
     4.09672    10.80441    -0.11957    14.78156

-l5:
    Field,       part,       press,       total,   ave over steps 100 200
========== Object 1 ->  1 radius=  1.000 ========  Charge=  -12.5664
    -0.02662     0.00000     0.00000    -0.02662
    -0.07211     0.00000     0.00000    -0.07211
    13.33594     0.00000     0.00000    13.33594
========== Object 2 ->  2 radius=  1.000 ========  Charge=  -11.9781
     0.00245    -0.02488     0.00016    -0.02227
    -0.03551    -0.38794    -0.00005    -0.42350
     5.79478     7.75847    -0.04958    13.50367
========== Object 3 ->  3 radius=  0.500 ========  Charge=   -3.7635
    -0.01770     0.04770     0.00002     0.03002
     0.01322    -0.46171    -0.00002    -0.44851
     2.68749     4.63474    -0.00459     7.31764
========== Object 4 ->  4 radius=  2.000 ========  Charge=  -11.0783
     0.01740     0.04902    -0.00096     0.06546
    -0.02187    -0.42372     0.00293    -0.44265
     2.55158    11.36046    -0.39078    13.52126

-l2:   Not so good!
    Field,       part,       press,       total,   ave over steps 100 200
========== Object 1 ->  1 radius=  1.000 ========  Charge=  -12.5664
    -0.08077     0.00000     0.00000    -0.08077
     0.03787     0.00000     0.00000     0.03787
     8.46597     0.00000     0.00000     8.46597
========== Object 2 ->  2 radius=  1.000 ========  Charge=  -10.6575
    -0.00736    -0.14493     0.00136    -0.15093
     0.00222    -0.00104    -0.00155    -0.00037
     1.70747     7.27180    -0.16739     8.81188
========== Object 3 ->  3 radius=  0.500 ========  Charge=   -3.4301
    -0.01574    -0.09479     0.00024    -0.11029
     0.01126    -0.05549    -0.00014    -0.04437
     1.09785     4.89843    -0.02017     5.97612
========== Object 4 ->  4 radius=  2.000 ========  Charge=   -7.3739
     0.01402    -0.06781    -0.00556    -0.05934
    -0.00764     0.00118     0.00398    -0.00247
     0.16782     9.49809    -0.81142     8.85449
-l1
    Field,       part,       press,       total,   ave over steps 100 200
========== Object 1 ->  1 radius=  1.000 ========  Charge=  -12.5664
    -0.01964     0.00000     0.00000    -0.01964
     0.04107     0.00000     0.00000     0.04107
     4.86549     0.00000     0.00000     4.86549
========== Object 2 ->  2 radius=  1.000 ========  Charge=   -8.0949
    -0.01932    -0.01404     0.00764    -0.02572
     0.01614    -0.05005    -0.00875    -0.04266
     0.21513     5.27831    -0.26057     5.23287
========== Object 3 ->  3 radius=  0.500 ========  Charge=   -2.5923
    -0.00825    -0.00650     0.00088    -0.01388
     0.01075    -0.06147    -0.00104    -0.05176
     0.27169     3.98918    -0.04810     4.21277
========== Object 4 ->  4 radius=  2.000 ========  Charge=   -3.6314
     0.00393    -0.00332     0.01446     0.01507
    -0.00176    -0.07535    -0.00118    -0.07829
    -0.05562     5.75820    -0.54000     5.16258
-l.5
    Field,       part,       press,       total,   ave over steps 100 200
========== Object 1 ->  1 radius=  1.000 ========  Charge=  -12.5664
     0.03778     0.00000     0.00000     0.03778
     0.07824     0.00000     0.00000     0.07824
     2.57617     0.00000     0.00000     2.57617
========== Object 2 ->  2 radius=  1.000 ========  Charge=   -4.2711
    -0.00236     0.23551     0.01142     0.24457
     0.00280     0.05457     0.00158     0.05895
    -0.02972     2.93894    -0.21933     2.68990
========== Object 3 ->  3 radius=  0.500 ========  Charge=   -0.6572
    -0.00026     0.27244     0.00152     0.27369
     0.00268     0.06801    -0.00461     0.06607
    -0.01624     2.56229    -0.09923     2.44682
========== Object 4 ->  4 radius=  2.000 ========  Charge=   -1.5672
    -0.00067     0.28145    -0.00817     0.27261
    -0.00131     0.05740    -0.00729     0.04880
    -0.00465     2.72331    -0.04260     2.67606

Switching to uniform grid in geomforce2 we get -l1 rather worse.
    Field,       part,       press,       total,   ave over steps 100 200
========== Object 1 ->  1 radius=  1.000 ========  Charge=  -12.5664
     0.01859     0.00000     0.00000     0.01859
     0.01352     0.00000     0.00000     0.01352
     4.62026     0.00000     0.00000     4.62026
========== Object 2 ->  2 radius=  1.000 ========  Charge=   -8.1152
    -0.02191    -0.04323     0.01097    -0.05417
     0.01764     0.06283    -0.00849     0.07198
     0.20519     5.45752    -0.26392     5.39879
========== Object 3 ->  3 radius=  0.500 ========  Charge=   -2.6212
    -0.01194    -0.01557     0.00079    -0.02673
     0.00766     0.06462    -0.00086     0.07143
     0.29040     4.08004    -0.04873     4.32170
========== Object 4 ->  4 radius=  2.000 ========  Charge=   -3.7099
     0.00518    -0.03094     0.02029    -0.00547
    -0.00061     0.03528    -0.00171     0.03296
    -0.05509     5.89466    -0.52665     5.31291

Examining the raw ug, in the case of -l1 it has a negative peak of magnitude
-1, and not very smooth in the vicinity of zero. Therefore its electric field
is actually varying very quickly in the vicinity of the axis and just 
picking the r=0 value off is not going to be very accurate. It's a bit 
surprising. Even with -l10 the negative potential peak on ug is very big.
Nearly -2. Why is that? 

Run on sceptic with 33 cells across +-6 gives result that has good agreement
between all the full-sphere forces and about half the error with the 
point force (~10% versus ~20%) cf the much coarser mesh. Thus discrepancies
do arise in the force estimates when the resolution is insufficient and or
there are mesh size changes in the interesting areas. Also the point force
calculation is not reliable even with a cell size of 0.36 with charge size
2 and -l10. 

This explains some of the strange results during the runs we've done for
transverse force. It's basically a problem with mesh resolution. 

4 Apr 11

To explore this further we need a routine that reads back the field file
and then uses the force structures to calculate the field average over
a series of spheres of different size. I think it would be a mess to 
use phiexamine itself. We should create a new analysis program. This can
use some additional routines in stress that are designed for this case.

A problem is where we should get cij from, since it is not stored in the 
phi file. Actually cij is only needed near an object boundary. We ought to
be able to use a simple field evaluation getsimple3field. Yes we can
but that means we need to change the routine in stress.f not to use
fieldatpoint.

Seem to have got this going but the results are all over the place.
We get completely different answers from pha and ua files. None of this
makes much sense. I think there must be bugs. Yes wrung them out. 
Now measuring the field force on at 4pi charge by averaging the field
over a sphere of radius r gives from both ua and .pha files:
r=    0.0100 Fieldforce   -0.043290    0.041318  -14.475240
r=    0.0200 Fieldforce   -0.041516    0.039458  -14.473969
r=    0.0500 Fieldforce   -0.036141    0.033882  -14.470119
r=    0.1000 Fieldforce   -0.027171    0.025364  -14.436084
r=    0.2000 Fieldforce   -0.016220    0.019686  -13.873970
r=    0.3000 Fieldforce   -0.025746    0.032109  -12.923309
r=    0.4000 Fieldforce   -0.035169    0.044168  -11.885883
r=    0.6000 Fieldforce   -0.037832    0.068202  -10.156130
r=    0.8000 Fieldforce   -0.041531    0.080158   -8.874148
r=    1.0000 Fieldforce   -0.052499    0.093228   -7.850270
This is a case that gives an average point force of 14.6507,
and at r=1 a field force of 7.50117 on charge of 12.1702. 
The latter is quite close to the force 7.85027 when charge is 12.5664.

I conclude that there is not a big interpolation error in the fieldforce.
Instead the fieldforce is actually too low by a significant amount.
This appears to be because the asymmetric charge that gives rise to 
the drag force is actually very significant close to the point. 
It actually peaks at about z=0.3. Therefore if one has cell size of
0.15, the charge is not being well resolved there within the Poisson 
solver. The charge-to-mesh spread is smoothing it substantially. 
There is also, by the way, a structure in the charge on this small scale. 
This is quite alarming if one wanted to use the field directly to 
estimate forces. Of course I don't. I use the particle momentum flux.

Interim Summary

Direct field evaluation for the point charge does give a force that is
less susceptible to fluctuations than the particle momentum flux
included in evaluations at larger r. However, it is instead
susceptible to substantial systematic errors apparently arising from
lack of resolution of the mesh charge in the vicinity of the point.
Perhaps if one had a finer resolution in the vicinity one might be
able to escape this problem, but I have a gut feeling one might at the
same time get back the noise problem.

2 June 11

Finished first draft of RefManual.tex and cvs added it.
Moved the fieldave.f file responsible for the above data to /testing
and cvs added it. 

Added dealing with a perpendicular velocity to the Bfield section
by subtracting and adding the perpendicular drift around the cyclotron
move.

Also discovered a bug in the cyclotronic mover. Sign error fixed.
Implemented a fall-back to summed acceleration if the Bfield is very weak
otherwise rounding in the subtraction of gyro radius becomes too large.

Profiled the costs. Adjusted placement of sin and cosine calls to make
sure they aren't done unnecessarily, since they otherwise would tend to
become a dominant factor in the costs. As it is, they are now much smaller
than the field interpolation costs.

Seems as if we have working cross-field (FxB) drifts.

Moving back to wakesmalllambda on sceptic. The initial runs were done
with -l.2 which is resolved by the mesh. We might be able to go another
factor of 2 to -l.1, but then we are going to have to go to quasineutral.
So tried quasineutral.

There's a display problem in that the potential inside the object is set
to 0, even when the object's boundary potential is non-zero. 

Actually it's not clear that there's much advantage in going all the way 
to quasineutral. Once -l is smaller than the mesh size, we are close to
getting the same result anyway. The number of iterations is small, so 
there's no big advantage in going to QN. 

The boundary condition at the probe is something of an issue. Provided the
probe is attracting, relative to the local potential, then probably there
is some place between the boundary and the first set of nodes where the
effective boundary position is located. Thus one can think of the BC
position as being the uncertain point. It seems unlikely that one can do
very much better than this by some sort of square-root shape assumption. 

26 Jun 11

Improved the testing built into make to compare prior potential file
if one exists. 

11 Aug 11

Fix reinjection outside particle region simply to retry.

Implement "infinite" magnetic field case (>1.e5) which amounts to
one-dimensional dynamics along B plus a fixed drift velocity perpendicular
to B.

There's a problem concerning reinjection with this implementation. It
is that no particle ever moves perpendicularly out of the region, but
there are particles that are reinjected perpendicularly. That
perpendicular reinjection must be stopped. I think. When B is finite,
we follow the Larmor orbit and constant escape and reinjection happens
across the field. Therefore the old cartreinject is ok (except maybe
for the drift velocity, must check, it seems to be ok).

As far as reinjection is concerned, in the infinite-B case, the drift
is purely along B, but so also is the random motion. Therefore the 
statistics of reinjection do seem to be rather different. 1-Dimensional.
It looks as if this needs changes only to cijinit which sets up the
statistical arrays. These are based upon (possibly) shifted maxwellian
flux and distributions from the functions ffcrein and fvcrein, which
assume the drift to be in the z-direction and have equal temperatures
in all dimensions.

Instead we want functions where the drift consists of two parts:
parallel and perpendicular to B projected in the coordinate directions:
vpar*Bfield(j)+vperp(j).
Also the thermal velocity (spread) must be the parallel velocity
projected into the coordinate direction. I.e. it is multiplied by the
direction cosine Bfield(j). 

Implemented new functions ff1crein and fv1crein and put in calls to them
if Bt is "infinity" meaning >1.e5


Plotting orbits we seem to get sensible results both with high B (<1.e5)
and with infinite B (>1.e5). Perhaps the issue with reinjection was not
so bad, since the orbits become nearly straight with high B well before
the new code is triggered. Of course I am not actually looking at reinjected
particles for the most part. 

I think the problem is that if the timestep becomes longer than the gyro
period, then during each step the particle actually makes (multiple) cycles
that when it is near the edge are in and out of the region. These exits
are not captured. Each of them would give rise to a reinjection of the 
particles that are close enough to the edge that their Larmor orbit 
intersects it. There are then multiple issues:

(1) Because the timestep is too large, even a particle whose Larmor orbit
intersects the boundary might statistically be able to move into the region
because it might take several steps with gyrophase such as to be inside
the region. In other words, just looking at the initial and final positions 
of a timestep is badly mistaken about whether or not one leaves the 
region during the timestep. 

(2) If particles are placed "just" inside the region, where the
distance inside is bigger than the Larmor radius, then they are artificially
prevented from making the multiple entrances and exits. 

If all this is right, then it appears that the reinjection scheme is 
already erroneous when B>~1/dt. I wonder if there's a way to correct it
in intermediate cases, rather than immediately going to B=infinity.
If not then evidently infinity ought to be considered to be a lot smaller
than 1.e5. More like 1/dt.

15 Aug 2011

One way to think about this for the edge of the computational boundary
is to do a calculation of whether the Larmor orbit intersects the
edge.  If it does, then during a time step of dt, a total of Nc=\Omega
dt/2\pi transits in and out of the region occur. There's no point in 
_actually_ reinjecting the particle Nc times, because we are trying to
model a region that is part of a bigger plasma. However, one might 
count the times that the particle would have transitted, and consider
that many reinjections to have occurred. 

The problem is that some surfaces have many of these type of
injections, and some don't. Consequently, the probabilities for edge
face at which to reinject need to be adjusted in the light of the 
reinjections already taken into account by the cyclotron process. 

One ought also to distinguish between solid surfaces, which would remove
the particle immediately in the first cycle, and computational boundaries
which would not. 

Clearly the effective statistics depend upon the timestep length dt. 
Perhaps one could set up the probability distributions to account for
this in the first place, but they would then depend upon dt, so 
acceleration would not be appropriate. 

18 Aug 11

Worked more on the infinite-Bt case, because the analytic efforts on the 
intermediate cases yield only rather uncertain interpolations.
Had to implement also the rhoinfcalc and ninjcalc appropriate for the
projected parallel distributions. 

Tests with -gi flag show plausible results. Particles are injected with
a spread of parallel velocities and with perpendicular velocity essentially
equal to the drift. The spatial distribution of injection appears to be
inconsistent with expectations. Zero on faces with no v- or B- component, 
but insufficient on the z-face into which the flow is directed when
B is in an orthogonal direction.

That's because I omitted to scale the distribution when projecting 
it so that the total area is constant. Did that. Then get sensible
result. Tried some vt2min adjustments. Smaller is better, but if you
make it too small then it can lose the integration entirely. 1e-5 is
safe and gives only about .002 flux through the side walls. That's
unlikely to be significant. The minimum v-difference with 2000 points
in the integration (starting at -2,2) is <1e-3. When vt2min=1e-6,
then the maximum value of arg is <1 so certainly there will be a nonzero
position. I think 1e-6 is safe. It gives flux .0007. That's pretty much
negligible.

NOW: we seem to have infinite-B reinjection working.

Start working on copticthin which is going to be a thin domain to
do effectively 2-D calculations with infinite Bt. We can't currently
make the z-direction thin because the drift velocity is purely in that
direction. So make the x-direction thin. 

Even with a mesh of four nodes in the thin direction there are big
problems with No Good Vertices if the object is as big as the domain.
And there are boxinterp zero weight everwhere errors. That's with a
sphere. Change to a cylinder, as is my plan. Still get those boxinterp
errors. It looks as if I am not rejecting ions initialized inside the 
cylinder, since I am using only 20650 tries for 20000. This is a 
radius 1 cylinder in a box 10x10. So that fraction is pi/100, which
is about right. So no. The fraction of rejections is reasonable.

Found the problem to be the special particle-1. It was being set outside
the mesh. Moved it in pinit so as to catch this sort of error and give
a warning.

Now we can use down to 4 cells in the thin direction. 3 gives a
sorelax error. But we ought not to need to use sorelax for
quasineutral, so setting -l0 this runs too. 2 won't initialize.
Recall that the first and last nodes in each dimension are ON the boundary. 
Their volumes are set to zero because the presumption is that they 
are going to be set by boundary conditions, not by solving Poisson's 
equation. psumtoq works only on non-edge nodes.

Seems to be working with quasineutral 3 nodes in x-direction up to 
velocity of 5. Trying 10 gives Getlocalregion excessive error and
field corruption. Arising because a potential is minus infinity.
I suspect that must be because the density happens to be zero. 
Probably we need some sort of truncation in the quasineutral 
calculation. Yes, added a floor to the density before taking its log
and the error goes away.

Tests with 400k particles on a 3x32x64 mesh give fluctuations of density
that are roughly 0.07. Particles per cell are something like 400k/32x64
=200 which seems to be statistically correct. 

With mesh allocation 4x100x100,   time is 43s. For 100 steps of 0.4M
With mesh allocation 100x100x100, time is 46s. Not that much
different!
4x200x200 allocation 44s.
Hardly changes when the used mesh sizes are doubled.
4x200x200 used: 1m6s. Slower.

Clearly it is possible to run with large grids without much explicit
penalty other than the need for more particles. 


21 Sep 2011

Took a long detour based on the realization that the stability calculation
is in many ways better done by analytics, than relying on PIC. Did not
come to a definitive answer but 

Trying to get back to this thin case for cylindrical operation in
general.  For the infinite-B situation there was no need to solve the
problem of ions leaving along an axis of translational symmetry,
because the drift velocity would normally be set to zero in that
direction. However, for finite-B if one wants to do a cylindrical
calculation, then periodic reinjection ought to be used for exits
through the symmetry faces. Otherwise one loses lots of orbits in a
perpendicular direction if the domain size is literally thin. An
alternative is to regard the domain as extremely large in the symmetry
direction (even though having only few cells). If large enough then
very few particles leave across it. This is almost equivalent to
reducing the symmetry-direction velocity of the particles. This sounds
like an easier way to solve the problem. 

Change the nameconstruct to use the y-length as the second parameter
that way the long x-domain does not screw name up. Seems to sort of work
with a long (100) x-domain and shorter in other directions. The inherent
inefficiency of using 4 x-nodes is that the ends take up 1/3 of the charge
which are then not used because the potential there is set from the 
boundary condition. So even if we average the potential in the x-direction
thereafter, we will have lost some efficiency. However, this does not 
seem a very serious problem. I therefore conclude that the thin version
is actually able to do cylindrical problems reasonably well. The bigger
inefficiency is probably in the particle moving where three rather than 
two field components are used and the interpolation over three dimensions
is probably a non-trivial cost. 

Did a commit. Big mistake. This version was not correct with a standard
copticgeom.dat. Mditerate error. Mditermults error. Ok this seems to be
caused by using ifull too small. Yes. Fixed by increasing to a decent
number. Ought to have a more sensible check. Implemented in meshconstruct
and in readgeom.

4 Jan 2012
Fixed some bugs in pex file writing where arrays were not being 
properly reinitialized for subsequent writes after the first.

6 Jan 2012

Rationalize the boundary setting code in bdyroutine.f. Add slpcom.f
and the ability to set BC type from command line.
Encapsulate the command-line argument handling code and remove it from
the main program to cmdline.f.

CVS commit.

14 Jan 2012
Trying out second derivative style boundary condition at trailing edge.
Doesn't seem to converge during SOR, although does not blow up, when
lambda=1, vd=.2. but dxk2 is too big: 2.6

16 Jan 2012

Struggling with segfaults with the pathscale compiler on loki.
Using no optimization -ffortran-bounds-check -DEBUG

Found some bugs in getfield that cause array bounds overrun. 
Circumvented.

Found an error in obj_geom zeroing. 
However, there's a big problem with pathscale bounds checking. 
It gives false positives for implied array subscripts during read
or write. Without the -DEBUG switch those mostly don't happen. 

Now we have an apparent error in the unit number for writing. 
That implies either a compiler bug or some serious overwriting. 

Actually I have a prior comment that pathscale segfaults from 
flush. Commenting that out stops it. Should have noticed.
Oh well, found some bugs as a result.

20 Jan 2012

Fixed bug in collisional code arising when a particle slot is set to 
empty. I had never used the collisional operation.

Realize that I have not got compensating Eneutral in code at present.
So program to put vneutral equal to vd by default. Can override with 
the -vn switch.

23 Jan 2012

For collisional cases I need another force variable colnforce(i,j,k).
This would be the rate of change of momentum in each dimension (i) for
each object (j), and step (k). This is equal to \nu.Sum_iparticles
(vd-v(i_p)). This is a bit tricky, since it is not evaluated by object
intersections or surface integrals like the other forces. It has to 
be evaluated by examining each particle and finding if it is in the 
object under consideration. It could perhaps be incorporated into the 
particle advance, to minimize the computational burden. The place to 
put it is right at the end of each particle advance, using iregion.

Updated the fluxdata to read and write the extra colnforce, and account
for the version number to accomodate old versions.

Implemented in the relevant places.

I realize now that there's a booby trap for these diagnostics. It is
that if I have millions of particles, and I add each one's
contribution separately, then I end up adding roughly speaking 1. to a
total of over a million. Therefore there's potentially lost accuracy,
using reals. Eventually the increment will be lost. Do I have to think
about using doubles for these? Wow, that's an issue for some other
things too! But all the other diagnostics apply only to particles
crossing boundaries, so they are presumably far fewer than the total
particles. And the bulk force will also apply to far fewer particles
provided the region of measurement is much smaller than the whole.
But it might not be (especially in sceptic). This problem is partly
alleviated by keeping the number of particles per node smaller than
about a million. Have to think about this in the sceptic context as
well. Sceptic seems to bundle up the particles into the cells first, I
think. If so, that's a big help, provided the number of cells to be
added together is not too large. OR provided one does preliminary sums
in (e.g.) dimensional directions, followed by summing the sub-sums.
The problem can in general be alleviated by making sub-sums. 

Working on makefile I seem to blow away coptic.f. I've lost the additions 
to it. I think that was mostly the addition of call to bulknorm. 
Put it back in. I think that's all needed.

How to fix the rounding problem. Probably implement an intermediate 
summation stage around 3000 long.

24 Jan 2012
The sceptic code uses partsum and vzsum diagnostics to calculate the 
collisional force. Therefore the most particles they sum is one cell's 
worth. Then cells are added. Hence there's no problem.

8 Feb 2012
Working on fluxexamine and related plotting and printing of fluxes and
forces.

Changed object plotting routines to have a leading argument iq which
refers to the quantity to be plotted. By default it has been taken 
to be one (flux) before now, but perhaps one might want to surface-plot
the particle momentum or energy.

11 Feb 2012
Change nameconstruct to use 100*vd unless vd is very large. 
Do the same for P the probe potential.
Fix makefile to refer to the new code.

22 Feb 2012
Various upgrades to philineread to improve plotting and analysis capabilities. 

Checking on the functioning of the collisional bulk force with 
./coptic -v1 -ct1. -ofgeomforce.dat -s200 -ri400
gives
    Field,       part,       press,     collision,    total,  steps ave 100 200
===== Object 1 ->  2 radius=  1.000 zcenter=  0.000 Charge=   -8.5655 =====
     0.01101    -0.01232    -0.00235    -0.09590    -0.09957
     0.00600     0.32662    -0.00069     0.00334     0.33526
     0.22893     1.79832    -0.15775     1.22717     3.09667
===== Object 2 ->  3 radius=  0.500 zcenter=  0.000 Charge=   -2.7139 =====
    -0.00124     0.19713    -0.00001    -0.03246     0.16343
    -0.00070     0.39909     0.00046    -0.00257     0.39628
     0.20808     1.76245    -0.02563     0.57719     2.52208
===== Object 3 ->  4 radius=  1.500 zcenter=  0.000 Charge=   -6.0945 =====
     0.00624    -0.07546    -0.01251    -0.13619    -0.21792
     0.00681     0.23792    -0.00955     0.02475     0.25994
     0.05635     1.62745    -0.33953     1.73184     3.07611
===== Object 4 ->  5 radius=  2.000 zcenter=  0.000 Charge=   -4.0442 =====
     0.00080     0.05888    -0.02615    -0.24646    -0.21293
     0.00534     0.42357    -0.03260    -0.00527     0.39104
     0.01015     1.56578    -0.47526     1.94861     3.04928

Showing good z-force agreement for radii bigger than 1, the point charge
radius. That seems to indicate that we can in fact measure the force
for collisional cases.

27 Feb 2012
If colntime .ne. 0 then append an extra section to nameconstruct to tell
the colntime. 

7 Mar 2012

In force calculations, especially with collisions we seem to be
getting single steps with enormously large force contribution. So
large that it biasses the average over 100s of steps. How can this be?

My idea is that it arises from the point charge treatment. If a step
takes a particle to a position extremely close to a point charge, then
it will experience a massive acceleration equivalent to a hit (in a
practically random direction) out of the park. It will leave the
entire region (and domain) in the next step with a monstrous
contribution to momentum in some quasi random direction.

If this is in fact the cause, then it arises because the leap-frog
finite steps are too big near the point charge to correctly conserve
momentum and energy. The point charge really ought not to be allowed
to do these big hits.  Close collisions, well inside the shielding
radius and b_{90}, ought basically to be 180 degree scatterings.
Those give rise to the maximum possible momentum transfer. Twice the 
prior momentum. 

These problems were observed with time steps -dt.025. and point charges
-1. at .2 in a domain -4to8x-4to4, quite severe at low drift velocities
0.1, 0.2. At velocity of 1, a step of .025 is a length of .025, which is
1/10th of the point charge extent. Therefore, if out of the park hits is
the problem, they are occuring deep inside the point charge extent. Thus,
the ion will have stepped inside the extent, and acquired a velocity 
mostly from the potential drop: 

1/2 v^2 = r_p\phi_p/r

at that point it steps to the charge:  dr = dt v = r so that

1/2 r^2/dt^2 = r_p\phi_p/r

r^3 =2 dt^2 r_p\phi_p = 2 dt^2 P   where P is the particle charge strength.

This defines the volume for which hitting occurs. Solution is

r = (2 0.025^2 .2)^{1/3} = 0.06.

rhoinfty is 128k, so the number of particles in this region is. 
4\pi r^3  /3 rhoinfty= 134 There are about 100 particles in the hitting 
region (of a total of about 100M) one particle in a million. 
Their velocity is approximately r/dt = 2.5.

An integral of the momentum discrepancy will be of the form

\int dv n 4\pi r^2 dr = \int  dt (P/r^2) 4\pi r^2 dr

which converges fine at the origin. So it's not obvious that there's 
anything other than a noise problem associated with hitting. 

b_{90} = r_p\phi_p/(v^2/2) is kind of self-defining when the major
part of the energy comes from potential. 

What will constitute a hit out of the potential well? A step where the 
impulse is such as to produce escape velocity in one step. I.e.

	dv = dt P/r^2;   (1/2)dv^2 = P/r. 
So
 dt^2 P^2/r^4 = 2 P/r   =>   r^3 = dt^2 P/2.

This is essentially the same criterion as above for stepping to the charge
in one step, except with a different (smaller) coefficient.

Solution of this is r = (.025^2 x .2 /2)^{1/3} = 0.040
There are about 25 particles in this region.

What constitutes a ridiculous velocity? Perhaps one that leaves the
region in one step. I.e. roughly v = 1/dt = 40. To produce that velocity
requires impulse dt P/r^2 = 1/dt. I.e.

	 r^2 = P dt^2    =>    r= P^{1/2} dt  = 0.01

We certainly ought not to use accelerations as great as arise from this 
distance. 

Alternatively, one might limit the impulse to of order unity: r^2 = Pdt.
For which r = 0.07

Options to prevent. 
1. Limit the maximum acceleration somehow.
2. Subcycle if we are close to a point charge.
3. Remove a particle if it comes too close to point. 

Actually I have some code for 2 in padvnc but commented out at
present.  Activate this code to subcycle for particles with very large
accelerations.  Subcycle is the maximum permitted ion impulse per
step. If it is exceeded, then the subcycle's time step is set short
enough to limit the impulse accordingly. A reasonable value for
subcycle is 1, which would force subcycling within about r=0.07. One
probably should not go less than 1.  Bigger might be sensible. One
needs to worry about the possibility of self-force arising.

The runs don't look like completing on loki. I suspect there needs to be
a limit to the degree of subcycling that is done. Did some trials on
tp400 with subcycling and printing the numbers nsubc. Limiting to 10
the degree of subcycling reduces the nsubc. 

The loki runs give zillions of ptch field overflow corrected errors when
subcycling was turned on. This indicates that an ion is within about
1.e-12 of the point charge. Therefore it looks as if the subcycling is
causing some particle(s) to spiral in to the point charge. No other runs
show these errors. This would presumably be less likely if the degree
of subcycling were limited.

The one run that got far enough to print it says Subcycled: 16721.
So there are obviously a lot of subcycling steps. 

In any case, if advancing steps are taken that are too large for
energy conservation or momentum conservation, then an error will occur
in the force calculation if that ion subsequently crosses the
momentum-measuring surface.

Probably I need at least to remove ions that come too close to the
point particle. This could be done in response to a return from 
getadfield, and bypass the flux collection stage. That would be arguably
better than an arbitrary ptch overflow correction, and might be worth
doing even at smaller accelerations. 

9 Mar 2012

Implemented particle dropping if impulse exceeds 5. [Probably ought to make
that parameter adjustable, but not yet.] Also report out the number of 
drops by master node periodically. 

11 Mar 2012

Modify fluxexamine to allow printing out of vd and a component of the force 
from a masked set of objects. This is to ease developing plots versus vd.
Committed.

5 Apr 2012

Modify diagexamine to do an overplot of velocity arrows on a final plot
of the density. Seems to work. Discovered one needs the right number of
arguments if there's a character array to be passed, else the character
array is not handled correctly, and the called routine looks for its
length in an incorrect storage location.

13 Apr 2012

Revisiting subcycling because there seems evidence that with collisions
the effects of proximity to point particles are in fact impacting the
results with slow flow. It might be possible to alleviate these by getting
subcycling to work better. 

The basic problem with current subcycling is that it does not
substantially reduce the acceleration duration at the first reduction
of movement duration.  That's because the kick has duration
(dtprec+dtc)/2. And if we've been moving fast so that dtprec is big,
then reducing dtc doesn't reduce the acceleration duration (kick) by
more than a factor of 2. What that says is that if we obtain an
acceleration that is too large, it is already too late to reduce much
the kick-duration because the that duration has been set by the prior
dtprec. We should not have allowed the ion to step into so high an
acceleration region with the previous timestep. 
However, we have the information to back-up the move. It is in the form
of dtprec and the current position and velocity. 

What we are really saying is that for subcycling we want to interrupt 
and reduce the standard KICK-timestep. This is different from collisions
where we want to interrupt the MOVE-timestep. So simply adopting the same
approach for subcycle and collisions is insufficient.

Now when backing-up, the prior kick has already assumed a step dtprec
and made its kick-duration equal to half of it plus half of the
previous step.  We need to change dtprec. But we can (I think) adopt
an alternate view of where the step starts and stops. Instead of
defining that through the move-step, we can define it through the
kick-step. So if we back up by half of dtprec, we are at the end of
the prior kick-step, and we can then change the current kick-step to
something new and smaller. That means the prior move step was half of the 
dtprec plus half of the new kick-step. This is illustrated by the following
diagram for a step-size-reduction occurring at *.

Kick Steps ----|-------|-------|---|---|--|
Move Steps |-------|-------|---*-|---|---|-|
                               a b c d e f g
Unchanged Moves ---|-------|-------|-------| 
                            dtprec

That can be contrasted with the step reduction occuring at a move-step
end (which is what is used for collisions) illustrated below:

Kick Steps ----|-------|-------|---*-|---|
Move Steps |-------|-------|-------|---|---|
                            dtprec c

Either one of these approaches has some error associated with the
transition in step-size. I don't think there's reason to suppose that
one error is worse than the other. We represent the values of position
at move-step ends (|). The velocities that are stored are those
corresponding to the middle of the prior move-step. I.e. at the prior
kick-step end. 

Backing up (the first of the above diagrams) corresponds to returning
to that prior kick-step end. The position is then half the dtprec
times the velocity back (*, a). We then want to take a smaller
half-move step forward to the adjusted move-step end (b), and make the
kick step based upon the acceleration at that position (b) and the new
duration.  We want to store as dtprec, the new kick-step duration,
half of which is the half-kick-step to the current position (b). Then
when we attempt to take the new step, the kick will be correctly
calculated using (dtprec+dtc)/2. 

If we then find the new kick is still too large and we want to reduce
the step-duration further, we can simply repeat the backing-up process.
We need to keep track of where we eventually have to get to,
using dtremain.

In neither case do we adjust the potential. So the field calculation
is done for a charge density that corresponds to the end of the prior
move step. In the prior approach to subcycling, that meant we use
advanced particle positions to obtain the acceleration, leading to
accelerating self-force. In the case of backing up, the field is
instead evaluated at a position (b) behind that (c) at which potential was
solved.  It should then give decelerating self-force at kick b, but
accelerating at d and f. The time g is where the subcycling catches up
with the rest of the simulation, and we move to the next time-step.

To implement backing up we need to package up particle moving into
a subroutine and then just do it backwards for specified negative dt.
Did the encapsulation.

14 Apr 2012

Seem to have subcycling working as planned. When a sub-step has been taken
the length tries to double or go the full length. That way if we substep
out of a high acceleration region, then we'll lengthen the step again.

22 May 2012

Trying to run some moon-relevant cases. Can do 3-d but then one gets a
(physical) peak on axis with no B-field.

2-d equivalent case is the "thin" 4-mesh in the x-direction. But this is
found to have boundary points that have stupid potential values. This is 
probably not affecting the result much, since we have taken the length
to be very large in the x-direction. But it reminds me that we really need
to develop better boundary condition control. In fact the old stuff is 
so ad-hoc that it probably can't be properly cleaned up. May be better to
develop a whole new system in parallel. 

There are 3 main categories: Spherical symmetry, Cylindrical Symmetry,
and Rectangular symmetry. The ad-hocs are mostly applying spherical and
cylindrical BCs to the rectangle. What we now need is rectangular control.
This means we can apply a different BC to each of the 6 box faces.

Proposed 1st-order scheme is that we allow ourselves to apply a Robin type
condition in the form:
	  A phi + B dphi/dn + C = 0
at each face: A(6), B(6). Probably the normal derivative ought to be 
OUTWARD, so dn is opposite in sign for opposite faces.

But we would also like to enable C to vary with space. In first order it 
might be 
      c = c_0 + c_x x + c_y y + c_z z.
However, we wish to be able to bypass expensive x,y,z calculations 
if their respective coefficients are zero. That might be done by making
c a function, but in any case, we need to save the c_coefficients, and
possibly a few other logicals for bypassing. If the boundary potential
is going to be continuous, then we require the c's to match along edges.
One way to ensure this is for all 6 faces to have the same c's. But that's
not the only way.

The degree of evaluation of the Robin condition would call for a third
phi value if we made phi and dphi/dn centered on the boundary, so one
might make it centered on the half-lattice position: 

phi = (phi_i+phi_b)/2              dphi/dn = (phi_b-phi_i)/|dn| 

But if B=0, it makes more sense to simply set phi_b. 

Then c ought perhaps to be evaluated at the same place (if it varies).
It's better to do the adjustment of c externally for spatial position
internally. Not in routine.

So:
	 if B=0 phi_b=-C(b)[/A]
	 else phi_b = - [C+(A/2-B/|dn|)phi_i]/[A/2+B/|dn|]

If A/2+B/dn=0 there's a singularity. One could verify initially that 
that's not the case. One ought also for speed to save the values of
A/2+B/dn A/2-B/dn for each face. 

	 logical LF,LCF(6)
	 real AF(6),BF(6),C0F(6),CxF(6),CyF(6),CzF(6)
	 real ApBF(6),AmBF(6)
	 common /FaceBC/LF,AF,BF,LCF,C0F,CxF,CyF,CzF,ApBF,AmBF

Programmed the mditerate routine. bdyface
Programmed the initialize routine. bdyfaceinit.
Added CFpack to the cmdline argument list. 
But in all honesty its going to be verbose to do this through
cmdline flags like this 
-BFidn,Ain,Bin,C0in,Cx,Cy,Cz

Implemented anyway. One can omit the Cx... if unused. Seems to be setting
the arrays correctly, but the BCs are not tested under solution conditions.
The way to do that is with -gs -gd -l1000 and the small cube.

As soon as one of these switches are set, we are using the new BC 
setup. But not yet implemented as actually calling the bdy routine.

23 May 2012

Now got a version of the bdyset routine working that uses the new 
conditions. After debugging a bit they seem to work.

Implemented setting in geometry file. Currently lines in copticgeom.dat
will override cmdline switches. It's a judgement call which prioity is
the most useful. This is the easiest to program. 

Periodic Potential Boundary conditions would be interesting. 

Within the present scheme, they are equivalent to setting the boundary
value equal to the first non-boundary value at the opposite face. 
I.e. phi(n) = phi(2), and phi(1) = phi(n-1). 

However, it would be much more elegant, and possibly faster, to implement
a truly periodic difference scheme in mpibbdy. At present the face
information is passed from adjacent mpi blocks. If the end block information
was passed in a similar way, one then would not have to call a separate
bdyset routine. This actually looks pretty easy, since all the infrastructure
for this is built into the mpibbdy. All one needs is to specify the 
cartesian comm world as having periodicity in the interesting direction(s)

As far as I can see, all we need to do is change the value(s) of lperiod(nd)
(that are set in sormpi) to true rather than false. I could pass information
in ictl to control that. Tried to get sortest going. It gives nans in u.
Drat! However, coptic appears to be working with the geomsphere and the
test routines. So ignore sortest which has not been used for at least a year 
and just work with coptic.

Implemented ictl switches to make periodic. Does not work. Just seems to 
hang. Seems to be a problem with the separate mpi_send and mpi_recv. 
How can one send to oneself like that? But actually, I'm running a 
serial process here. So maybe that's the problem. Under mpiexec it
still hangs. So that's not really it.

Replacing the separate sends and receives gets one through to the second
step. And replacing the second with a sendrecv fixes the hang.
Result looks plausible, but I've still go the separate bbdy setting the 
final result to something different, which is not what I want, of course.
Still I now have a working periodic BC version of coptic working with
only the need to turn off the separate bdyset. Done that. 

Now we can use periodic BCs on potential. (Don't yet have particle
reinjection periodicity). There seems to be a problem with multiple
processes and periodic BCs. They don't seem to be working correctly,
at least on the thin domain. I don't see the problem with the test
cases. 

24 May 2012

The symptom is that in the x-y plane, with z in the lower half, only
the bottommost y-block has anything other than zero on the boundary.
While with z in the upper half, none do anything except zeros.
This can be seen in dimension 1 plane 1 very clearly. Actually its the
lowest block of y and z that has non-zero. 

Changing the default face bc to a different value, one finds that the 
erroneous faces stay equal to zero. So presumably they are never being
set. Perhaps this makes sense. I only pass the values from the other
blocks that correspond to non-ghost cells. Therefore the ghost-cells 
that belong to the other blocks, around the edge, never get set in the 
master node's version of the potential. That's the problem. 

Each process in sormpi is looping on [bdyset, bbdy, sorrelaxgen.
Each has an array for the whole potential, but updates only part of it.
The information for the boundaries of each block is passed, so the 
inner solution is correct. The only problem is that the boundaries of
the master node's arrays belonging to other blocks never get set. 

For some reason, bdyset is called twice at the end of the iteration.
Why did I do that? But anyway that is when the boundary is supposed
to be set to be correct for the final result. With periodicity that
does not work for blocks not already set. Probably the correct way to
do this is to do reverse communications after the mpialltoall, to 
send the information back to the ghost (edge) cells. But this needs
some careful thought. I don't see how to do it.

Instead I just fixed bdyset to explicitly set the boundary values in
accordance with periodicity with a periodic direction. This is
unnecessary for iterations, and so is a possibly important
inefficiency.  Profiled to check. bdyface is using about 12% of time
in a solution-dominated case with a thin 4-element
x-direction. Compared with sorrelaxgen 35%.  So it is an inefficiency
but only roughly a 20% maximum hit, in a bad situation like the thin
geometry. However, this inefficiency will become very bad once we
have lots of processors and the sorrelaxgen costs are shared. It 
does not scale at all. In the end, for a periodic system the boundary
points are irrelevant: they correspond to the adjacent point of the
opposite face. So it isn't really necessary to set them for anything
other than final aesthetics.

This is pretty much a universal problem with the way that sormpi is 
written. The bdysetting costs don't scale because every processor sets
all the bdy points, regardless of their relevance. If all boundaries
were periodic, though, it would be unnecessary to set any of them. 
One approach to solving this issue (which would not solve the prior 
problem of sharing the boundary with the master node) would be for 
each process to detect when it is not required to receive information
about one of its faces. When this happens, it is because the face 
has been set by the bdyset routine. So the _process_ ought to set that
face according to the boundary condition, and do so for any face that
it does not receive from neighbour. In that case the bdset routine becomes
redundant. 

The challenge for this is that the bdysetting routine is currently global
and defined in terms of a mditerate process. The bbdy routine setting 
would be different and more local. It would be a subface-filling
procedure. But it would be local to the process's block. The hard part
is probably how one tells how to do the setting without entanglement
into the sormpi mpibbdy code. The information would have to be gained
other than through sormpi. We'd need an api.

It looks pretty tricky given the density of the mpibbdy code to figure
out how to do this. I think it's time to commit what we have. 

I think that in sormpi the information returned from bbdy is sufficient
to figure out the location of the block and which boundary points to
be responsible for. So it would be possible to parallelize the boundary
setting. That would overcome the non-scalable bdy effects.

25 May 2012

Trying to parallelize the bc setting. 
First try to get sortest working.
Actually, I find that sorserial works, while sortest doesn't. 
Update sortest.f to use the new cmdline setting code.
Found that the sortest error arises because plascom is not in 
sortest. Put a trap in bdyset to prevent divide by zero debyelen.
Why do I not put plascom.f into sortest? Just for independence, I suppose.
If it is included then sortest works like coptic. If not then 
there's more independence, I suppose.

sorserial replaces the bdyroutine with a null so it gives a different
solution.

OK sorserial and sortest are working.

Implemented an mditerate over the boundary of a block of thickness 1. 
bdyshare.

Got it to work with explicit setting of u to a hard coded value. 
With mpi processes, the boundary value is not actually set in the
master frame, as expected. If needed, this can probably be fixed 
afterwards by one call to the bdyshare routine which tells it that
there's only one process. [No that does not work because bdyshare 
never actually sets boundary values for periodic dimensions, it just
relies on the mpi communication. That means that sorserial won't work
properly with periodic BCs. 

Now working with -bf face boundary condition setting and -bp periodic.
For the old -bc settings the global code is still used. This is also 
called last thing in sormpi so that the boundary is set for the whole
of the master node's mesh. This is purely for aesthetics with periodicity.
It might be worth considering implementing explicit boundary setting
in periodic directions so that it would work with sorserial. Then the 
final call could be to bdyshare routine telling it one block. 

26 May 12

Fiddled with partaccum and related codes to allow one to choose a 
block configuration in the postprocessing of particle data. 
That way we can specify e.g. 1 in the x-direction and more in the 
other directions.

Found a bug in the binfinity ninjcalc and fixed. 
But there are other bugs. One seems to return particles which have
purely z-velocity at least when reading back from the partfile.

B-field is not working properly yet.

28 May 12

Fixed partaccum to do sensible things if all the particles have the same
velocity. They do in infinite-B cases in a coordinate direction perp to
the field.

Fixed the particle moving to make the perp velocity just the drift. 
(For some reason I'd had it set to zero.) Now infinite B seems to be
working.

Succeeded in getting a magnetized moon case to run. Looks sensible.

1 June 12

Cleaned up the makefile and that of accis so that it is substantially 
more logical and robust in making decisions about drivers and compilers.

2 June 12

Discovered that the serial version of coptic does not communicate the
periodic boundary conditions properly. I had thought about this but
came to the conclusion that it did. That was wrong. But I don't know why.
What bdyshare currently is supposed to do is to set the guard boundary
values when the block boundary is actually a domain boundary.
However, it only does that when the boundary is NOT periodic. 
When it is periodic the bdyshareroutine does nothing, because it is
assuming that the boundary has been set by the bbdy communication. 
Thus, when there is no bbdy communication, the boundary is not set 
properly. That explains my observations. 

Ideally, we would like a call to bdyshare that changes its behavior and
instead does periodic setting rather than nothing. 
Currently we pass offset into the bdyshareroutine and this points to the
adjacent position. For periodic setting, we could perhaps pass a different
offset that pointed to the corresponding point at the opposite face, and
have A=0,B=1,C=0 (zero slope) which sets the boundary point equal to the
adjacent point, and temporarily set LPF off for this id.

Instead simply increased the information passing capability by using 
more of idone. If idone(2)=1 then the periodic conditions are set 
explicitly, using the ABC coefficients.

Also fixed the object file setting of periodicity so it works.
It's possible to get an inconsistent result by setting periodicity 
then setting _just one_ of those faces back to face conditions. The
other face then uses zero derivative condition left over from the
periodicity settings. But it then seems to get the boundary set wrong
at the end as if it were periodic. Hmm. There's an error in logic.
Fixed that.

Implemented an option in partaccum. If ivproj=1 then we project the
accumulated velocities: id=1 parallel, id=2 perp, id=3 z-component.
Added -p switch to activate in partexamine. 
Added default Bfield setting in partexamine if not already set. 
But that needs some better switch to prescribe override.
Implemented ability to prescribe Bdirs after -p.
It overrides if it was set with the full three components. Otherwise
it is used only if Bfield was not read from the partfile.
Seems as if this option is working: finding projected particle distributions.

Needs some work on the annotation of the plots.

Thinking about rationalization of the ndims argument and parameter mess.
What are the causes of problems.
1. Local Scratch arrays can't be created legally unless ndims is a parameter.
2. Consequently, if ndims is _passed_ which means we are prevented from
   using it as a parameter, then we do duplication with modified name. 

How do we avoid the crazy multiplication of ndims parameters?
Perhaps have an include file that does nothing except define the
ndimsparam which we use for all such dimension definitions.
Unfortunately this mess is very bad. Plus, in quite a number of places
the coding is implicitly assuming 3d. For example in some of the surface
and flux tracking code. There are literally hundreds of places that ndims
appears in the code (with various prefixes and postfixes.)

Converted all the objcom.f _sor labels to _cij. 
Found that there's a call in sorrelaxgen that references ddn_cij.
So that there's some cross-linking even here. That's a bit of a problem.
Actually it is unavoidable. It is the place where the adjustment of the
cij stencil takes place. It uses extra information stored elsewhere.
Only the cij code knows the structure of that extra information.

But anyway. I seem now to have rationalized one naming convention so
that _sor really does refer to the sor code. While cij refers to the
objcom object boundary code.

Made bdyshare an argument passed to sormpi.

7 June 2012

Constructed github repository COPTIC and successfully exported from cvs
and imported to github. Clone it via

git clone https://github.com/ihutch/COPTIC

Seems to be ok.

14 June 2012

Getting back to this after week of CAP and physop duties. Next steps to 
incorporate some instability calculation into the particle analysis.
For this purpose, we need the projection along (and potentially across)
the field. That's implemented above.

Then we need to be able to do the stability analysis of the resulting 
distributions. This presumably involves the chicomplex code that has
been developed. Probably this can use the pex file?

First trying git out on loki, sceptic get a certificate error problem.
It is fixed by doing:

export GIT_SSL_NO_VERIFY=true

Apparently 
git config --global http.sslVerify false
also does the same thing.

This is a problem with the older operating systems on loki and sceptic.
Their certificate bundles are out of date. 

15 June

Found a bug preventing convergence in the benchmark runs. If only x
has multiple processes then iterations don't converge. E.g.

mpiexec -n 3 /home/hutch/src/coptic/coptic -bf7,1,0,0 -ri800 -v.5 -t.01 -dt.025 -rx.0 -l1 -s3 -w-99999 
# Point charge at origin. Spanning a radius .2. At which its potential is...
513 , 0.,0.,0., 0.,0.,0.,   .2,.2,.2,   0,-1.,0
# Measure forces at .3,.5,1.
1, 0.,0.,0.,   0.,0.,0.,  .3,.3,.3,     4,5,5
1, 0.,0.,0.,   0.,0.,0.,  .5,.5,.5,     4,5,5
1, 0.,0.,0.,   0.,0.,0.,  1.,1.,1.,     4,5,5
#
91,1,33,68,100,0,-4.,-1.,1.,4.
92,1,33,68,100,0,-4.,-1.,1.,4.
93,1,15,82,100,0,-4.,-1.,4.,8.

This is purely a point charge. Also there's a bug with the test solution
not converging because it is zero everywhere. That's easily fixed.

The other bug is present for pure coptic make runs with mpi, and with -bf
boundaries, but not without.

bf BCs don't actually seem to be working.

Tracked down this major bug. It seems that if you pass a logical array
(lperiod) whose dimension is also passed as an argument (ndims) then
within this routine, the logical array is incorrectly aligned, and also
other arguments after it in the argument list are misaligned or otherwise
misinterpreted. If however, the dimension of the logical array is a
parameter within the routine, then it works correctly (actually not). 

This problem exists with gfortran as well as g77.

But here's a major worry. I used this approach with bbdy too. So how 
come that is not broken?

No the thing is still broken even with the declaration. I don't think I've
found the real problem yet. Tried a bunch of things without success.

Moved the lperiod to the end of the argument list of bdyshare. Seems to work.
But there's still a worry about bbdy. 
Also works reverting to passing the ndimsdecl argument.

Because I am really worried about this problem. I move lperiod to the 
end of the argument list in bbdy() too. This may have affected some
routines in testing/

bbddecltest.f tries to reproduce this error in microcosm. but fails to do
so. Therefore I am still worried, because perhaps there is another 
problem in bdyshare that was the real cause and I've just bypassed it 
for now.

23 Jun 2012

Changed the partaccum to a septic profile to get rather more points
further out on the velocity range. The results are slightly better for
the moon case. But surprisingly still don't seem to put enough points
far out. It may be we need a different algorithm for such high speed
cases with big dynamics.

24 July 2012

I need periodic boundaries for particles as well as for potential. 
For example, in trying to set up a plasma with 1-D gradients to mock
up (e.g.) a presheath. This ought not to be too difficult. But it 
needs changes to padvnc and to the reinjection routine.

I think the natural place to put the periodic particle relocation is 
in partlocate. There are in that routine, tests for each dimension to
determine whether the particle is in the mesh (linmesh). Those can be
readily extended to relocating the particle periodically. 

Create a new set of flags ipartperiod(ndims) in partcom.f that tells
if particles are to be periodic in a dimensions (!=0 yes). Initialize
and set in copticcmdline.

Implement in partlocate the relocation in periodic directions. 
partlocate returns the region of the relocated particle. It seems
this is sufficient to do everything that padvnc needs. Notably, it seems
that this will satisfy all the region tallying etc. at least for objects
in an unbounded mesh region.

However, we now need to fix the reinjection statistics etc. 
This only really applies to cartreinject. But ought not to be as
difficult as the strong magnetic field correction.

In the initialization of gintrein, it is constructed by multiplying 
grein by area(id). It looks as if one can eliminate injections on
faces in one dimension by simply putting that area to zero, or putting
grein to zero. Area is also used in ninjcalc. That's another place to 
correct. It does not use grein. rhoinfcalc too.

Renamed area in cartreinject to fcarea to disambiguate.
Added fcarea to partcom.
Removed fcarea local declarations and now rely on partcom declarations.
Now they should be calculated in common. So once this is initialized
it ought not to be necessary to recalculate in rhoinfcalc, nijcalc.
Actually ninjcalc it is necessary because that is called before we do
any reinjections. That's the place where it is first initialized.
But that's not called unless ripernode is set. So we still need to do
initialization in cinjinit. Unfortunately rhoinfcalc is also called
before the first padvnc. So initializing in cijinit is irrelevant.
The fact that making this common has not changed the result seems to
show that the recalculation of it in rhoinfcalc does not change the 
value. No that's irrelevant. fcarea is only used once in cijinit, 
and it's calculated there. Bottom line. Attempts to rationalize fcarea
calculation are not worth it. The uses are effectively disjoint.

Put in 3 places fcarea(i)=1.e-6 (factor) for periodic directions.
What do we do if all three directions are periodic? In ninjcalc and
for reinject this ought to be ok without an absorbing particle because
we ought never to reinject.  For rhoinfcalc, we ought to get the
fallback for nrein lt 10 to just divide the number of particles by
volume. 

But what happens when we have an absorbing object present? Then we'll
get some reinjections, and these might well be more than 10. Comes
down to the fact that calculating rhoinfinity on the basis of
reinjections makes no sense with fully periodic particles. Perhaps one
ought to track the number of relocations as well. That's the thing 
that includes the same quantity. A relocation is practically equivalent
to a reinjection. If we treated a relocation exactly as a reinjection,
then we would not have to do different calculations for ninjcomp or
rhoinfcalc. The area modification ought to be removed.

Add an extra argument nreloc to partlocate. Increment it when relocated.
nrein is passed when it is called in padvnc. Remove the adjustments from
ninjcalc, rhoinfcalc, because now nrein includes all relocations.



25 July 2012

Ok, I think we should be done. How to test this? One way might be with a 
uniform plasma and no objects. Set up a geometry file uniform.dat for this.
But actually on the way implemented extra interpretation of the object
file that allows to put partperiod information into it. Also did some
improvement of the -h documentation of values, and adjusted the order
of things and the use of copticmdline to enable commandline arguments
to override the values set in the geometry file. 

Running a no-objects geometry with periodic particles and potential in
the x and y directions and fixed potentials on the z-faces.  If
potentials are equal to 0. (-l.01) the plasma is simply uniform for
all (subsonic) velocities. If they are different, then the plasma
potential floats up a bit and there are sheaths adjacent to each end
of the box.  This is with nominally fixed injection count. The actual
nrein exceeds that count giving e.g.  2651 2572 2546 vs 2457. This
might explain why the potential rises above zero. I'm not crystal clear
why this excess arises. I guess that it must be made up of periodic particles
after the reinjections of end particles have stopped. It's dropping only
very slowly towards the right value (after 300 steps). It can be made closer
by using acceleration up to -da20. Then the systematically high average 
goes away. 

Perhaps one ought to discard relocated particles if nrein is already 
exceeded. That way presumably one will approach the right density 
more rapidly. May not be necessary with acceleration.

Now, can we implement collisions in such a way as to give a systematic
gradient? Initially collisions don't do much. This presumably because 
they are implemented at exactly the drift speed, so they don't actually
drain any momentum from the ions. This can be fixed by doing -v.5 -vn0.

Find that yes rather consistent results are obtained with velocity profile
depending on collisionality.

26 July 2012

Implemented periodicity ninjcalc and rhoinfcalc that ignore periodic 
boundaries unless all are periodic. Also partlocate increments nreloc
only if all ipartperiod are non-zero. This causes the potential rise
to be less than with the earlier approach which always increments nreloc.
However, it also shows instability which drives the potential to large
negative values under some conditions. So it's not so obvious this is a
good plan. The velocity profile obtained looks similar to the other
count case.

27 July 2012

I realise there is an inconsistency in the implementation of particle
periodicity. The potential is periodic by virtue of the fact that the
edge potential value corresponds to the nodes _adjacent_ to the edge
on the opposite face. This is part of the guard-cell approach to block
decomposition. Consequently, the join between the opposite faces
really takes place half-way between the adjacent nodes and the edge
nodes (on both sides). However, particles are currently allowed to
travel all the way to the edge node location before being considered
lost. Then in the periodic particle situation, they are moved to a
periodic position on the opposite face, using the full domain. In
short, the periodicity length of the potential and the particles are
different the way I have implemented it so far. This is definitely
unsatisfactory. The charge on the edge nodes is never used in the
potential solution (I think). The edge potential nodes are set purely
by the boundary condition. However, the charge on adjacent nodes IS
used and in non-periodic cases requires charge to be deposited from
the full cell out to the edge nodes, so that they have full contributions.

If, therefore, periodic particles were implemented consistently with
the potential domain, i.e. out to the half-boundary (half way between
the last two nodes), then one needs additional contributions to the
adjacent node charge. Presumably this contribution is from the
opposite edge nodes. So one way to implement this consistently would be

1. Particles occupy only out to the half-boundary. Then they are relocated 
to the opposite half-boundary. 

2. Charge is deposited between boundary and adjacent nodes in the usual way.

3. The total charge on the adjacent node is the sum of its directly deposited 
charge PLUS the charge deposited on the opposite edge node. 

[4. This can also be considered the charge on the opposite edge node.]

This requires changes to the charge deposition for periodic faces compared
with non-periodic (as well as relocation etc). For consistency it also
requires the last cell to have the same size on both (opposite) faces.

An alternative would be to retain the full particle and potential
domain but to implement the potential periodicity differently. Namely
that the periodic boundary is exactly at the edge nodes (on opposite
faces). AND the potential on those nodes must be determined by a
difference stencil that includes the adjacent nodes from both faces.
This is a bit troublesome, but probably not infeasible. It would not
require changes to domain tracking or charge deposition, but would
require summing both edge charges. I don't like this option much.


28 Jul 2012

To implement the changes to chargetomesh, run up against the need for a
general iterator over indeterminate dimensions again. Could use the 
mditerate scheme, but that is really cumbersome. There's got to be 
a better way. It is much more transparent if the code one wants to 
iterate is able to be placed in line, surrounded by the equivalent of
nested do loops. It ought to be possible to create a more general 
iterator that works like the beginning and end of loops. This is appropriate
in chargetomesh, because there, the arrays are in abstract form.

I think I need to think harder about the minimal requirement of an iterator.
It seems that the fundamental requirement is to calculate an array index
that corresponds to the next position, and to determine when to stop.
How the next position is determined might be variable, but could perhaps
be specified in terms of a logical step size that is converted by the 
iterator into an actual step. 

If the iteration is specified in terms of a step in the lowest dimension,
things are easy. But if instead there is a fixed value of a higher dimension,
how do we do that? 

One abstraction that might help think about this is the idea of a "view"
of a data set. In NumPy, this can be in the form 
 A(start1:end1:stride1,start2:end2:stride2,...)
In other words, each dimension can be strided and truncated.
In a sense I already do truncation by having ifull and iused. iused
defines the end for each dimension. In a sense I can define the start
when passing an array like A(2,2,2,...) or the linear equivalent. 
I lack the striding ability, except by specific incrementation. 
The NumPy discussion conceives of a general iterator as:

	set up iterator
	  (including pointing the current value to the first value in the array)
	while iterator not done:process the current value
	point the current value to the next value

which is consistent with my conception. 

A general structure for specifying NumPy equivalent striding would be
  iview(3,ndims) where for each idim we store start:end:stride.

Illustrating
1. if each iview is 0:n_idim-1:1 we iterate the whole array.
2. if for dimension id iview is X:X:1, but the others are as in 1, we iterate
   a slice in dimension id.
The following is wrong. Using a stride of N-1 gives a diagonal slice.
3. if for dimension id iview is 1:N-1:N-1, we iterate only the id boundaries.
Boundaries would have to be done with two iterations 0:N-1:N and N-1:N-1:N.

In current implementation, I am generally passing end (iused) and full (ifull).B
ut not start or stride.

The general iterator has to move the pointer and decide if we are finished.
The conversion from multidimensional to linear pointers is 

    ipointer=indi(ndims)
    do id=ndims-1,1,-1
       ipointer=indi(id)+ipointer*ifull(id)
    enddo

which gives 
  ipointer=indi(1)+(indi(2)+(..indi(ndims)*ifull(ndims-1)...)*ifull(1))

Implemented as integer function indexcontract. Then the iterator does
not need to know about ifull, because only indi is used.
Created an integer function mditerator to be used as a generic 
iterator.

30 July 2012 

Implemented periodic partlocate that does relocation of particles that
are outside the outer half of the edge cell.

Implemented chargetomesh periodic summation of psum (not yet diagsum)
to compensate. 

Got to run, but realize that there are several other problems seemingly
all associated with xmeshstart/end. 

1. pinit: we don't want to initialize particles outside the periodic domain.
2. reinject: we don't want to reinject particles ditto.
3. reinject etc: we want the area of faces to be correct.

These all seem to be dependent on xmeshstart/end. So if we simply change
meshconstruct to define xmeshstart/end for periodic dimensions to be at
the half-cell position, then probably these are fixed. 

The only other place that xmeshstart/end are used is in partaccum for
particle diagnostics. Again that seems to be corrected by using the adjusted
values. 

Actually, the partlocate could have used this redefinition, except that
it operates in mesh-space rather than real space.

So modified meshconstruct to make xmeshstart/end different for periodic
directions. Code runs at least.

---------------------------

Hummm. MPI does not seem to work for multiple processors. Perhaps it does
not work to put this exchange in the chargetomesh. Each processor only 
has its own charges until after psumreduce and diagreduce. 

Separate out the psumperiod and diagsumperiod parts into their own 
routines that are called after the reduces. Still doesn't seem to fix
the mpiexec -n 2 result. Actually I don't think there's really a problem
with mpi and particles. Each processor has particles everywhere. So
the separation wasn't really necessary (but did not hurt). 

Running with high Debye length shows this bug to good effect. The 
density on the positive z-edge for x<0 progressively increases. But 
only with two processors. It is the x-direction that is split.

It happens even with no periodic particles! Also without collisions, but
not so bad. DOESN't happen if -bf6,1,0,0 instead of -bf6,1,0,4 
(but that seems bizarre.) This is pretty puzzling.

Moving aside my cartreinject.f and doing git checkout -- cartreinject.f
brings back the old scheme. It shows the same error.
This seems to show this is not a new error in cartreinject.

The 14Jun version which is mostly from 6 Jun does not show this problem.
Cloned the current git repo from tp400 which is later than the github
version to copticcopy. It does not show the problem with
mpiexec -n 2 ./coptic unif5.dat -s300 -v1.6 -vn0. -t.1 -da5 -l100. -ct20. -gs -gp -bf6,1,0,.5

There are many files that differ. I've shown it isn't cartreinject. 
Files 3dobjects.f and ../coptic/3dobjects.f differ
Files cartreinject.f and ../coptic/cartreinject.f differ
Files chargetomesh.f and ../coptic/chargetomesh.f differ
Files cmdline.f and ../coptic/cmdline.f differ
Files coptic.f and ../coptic/coptic.f differ
Files facebcom.f and ../coptic/facebcom.f differ
Files interpolations.f and ../coptic/interpolations.f differ
Files mditerate.f and ../coptic/mditerate.f differ
Files meshconstruct.f and ../coptic/meshconstruct.f differ
Files padvnc.f and ../coptic/padvnc.f differ
Files partaccum.f and ../coptic/partaccum.f differ
Files partcom.f and ../coptic/partcom.f differ
Files pinit.f and ../coptic/pinit.f differ

The issue did not arise from the bbdy fixes, therefore. But it is hard to
guess how to figure out where the problem arose. 

Copied 3dobject.f from ..coptic. No problem.
Copied cmdline.f segfault. Not tested. 
Copied partcom.f. No problem
Copied facebcom.f No problem
Copied cartreinject.f No problem
Copied interpolations.f No problem
Copied mditerate.f No problem
Copied partaccum.f No problem

So we've narrowed it down to 
Files chargetomesh.f and ../coptic/chargetomesh.f differ
Files cmdline.f and ../coptic/cmdline.f differ
Files coptic.f and ../coptic/coptic.f differ
Files meshconstruct.f and ../coptic/meshconstruct.f differ
Files padvnc.f and ../coptic/padvnc.f differ
Files pinit.f and ../coptic/pinit.f differ

Copy meshconstruct segfault. This is longer argument list. git
checkout meshconstruct.f

We need now to change to the new coptic.f calls. First commandline.
In copticforward directory.
$ for file in coptic.f ; do diff -u $file ../coptic/$file > patch; done
Then patch <patch makes both coptic.f the same, but git checkout coptic.f
does not restore the old one in copticforward. Thatmeans I must have
patched the ../coptic/ version. This is corrected by  patch -R <patch
which seems to reverse the patch.

$ diff -u ../coptic/coptic.f coptic.f >patch.forward
$ patch <patch.forward
patching file coptic.f
Reversed (or previously applied) patch detected!  Assume -R? [n] y
That patches the whole thing. But I just want to do the beginning. 

So just patched the first hunk which should be cmdline and meshconstruct.
Copied cmdline.f PROBLEM.
git checkout cmdline.f No problem. 

Thus the problem is definitely involved with the new cmdline.f
It has a more complicated structure to do with first and second calls.
Patched the leading part of the old cmdline.f to pass arguments,
ipartperiod. Then perhaps there is a problem but partly obscured by 
rounded values. But that problem is there with old cmdline too. 
Consequently, I am not sure I've really localised the problem. 

I need to quit for now. I am not really to the bottom of this. 
Save the partly edited coptic.f as coptic.stage1, and cmdline as cmdline.stage1
Just using cmdline.stage1 does not introduce the problem. It appears
that coptic.stage1 is where the problem lies. The double call, and
some repositioning of readgeom. 

1 Aug
I'm not sure that the conclusion I made is true. Certainly there's some
combination of coptic.stage1 and cmdline.state1 that seems to produce 
a problem. But I don't think I can isolate it yet. But now running
with the patched coptic.f, but old cmdline.f, AND commenting out the
second call to cmdline, I get ok results, while with the second call I don't.
Without second call the return from cmd gives:

Two cmdline calls with the fully updated cmdline.f gives:
0:  T F 0 1  0. T 1 T F F -1 0  0.100000001 1 100 0 1  100.  0.5 0  20.
0:   0.100000001  5.  0.  10.  1.  0.  0.  0.  0. 0 300 3000  0.  1.60000002 0 7
0:   100.  0.100000001 99999 0 F
0:                                                                                                      
0:   0.  0.  0.
0:  unif5.dat                                                                                           
0:  F  0.  0.  0.  0. 3 0  1.60000002  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.
0:   0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.
0:   1.  0.  0.5  0.  0.  0. 5 T T F 0 0 0
Gives asymmetry.

1 call with cmdline.stage1, (new coptic.f)
0:  T F 0 1  0. T 1 T F F -1 0  0.100000001 1 100 0 1  100.  0.5 0  20.
0:   0.100000001  5.  0.  10.  1.  0.  0.  0.  0. 0 300 3000  0.  1.60000002 0 7
0:   100.  0.100000001 99999 0 F
0:                                                                                                      
0:   0.  0.  0.
0:  unif5.dat                                                                                           
0:  F  0.  0.  0.  0. 3 0  1.60000002  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.
0:   0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.
0:   1.  0.  0.  0.  0.  0. 5 T T F 0 0 0
No asymmetry problem. 

There's a difference in the last line, item 3: 0.5 That seems to be the last
CFin. Without it there's no asymmetry problem. 

Is that being reset to zero by readgeom? Seems so. Yes. That's because
there are lines in unif5.dat that set it. Confusing.

Make all the arguments in the commandline to avoid confusion.
Then it seems that the original version of copticforward gives the
asymmetry.

mpiexec -l -n 2 ./coptic unif5.dat -s300 -v1.6 -vn0. -t.1 -da5
-l100. -ct20. -gs -gp -bf6,1,0,.5 -bf3,1.,0.,0. -bp1 -bp2

coptic14Jun does not give asymmetry. Seems my previous tests were
incorrect. Have to backtrack.

partaccum.f
mditerate.f
interpolations.f
cartreinject.f
And all the rest.

It transpires that there IS an asymmetry with the copticforward
version.

Ok I confirm that checking out the prior commit gives no asymmetry.
It was that last commit that caused it. One can go back and forth
between known versions by 
git checkout bc8df1863f8e0a8faa698e70cb03d52db0fc9dab
and  
git checkout 8c25d5581d646f0dec2efde1ffc07edb45f5261d

But don't do this in the coptic directory!!!!

Summary for today. 
The asymmetry bug arose between the above two versions. They differ
by the discovery of the argument problems passing lperiod and the 
reordering of the arguments of bbdy and other places, to try to 
avoid bugs. Seem to have introduced bugs instead. What is needed next
is to return to that prior bugged version and find out where the
asymmetry really turns on. Given that the prior bugged looked like
a compiler bug. I am in a very sticky spot. Might try a different 
compiler on Loki???

Return to the backward version 8c25d558. 
 mpiexec -l -n 2 ./coptic unif5.dat -s300 -v1.6 -vn0. -t.1 -da5 -l100. -ct20. -gs -gd -bf6,1,0,.5 -bf3,1.,0.,0. -bp1 -bp2
shows incorrect boundary setting. Periodic boundaries are not working.
But periodic are working with -n 1. However, -bf6 does not show the 
value -0.5, although the C0F does show 0.5.

Put a write on entry to bdyshare and of arguments just before call.
They don't agree. Arguments are not being passed correctly into bdyshare.

Constructed a bbdecltest.f that works and calls bdyshare. 
Made bdyshare implicit none. FOUND THE PROBLEM! It was an incorrect
length first line of argument list that happened to have the final
comma in column 73. It therefore concatenated that with the first
argument of the next line and misalinged all the succeeding arguments.
That was the issue. The passing of logicals etc was irrelevant.

Now how to back-track? I'd like to get the reorganization of the
argument order out of the picture to eliminate that possible cause 
of problems. However, the reorganization of the argument order did
indeed remove the column 73 error. So possibly one could accept 
that reorganization. 

Actually with the fixed copticbackward. I find that the asymmetry 
problem is present. Therefore, the asymmetry problem was in the 
copticbackward version but was masked by the error in the bdyshare
call.

I need to go further back. Try commit c1301209920104a0df8c3946e3a0aa3b3126f9cd
doesn't seem to show asymmetry.

Try 08ed85fb4c8e298 May 29th ditto.

Try d829957c4c6c515e9fd Jun 3rd has asymmetry.

Try 4383ebc59c3145 Jun 2 has no asymmetry.
Call this the new copticbackward 

Call d829957c4c6c515e9fd the new copticforward

Files ../copticbackward/3dobjects.f and 3dobjects.f differ
Files ../copticbackward/bbdydecl.f and bbdydecl.f differ
Files ../copticbackward/bdyroutine.f and bdyroutine.f differ
Files ../copticbackward/bdyshare.f and bdyshare.f differ
Files ../copticbackward/cmdline.f and cmdline.f differ
Files ../copticbackward/coptic.f and coptic.f differ
Files ../copticbackward/partaccum.f and partaccum.f differ
Files ../copticbackward/partwriteread.f and partwriteread.f differ
Files ../copticbackward/ptaccom.f and ptaccom.f differ
Files ../copticbackward/sormpi.f and sormpi.f differ

Copy partaccum ptaccom to backward. No asymmetry.
Copy partwriteread. No asymmetry.
Copy cmdline. No asymmetry.
Copy coptic.f. No asymmetry.

Copy sormpi.f Big changes and strangeness, so put it back.  Now

Files ../copticbackward/3dobjects.f and 3dobjects.f differ
Files ../copticbackward/bbdydecl.f and bbdydecl.f differ
Files ../copticbackward/bdyroutine.f and bdyroutine.f differ
Files ../copticbackward/bdyshare.f and bdyshare.f differ
Files ../copticbackward/sormpi.f and sormpi.f differ

Copy bbdydecl.f. Have to remove the ifull declaration from sormpi and bdyshare.
Then no asymmetry.
Copy bdyshare.f no asymmetry.
Copy 3dobjects.f no asymmetry.
Copy bdyroutine.f no asymmetry.

Files ../copticbackward/sormpi.f and sormpi.f differ

Doing ediff. Update idone to an array. No asymmetry.

Current differences:
232,234c232,243
< c Boundary conditions need to be reset based on the gathered result.
< c But that's not sufficient when there's a relaxation so be careful!
<          call bdyset(ndims,ifull,iuds,cij,u,q)
---
> c Boundary conditions need to be fully set based on the gathered result
> c at least for the master node, for aesthetic plotting reasons.
> c Call the parallelized boundary setting routine but lie to it that 
> c it is the only process. Also insist on explicit setting.
>       idone(2)=1
>       idone(1)=0
> c Actually it would not hurt if every process did this.
>       if(myid.eq.0)call bdyshare(idone,ndims,ifull,iuds,cij,u,q
> c     $    ,iLs,idims,lperiod,icoords,iLcoords,myside,myorig
>      $        ,iLs,ones ,lperiod,zeros  ,iLcoords,iuds  , ones
>      $        ,icommcart,mycartid,myid)
> c Old global setting. Obsolete.

Yes this is the difference that causes asymmetry.
The offending item is if(myid.eq.0). All processes must call the 
global boundary setting, otherwise they do not have the full potential
grid set properly. Since particles use that grid, it is not optional to
have it fully set. 

I think I ought to be able to fix this whole thing by removing if(myid.eq.0)
from the current version. Done. 

Commit Push
------------------------------------------------------------------
3 Aug 2012.

It would be quite easy to introduce collision time that varied as a
function of position or of velocity. All that would be required is
a call to some scaling function at the time of deciding whether a
collision has occurred. 

One possible use of this would be to make more efficient use of the
code in application to a collisional presheath. Since the scale length
of the presheath is the collision mean-free-path, one could scale the
collision length to be shorter, at positions further from the sheath. 
That would allow the distant solution to be well converged within a
shorter length, and mean that one did not spend so much of the PIC
effort calculating what is effectively the background problem, rather
than the local problem (e.g.) surrounding a grain in the (pre)sheath. 

It's rather important to have -rx0. for these full domain cases with
periodic conditions, otherwise instability occurs induced by spurious
attempts to adjust to the edge potential. Put in an exception to prevent
such operation.

In particle-free collisional gradients, there appears to be a z-flux
gradient (diag(4)) that depends upon the drift velocity specified. I
don't understand that. It is not reduced by smaller mesh spacing.
Or by wider transverse dimension. Actually it seems to correlate with
a time derivative of n_part. So perhaps it is simply correct that there
is a divergence. I'm going to adopt this view for now.

5 Aug 2012

Reimplement mditermults and mditeradd using the new mditerator.
They are much easier and more transparent.

Plan for rationalization.
1. Turn all calls to mditerate into mditerarg. Arg just allows more
   arguments on the end. Consequently straight replacement ought to work.
   Right now only bdyroutine and bdysetsol call mditerate.
   Do replacement. Nothing seems to break.
2. Reimplement mditerarg.

There's a problem reimplementing mditerarg. It is that the routine 
gets called with essential inconsistent indi sets 
For example with all lengths 6, the old mditerarg goes
  4  1  1  20104    1
  5  1  1  20105    1
  6  1  1  20106    1
  1  2  1  20201    1
  2  2  1  20202    1
versus what a straight forward reimplementation gives:
  4  1  1  20104    1
  5  1  1  20105    1
  0  2  1  20200    1
  1  2  1  20201    1
  2  2  1  20202    1
where the arguments have wrapped (correctly) at the 1-boundary.
This appears to be because in mditerarg:

c Wrapping occurs at the _iused_ dimension
c length relative to the starting indices.  In other words, if the
c incremented indi(id)-indinp(id)+1 exceeds the used length iused(id),
c then the next higher dimension is incremented by 1, and indi(id)
c decremented by iused(id).
I have not implemented the starting index part of this yet.
Fixed. 
Also had to implement iLscom setting. Then finally we get the same
answer.

Time testing on mditeratetest shows the new mditerarg is slower by 
perhaps 50%. Profiling shows that the costs are shared between the 
three routines rbroutine, mditerator, indexcontract; as one might
have expected. However, the speed is very high. The total time
for 1000x100^3 rbroutine calls is about 8s. So that's about 8ns per
point. The only place where that might be important is within the
sormpi iteration loop. I.e. boundary setting. And even there it is 
not likely to add up to much.

iLscom is a total hack. Several routines use it but it is set only in
mditerarg (I think). That's a major problem. Most of the usages arise
because of mditerarg argument conventions. Therefore the sensible
approach to getting rid of it would be to moved towards getting rid
of mditerarg. Alternatively we could change the argument list of 
the routine called by mditerarg to include it. 

There are ~15 different routines called by mditerarg. All would
have to be changed. A bit of a pain.

There would be residual problems with reduce because iLs is not included
in mditcom.f. Still, since it isn't used in those routines it probably
wouldn't be too bad. I don't think there's a problem.

git commit -a

-----------------------------------------------------------------------
fgrep 'mditerarg(' *.f | sed -e "s/^.*mditerarg(//g" -e "s/,.*$//g"

bdyface
bdyslopeDh
bdyslopescreen
bdymach
bdy3slope
bdyslopeDh
bdy3slope
bdyshrroutine
bdyshrroutine
cijroutine
volnode
cijedge
psumtoq
quasineutral
routine
ucrhoset
iteradd
iteradd
cijroutine
cijedge

Purged out the iLscom from all files, by adding to argument list.
Also in phisoluplot.f calculated and passed to needy routines.
mditerarg still needs to calculate it and that fixes reduce.

6 Aug 2012
Fixed an error in the new mditeradd.

git commit -a

Fixed bulkforce to use vneutral to calculate the momentum loss.
This should work correctly provided there's no driving Eneutral, which
is not (yet) implemented in coptic.

 mpiexec -n 2 ./coptic uniform2.dat -s400 -v-.6 -vn0. -t.03 -da10 -ds1. -l1. -ct20. -md4 -gs -gd -rx0.
produces a bulkforce that is still evolving (getting larger) with time during
the run. It also does not give agreement between different spheres.
Not clear what the problem is. 

Compare ./coptic -v1 -ct1. -ofgeomforce.dat -s200 -ri400 with new coptic 
    Field,       part,       press,     collision,    total,  steps ave 100 200
===== Object 1 ->  2 radius=  0.500 zcenter=  0.000 Charge=   -2.6908 =====
     0.00883     0.01075    -0.00072     0.01021     0.02907
     0.00612     0.00070    -0.00070    -0.00032     0.00580
     0.19566     1.17729    -0.02441     0.56033     1.90886
===== Object 2 ->  3 radius=  1.000 zcenter=  0.000 Charge=   -8.5194 =====
     0.01088     0.04131    -0.00717    -0.01681     0.02821
     0.00498     0.05935    -0.00304     0.01155     0.07284
     0.22069     1.23143    -0.15176     1.19035     2.49071
===== Object 3 ->  4 radius=  1.500 zcenter=  0.000 Charge=   -6.0372 =====
     0.00744    -0.00210    -0.01884    -0.04048    -0.05398
     0.00531    -0.03313    -0.01100    -0.02103    -0.05985
     0.05078     1.07863    -0.32375     1.65627     2.46194
===== Object 4 ->  5 radius=  2.000 zcenter=  0.000 Charge=   -3.9712 =====
     0.00473    -0.05140    -0.04172    -0.05803    -0.14642
     0.00031     0.04795    -0.01667    -0.08757    -0.05599
     0.00546     1.02033    -0.43453     1.96473     2.55599

and Jun14th version:
    Field,       part,       press,     collision,    total,  steps ave 100 200
===== Object 1 ->  2 radius=  0.500 zcenter=  0.000 Charge=   -2.7091 =====
    -0.01209    -0.01049     0.00118    -0.00415    -0.02555
     0.01653     0.00276    -0.00164    -0.00548     0.01217
     0.19764     1.06559    -0.02442     0.55081     1.78963
===== Object 2 ->  3 radius=  1.000 zcenter=  0.000 Charge=   -8.5595 =====
    -0.01439    -0.02641     0.00924    -0.02864    -0.06021
     0.02715     0.00463    -0.01409    -0.04490    -0.02721
     0.21429     1.16204    -0.14992     1.24086     2.46728
===== Object 3 ->  4 radius=  1.500 zcenter=  0.000 Charge=   -6.1374 =====
    -0.00737     0.02001     0.02408    -0.08309    -0.04637
     0.01199    -0.05386    -0.03684    -0.06926    -0.14796
     0.05046     1.00457    -0.31815     1.72937     2.46625
===== Object 4 ->  5 radius=  2.000 zcenter=  0.000 Charge=   -4.0830 =====
    -0.00037    -0.02474     0.03537    -0.17158    -0.16132
     0.00638     0.04012    -0.06936    -0.12916    -0.15202
     0.00812     0.75871    -0.43419     2.02579     2.35844
Not identical but pretty good agreement.

That is with point charge radius 1. potential -1. which is less than
what I am running with sheath model: radius .4, potential -1. The
mesh spacing is ~.25.

Did a new run with sheath model and -ct2 still does not seem to make any
consistent sense. Still evolving substantially.

3 Aug 12 Non-uniform sheaths. See hand written notes.

22 Aug 2012

Implement separation of Tneutral from Ti, in cmdline setting.
In padvnc change tisq to be sqrt(Tneutral) instead of sqrt(Ti).
Thus we can make the temperature of the reinjected ions Ti different
from that of the collisional ions.

Thinking about giving true drift distribution input to reinjection.
Functions ffcrein and fvcrein are the crucial ones. They are
Maxwellians (at present) that are integrated using cumprob to give
the flux and velocity distributions of various dimensions. The idea
is that we change these just for the drift direction (z). At the
moment these are unnormalized. This does not matter because the
same functions are used for all the faces and so the relative probability
is all that counts, and the relative unnormalization is the same. 
If, however, we substitute a different function, the collisional drift
function, for one dimension, this must be correctly normalized relative
to the Maxwellian function. Those functions are given in terms of the
standard dimensionless velocity (i.e. not normalized to the ion or neutral 
temperature thermal speed; the T_i is explicitly accounted for in the 
function). And they are equal to unity [times abs(v) for ffcrein] at the
Maxwellian's peak. It seems that if we normalize them so that the 
area integral \int f(v) dv = 1, then we can normalize the drift distribution
the same and we'll be able to use both simultaneously for different
faces. This means we have to divide by sqrt(2\pi T_i). Multiplying
each by 2 gives no change to test output. Dividing by  sqrt(2\pi T_i) gives
a change. I'm assuming this is a rounding problem.

Implemented drifting distribution in the reinjection. In general, this 
will revert numerically to drifting Maxwellian distribution when vneutral
is equal to vd. Therefore, the code is left as the default for all cases.
The code for 1-D distributions with strong magnetic field not fixed
implemented with the collisional drift effects yet. 

Tested using testing/creintest.f with some development to normalize the 
routines and compare them. 

It would be helpful also to have an absorbing boundary choice such that
particles are never injected from some boundary, e.g. because it is a
wall. The natural way to do this is to set fcarea to zero for this 
face (as I do with periodic conditions), but this has only 3 dimensions,
so it does not work when a face is opposite a non-periodic face.
Therefore I have to set grein(id) equal to zero for this face, prior
to the calculation of gtot and gtrein. 

How to denote this? Probably use different values for ipartperiod. 
We could use the first two bits to denote non-injection for the lower
and upper faces in a specific direction. In that case, 1 would denote
non-injection lower, 2 non-injection upper, 3 non-injection both.
Perhaps we therefore make 4 periodic particles. In that case 3 can
be used for two absorbing faces opposite one another. 0 means nothing
special, open boundaries. Need then to correct other routines for this
change of meaning. 4 <-> periodic.

There's still an issue of ninjcalc rhoinfcalc etc. This must be fixed
in fonefac. 

24 Aug 12
Implemented new section in fonefac that represents the fluxes in either
or both faces for possibly different vd and vneutral. This is supposed
to give the flux from the general drift distribution, which can vary
from drifting maxwellian (when vd=vneutral) to pure drift when vneutral=0.
This section gives the same result as the old explicit form for a 
drifting maxwellian when vd=vneutral (after corrections!). However, 
it is not fully tested for situations when vd != vneutral and the drift
distribution is fully in play. 

25 Aug 12
Seems to be in good shape now. BUT proper treatment of collisions with
magnetic field has not been implemented. So the drift-collisional 
distribution is not used with infinite field, and it is not correctly
field-aligned with a finite B-field. 

Collisional sheath simulations work much better using phi derivative equals
zero on the upper face. We now can also set the lower face to be absorbing
so that the physics is much better defined. One can run these without
collisions too and the potential tends to be much flatter in the presheath.
The diagnostics don't seem to be what we expect at the z-boundaries. 
I'm not sure the injection is right. Found the problem. Caused by my
non-intuitive order of grein chosen long ago. Fixed.

Now one needs rather slow injection speed (relative to zero neutral
speed) to avoid a non-monotonic phi. About -v-.1 for -ct10. I don't
understand that. Can make the -ct number high (e.g. 50) without much
change.  Actually mpiexec -n 2 ./coptic uniform3.dat -v-.5 -rx0. -da10
-s200 -vn-.0 -t.1 -md4 -ct40.   does not give non-monotonic, except
just at the end, so maybe the non-monotonicity was a different
problem. I can't reproduce the non-monotonicity immediately.

28 Aug 12
Changed the -gd -gp switches to be more rational, and removed -gs switch.

12 Oct 12
Problems of data storage and restarts. 

At present on loki the end of the run is bogged down if one writes 
the particle data to disk because each processor tries to write to the 
central (head) disk and the NFS has a problem with all the pressure on 
the ethernet with all trying to write at once. The result is that it can
take ten minutes to write the data (files of maybe 100MB per node). 

Actually loki has local disk space to which the writing could be done.
Then after the fact one could go back and grab the written files if
one wanted, or a script could copy them sequentially to the head node
(thus avoiding ethernet collisions). 

Rewrote datawrite, namewrite etc to prefix files with the path kept in
restartpath, which can be set with or without restarting.

Reorganized restarting to use an integer switch. It can then indicate
whether or not we are reading flux (bit 3: 2). If we are just restarting
from a saved state in order to initialize the particles and potential, 
then we normally don't want to read the flux file. For that we would
use -fs1. A full restart is -fs3, which shows up by the fact that the
step number adds to the previous steps instead of starting from 1. 

13 Oct 12
Added ability to use the name 'restartfile' for the restartfiles.

18 Oct 12
Stop slave nodes warning about special particle error.
Added collision settings to write/readfluxfile. New version 2. 
Fix coptic -h call when coptigeom.dat file is absent.
Fix diagnostics ipartperiod(i).gt.0 error in meshconstruct.
Fix bug in diagfilename generation arisen from name code changes.

I realize that COPTIC does not have an Eneutral. So when -v and -vn 
are different, we need an actual potential gradient to cause that.
We can't just take it to be the case and do reinjection accordingly.
In sceptic Eneutral=colnwt*(vd-vneutral). In COPTIC colntime is the
inverse of colnwt. So presumably Eneutral=(vd-vneutral)/colntime.

Another issue I find is that in low collisionality sheaths, the density
evolves very slowly, and is not in equilibrium by the end of the run.
We ought to have a clear diagnostic that this is happening. Perhaps
simply the npart (and rhoinf) or their ratio.

Implemented saving of n_part. And fluxexamine gets it back and plots it.

19 Oct 12
Understanding the shark-fin behaviour of n_part on restart. Did an experiment
multiplying dtprec by bdt on restart. This is to find out if it is something
to do with dtprec that makes the sudden jump. Shows very little difference
with or without this multiplication. I conclude it is not just a transient.
This conclusion is consistent with the fact that the sharkfin looks like
the timestep, not like its derivative. 

Try running cases with different -dt. .1, .5, .03. These seem to show
that there is a systematic increase of n_part as dt becomes larger (in
near steady state). For -dt.03 about .96e5, for -dt.1 about .963, for
-dt.5 about .97. However, these do not seem to be fully consistent
with the enhancements associated with acceleration. Those might be
dominated by the initial transient. So one probably needs to look at
restarts for real consistency.

20 Oct 12 

Problem with the collisional part of the force for the new
collisional treatment.  As of now, the bulkforce is taken as given by
the sum over particles in the measurement sphere of
(vz-vneutral)/(rhoinf*colntime). Seems to be correct. But in runs it
is way out of whack, giving values of about 4 for spheres of radius
1.5, when rhoipernode= 500 for 8 nodes. Total rhoi is about 4000, so
there are 4\pi 1.5^2/3 ~ 10*4000 in sphere.  If collision time is 2,
then the normalized would be about 5 (check) but perhaps the problem
is that dt is not accounted for. It seems correct because the force is
the momentum difference per unit time. So we should not multiply by
dt. The particle force is obtained by adding all the particles
crossing and dividing by dt and rhoinfinity. The collision force is
the momentum (difference) times dt/colntime normalized by
dt*rhoinfinity which is what I've got.

I think the issue is the absence of Eneutral, in COPTIC, versus its
presence in SCEPTIC. If we consider a uniform quasineutral plasma, in
which the ion drift is driven by Eneutral, then within a measurement
sphere the loss of momentum to neutrals is made up by Eneutral. In
sceptic Eneutral is explicitly added to the ion acceleration, and the
colnforce is collf=-colnwt*(vz-partsum*vd) which is -(vz-vd) per
particle, while Eneutral=colnwt*(vd-vneutral). Assuming vneutral to be
zero, this makes force per particle=-colnwt*vz+Eneutral, which cancels
out when the vz is equal to vd. The remainder of the electric field
is explicit and acts on both the ions and the electrons. Eneutral does
not act on electrons (effectively). 

In COPTIC there is (as yet) no Eneutral. All the drift comes from the
explicit field. The explicit field acts on both ions and electrons.
There is then no steady uniform state with Boltzmann electrons,
because the potential is varying, so the electron density is
non-uniform.  The electron pressure force then balances the electric
field force on the electrons. If electron and ion densities are equal
so the charge density is zero, then the field force on the ions is
equal and opposite to that of the electrons. The ion electric field force
is balanced by the collisional drag force. Therefore in the absence
of a grain, the collisional drag force ought to be equal and opposite
to the electron pressure force. (That's not quite true if there's
any imbalance of charge density, but it ought to be close.)

This does not seem to be happening. Looks like a bug.

At z=20 the potential gradient is 0.13 in 2 i.e. 0.065.
Also the value is -0.6. 
So grad(n)/n_infty is (e/T)grad(\phi) exp(e\phi/Te) = 0.065*exp(-0.6)
= 0.036
At z=10 it is ~0.4 gradient and value -2.0. grad(n)/n_infty=0.054

For a volume of 4/3\pi ~4 (radius 1) we expect a pressure force
roughly .14 or .2. We actually get .17 and .19, I think that's near enough.

The collision force is about 10 times larger. Can't be correct. Is it 
possible that I am not dividing by the summed rhoinfinity? Doesn't seem
so. However, I am running with -rx0. which means there's no accounting
for the potential of reinjection in calculating rhoinf. That might be 
a significant effect (but not a factor of 10). 

There are at the end about 270000 particles in one process in volume 
40x5x5=1000. So 270 particles per unit volume, ~1000 in unit sphere.
For 8 processes there would be 8000. If colntime is 10 which is what I
am examinining, then there would be 800 collisions per second. Each would
lose a momentum of about 1 if the flow is roughly v=1. So the momentum
divided by the rhoinf (~2000) would be about 0.4 whereas I'm getting
about 1.5. This doesn't quite resolve the problem. We are sort of half
way between. Actually at z=20 the velocity given by diags is about 0.5.
And if I took that, I'd get force around 0.2 which is about equal to the
pressure force. At z=10 v=1.5 but there the density is substantially 
lower, so probably the collisional force would be substantially reduced.
Conclusion: it looks as if the value of the collision force is too large
in the code for what it ought to be. 

Running with 2 process produces a collisional force only < factor of 2 less
than the 8 process case. There is not an obvious problem with node accounting.

Doing a restart for a second 1000 steps and then averaging over 1400-2000
produces far smaller collision and residuals. Looks as if it is working.
Maybe it was working all along but just not converged.

There's another problem with the x/y boundaries causing some sort of density
rise. With 8 processes the rise is about 8%. With 2 processes, about 6%.
Not clear that this is significant. But I need to figure out why it's there.

One problem is that the mesh is visibly asymmetric. 

That's a particular problem with periodic domains because the presumption
is built in that the cell-size is the same on either side. But fixing it
does not fix the particle excess near the boundary. It does not show up
on the density, only on the particle distribution. That's peculiar.

22 Oct 12

Tracking down the edge particle excess for the sheath domains.
First do particle accumulation in restricted z-ranges. There's no
obvious variation with z. That suggests it cannot be caused by 
nonuniform injection across the z=40 face, because that would 
quickly be smoothed out by subsequent transverse travel.

Note, this 6% ion density variation for transverse temperature of .02
would arise from the ion response to an effective potential such that
exp(d\phi/T_i)=0.06, i.e. d\phi = 0.06*0.02= 1.2e-3 really small!

Looking harder at phiexamine, I see there really is an edge potential
drop of order 1.e-3 between the second and first cells from the boundary.
(The boundary itself is periodic and there's not much difference between
it and the adjacent cell.) Thus we have a physically consistent result
in that the ions are responding to the presence of a (very small) 
attractive well at the boundary. This is for the 8-processor run which
has uneven cells on either side. Looking at the 2-processor runs with
symmetrized cells, they still have some potential effect but it's nearer to the 
noise level. There's a sense that perhaps it is the corners that are
more systematically low. Running a 8-processor symmetric case does not
get rid of the particle excess, and confirms the 1.e-3 edge potential drop. 

The lower-potential edge is counter to the higher ion-density there. 
In other words, the potential value is inconsistent even though the
particle density is consistent. The problem must lie in the determination
of potential. 

The question is, where is that well coming from? 
The objectdata file has
####################################################################
# Domain BCs
# Periodic phi-BC in dimension 1 and 2.
111
112
# Specified phi on face 3 (lower z-face)
103,1.,0.,10.
# Specified zero derivative on face 6 (upper z-face)
106,0.,1.,0.
# partperiod: particles periodic in directions 1 and 2, absorbing lower 3.
98,4,4,1
# It's important to end the last line.
####################################################################
This appears to be correct. 

The cell size is 2/10 at the edge, which is 0.2/5 = 0.04 of the full
width (5). If there were a second order error arising at the edge
perhaps it would be the square of this which would indeed be 1.6e-3.
Thus we might need to look very hard at the difference scheme.

Run a case with uniform 48 cells in x and y. Reduces the cell size to
5/48 ~ 0.1 (vs 0.2). If the problem is second order in cell size,
this ought to drop the effect by a factor of 4. In fact the particle
excess at the edge (partexamine) is 0.3 in 5, i.e. still ~6%. It has
not changed. Similarly, the potential dip at the edge is ~1.e-3.
This seems to show decisively that it is not a resolution problem. 
The error is independent of cell size. 

I notice that the potential drop gets smaller as we go lower in the
sheath, so it is nearer 1.e-4 at the bottom. This is probably consistent
with there being a perturbation that is a constant fraction of the ions.

What might make the corners more susceptible to this drop?

Try increasing the timestep to .2 to see if that changes the magnitude.
Makes no difference. 6% particle excess, ~1.e-3 edge potential drop.
It's not a timestep scaled error.

Checked the volumes at the edge of the mesh. They are
0.  0.00121212041  0.00121212006  0.00121211959  0.00121212006
showing that rounding is only at the 10^-7 error level.

Looking at denexamine, we see that the density appears low at the
edges.  By about 0.04 near the top (much less, ~0.01 near the
bottom). This contrasts with the particle distribution which gives it
6% high. Seems to show that there must be errors in translating from
particles to density at the points adjacent to the boundary.
This density effect is NOT visible in diagexamine. That's interesting.
The particle boundary exchange for diagnostics is done by diagperiod.
Meanwhile the psum boundary exchange is done by psumperiod.
Perhaps there's a bug in psumperiod. There is a difference, in that
psum on the boundary is not set to zero, while diagsum is, in the
process of particle transfer. 

Test whether using diagperiod in place of partperiod gets rid of the
problem. It's too hard to tell with 2-processor case. In the 8-processor
case it is clear that the replacement of psumperiod with diagperiod
has NOT fixed the problem. 

However, I note that psumreduce does not reduce the boundary nodes.
Moreover it is called prior to psumperiod. Therefore, it appears that
there's a bug in that not all the boundary values are going to be 
reduced to the final result. So try exchanging the order of psumperiod
and psumreduce. 

YES!!! This was the bug. Because of incorrect order of period cross
sum and psumreduce, the multiprocessor exchange was not being
correctly evaluated. With them swapped, it is and the particles are
uniform.

Conclusion for commit:
---------------------
The particle non-uniformity for periodic faces was a bug arising from
incorrect order of psumperiod and psumreduce. Because psumreduce does
not reduce the boundary values, the periodic sums psumperiod must be
done BEFORE the psumreduce. Now fixed.


Rationalized the restart code so that it does not matter if the potential
is not correctly set. In any case it is simply calculated from the particle
data, so it does not matter if one fails to read the potential file.

There's a problem, though, with chargetomesh, the first time it is called
after a restart, if the mesh spacing has been changed. It is that the
top 3 components of x_part, which give the mesh position, are inconsistent
with the position in the context of a changed grid. Chargetomesh uses
these in preference to partlocate. Fixed this with a new initialization.
locateinit.

Improved partexamine to allow summing subcells over dimension x,y, or z.

Fix restart so that it does not read the potential file if it fails to 
read the flux file. The reason is that the potential file contains 
ixnp, xn etc, which clobber the mesh setup. So if a potential file
is read then one can't restart with a different mesh spacing. Arguably
this could be fixed by simply ensuring there's no potential file to
read. But I think it is probably safer to avoid the read in all situations.

Edit the RefManual to reflect recent changes.

Tracked down the problem with titles in denexamine. It is that there's
a bug in the extension of slicegweb to have optional array arguments.
That can't be done because one of the arguments is a character string.
It's length is invisibly added on to the arguments at the end. If the 
call has fewer arguments than the definition, then that character 
length specifier is in the wrong place and the subroutine gets confused.
Actually it grabs something beyond the argument stack, which might 
cause a segfault. This needs to be fixed. One way to fix is to give
all the calls the full argument list with two dummy real arguments
after the utitle. This is what I've done with all the analysis routines
and coptic routines for now.
Possibly we should change the subroutine name of the extended version
and wrap it in the old name with dummy arguments. Not done.

9 Nov 2012.

Running check shows that restarts are not now correct. That's a problem.
Commenting locateinit does not fix. Bad. It's not clear how long this
problem has existed. Things do not look so bad on loki. But there are
in fact differences with that version. I need to track back.
Coptic 14June has same differences. So this has not been introduced 
by the recent changes. Looks as if it might even predate the git
repository move. Even so, I ought to be able to get a version out
of the repository if the cvs import kept track of all versions.

Using copticforward to bisect to where the error occured.
2012-01-04 ok.
2012-04-17 broken.
2012-02-09 broken.
2012-01-15 broken.
2012-01-05 ok.
d4661836cc2e035fbd83aece5b6b0bdaa25cf1d4 09 broken.
8d8b068bd920ffdeea22f6508a286f590ba1a811 04 ok.
c22bbfa3aa8ae99c9cf0528267bb8f85802c405c 05 ok. 
627bf5af9ea6b1ea5858c65cacef01f6f734dee1 05 ok.
--This is where the problem arises.
d4661836cc2e035fbd83aece5b6b0bdaa25cf1d4 09 broken.

This is the place where the command line argument setting was encapsulated
into cmdline. That gave me big strife before too. Also boundary setting
was changed.

10 Nov 2012

Went into copticbackward. Deleted some files so that 
git checkout 627bf5af9ea6b1ea5858c65cacef01f6f734dee1
would work.
Then ./check is broken. This seems to contradict the above results.
That's very puzzling.
Cloned the home COPTIC to copticfix. 
If finds 627bf5af9ea6b1ea5858c65cacef01f6f734dee1 checks ok. 
copticbackward is somehow broken. Delete. 

copticforward is the broken version
copticfix is the unbroken version.

In copticforward, put aside coptic.f copy coptic.f from copticfix.
./check is unbroken. Hence the problem is in the changes to coptic.f.
These are almost entirely the encapsulation of the arguments.
Things become confused again in copticforward.
Do the same cloning process to get copticbreak version. It is broken.

copticbreak and copticfix give different solutions on make regardless
of the restart. break:0.875556827 fix:0.875226319. Must be some
effect of the reorganization.

Tried doing the initializations. Did not help.
Tried redoing the commandline interpretation in line. No improvement.
Put in initialization and inline cmdline. Comment cmdline call.
Fixes! Uncomment call. Unfixes. Moved the call till after the
initialization. Still broken. 
Ah! The difference is in crelax. With old version it is 0. With new
it is 0.5. Putting back to 0. gives old result. 
Taking the broken version, setting crelax to zero after the cmdline
call fixes both the difference in value and the restart check.
However, setting it explicitly to 0.5 causes the restart to break.
Non-zero crelax, which only happens by default in the new version is
the cause of restart errors. crelax multiplies phirein and averein in
cartreinject.

The following statements in cartreinject are both causes of check
errors when crelax is not zero.:

in avereinset
      averein=crelax*phi+(1.-crelax)*averein

in rhoinfcalc
         chi=max(crelax*(-phirein/Ti)+(1.-crelax)*chi,0.)

Currently averein is not stored. So avereinset is definitely going to
fail to set averein correctly, since it depends upon the prior value.
Averein is used in only one place in cartreinject: 
	x1=(x2+2.*max(0.,-averein))/x2
which is where the reinjection energy is scaled up by averein. It's
of dubious value. That checks out as the place where the averein 
error enters. Also chi is a saved variable not read back. So 
both of these averaging problems arise from non-writing of needed
information if crelax.ne.0.

Fixing this is awkward. averein is hardly used, but lives in reincom.
Really reincom ought not to be used by cartreinject because it is
some extra stuff in reincom.f left over for orbitinject. reincom.f
cannot be widely used because it has spurious stuff in it. The other
variable in reinextra is adeficit, also not used except in orbitinjnew.

Probably all cartreinject usage should be divorced from reincom.f and
hence from averein. Get rid of averein from cartreinject and use
a variable caverein instead. Also get rid from coptic.f. Put caverein
into partcom and write/read it. Then we solve the restart check for
the caverein. 

Do the same for chi: put in partcom, read and write. That fixes the
phirein problem. Done. Checks without error. 

15 Nov 2012

Upgrade diagexamine to enable averaging over transverse directions and
plotting the profile of the result in a particular axis direction.
Also upgrade reading of additional potential and density files, 
with the simultanous profiling of them.

23 Nov 2012
Allow a region to be _ex_cluded by a non-field-object in the object file.
This is to allow a point charge to be specified with a finite-radius 
region around it that does not specify potential boundaries but just
ensures that it acts as an absorber. This ought to enable us to have
finite sized small objects too small to be resolved by the mesh, but
with a specified charge, using the PPPM technique.

28 Nov 2012
Changed the handling of excluded finite grains represented by point charges.
If a node is outside the particle region, but inside a point-charge region
(which is the case if we have concentric point charge and finite sized
grain) then do not set its charge to compensate the electron density by
setting rho=faddu. That is problematic in such a situation because the
rhoci of the point charge gets included in faddu. Instead simply set the
(ion) charge to zero. 

6 Dec 2012
Idea about drift distribution reinjection for collisional cases with 
different velocity dependence of cross section. Riemann shows, that 
for (stone) cold neutrals, one can obtain a closed form solution of the
Boltzmann equation for constant cross-section, when the potential gradient
can be approximated as uniform. This is a good approximation for large
distances from the wall in a sheath, because we are into a regime where
there is only logarithmic dependence on distance. 

This works only because the transverse velocity dependence of the solution
disappears, because the transverse velocity is zero. In other words the
cold neutral approximation turns the problem into a perfectly 1-d problem.
Finite transverse temperature cannot be accommodated in that way because
any collisionality other than \nu=constant couples the transverse and 
longitudinal dependencies of f(v). However, if the neutrals are fairly
cold, T_n << v_d^2, then it will still be quite a good approximation to
take the longitudinal distribution to be the 1-d solution and just
add on the transverse Gaussian. This approximation ignores the coupling,
but that is justified if the T_n is small enough. [It's not clear whether
one can improve the approximation by getting a 1-d solution whose 
collision rate accounts for the transverse velocity at low longitudinal
velocity.] 

At any rate, it seems not unreasonable that such an approximation would
work rather well for low neutral temperature reinjection with more 
general collision cross-section form.

8 Dec 2012
Implement command line argument to specify the cartesian topology.
Fixed bug in dims_create.c of mpich2, so it finds the right distribution
of dimensions when all the factors are the same.

12 Dec 2012
There are required corrections to moveparticle and accelerate particle
for magnetized cases. It would be better to rationalize that whole
code region of padvnc.f

15 Dec 2012

Doing a big clean up with help from ftnchek. There is a major issue
that is a violation of fortran standard but probably does not make any
problem. It is in places where I pass e.g. objg into pllelplot. objg
is a particular object element from the array obj_geom, which is defined
in common objgeomcom. When objg is modified in such a routine, this is
a violation of aliasing rules in fortran, which state that if two dummy
arguments are aliased to one another, they may not be changed in the 
routine.

Working on this in objplot.f Fixed pllelplot. Had to change fluxdata.f
too. Fixed cubeplot. That fixes objplot.