SOR NOTES

Problems with constructing the MPI red/black boundary communicators
___________________________________________________________________

In general, the construction of the multidimensional odd/even
communicator can be thought of as follows:

odd(0)=this element
even(0)=null element
for dimension n=1,nd
	laminate alternating odd(n-1)/even(n-1) into odd(n) face
	laminate alternating even(n-1)/odd(n-1) into even(n) face
endfor

For n>0, the number of non-null elements is the same for odd and even.
But for n=0, the even element is of length 0 which is troublesome.

Therefore it might be better to start at level 1:

odd(1)=odd elements in this dimension
even(1)=even elements in this dimension
for dimension n=2,nd
	laminate alternating odd(n-1)/even(n-1) into odd(n) face
	laminate alternating even(n-1)/odd(n-1) into even(n) face
endfor

This construction must be done nd times, for each one, omitting the 
direction of the face normal nn from the dimension iteration. So really
it is

for dimension n=1,nd
	if(n.ne.nn) then
	laminate alternating odd(n-1)/even(n-1) into odd(n) face
	laminate alternating even(n-1)/odd(n-1) into even(n) face
	endif
endfor

or maybe better:

for dimension nc=nn,nn+nd-2
	n=mod(nc,nd)+1
	laminate alternating odd(n-1)/even(n-1) into odd(n) face
	laminate alternating even(n-1)/odd(n-1) into even(n) face
endfor

Because we are alternating types, we have to use
MPI_TYPE_CREATE_STRUCT, which is the only constructor that allows
different types in the creation.  In that case, we need arrays of
lengths,displacements,types. The length of these arrays is (1/2 times)
the block-side-length, which won't be known. We would either have to
choose a long length or allocate working storage.

Also, it is not clear that there exists a null element in MPI.

If we have to provide working storage, then maybe we ought just to
construct our own indexes and create the communicator directly by
indexing. Then we only have to provide an index array. But it might
have to be substantially bigger.

If we do the lamination directly, what does it consist of?

The even or odd type can be considered to consist of an array of
indexes, referring to the addresses of the values, plus a total length.
A lamination then consists of taking an odd type, taking an even type
shifted by appropriate amount, and concatenating them, then taking
a shifted odd concatenating that, ...

oddnm1=start of odd
evennm1=start of even
odd=start of new odd
do i=1,nsidei/2
   do j=1,nsidej/2
      index(jc)=index(j+odd)
      jc=jc+1
   enddo
   do j=1,nsidej/2
      index(jc)=index(j+even)
      jc=jc+1
   enddo
enddo
ditto for even

-------------------------------------------------------------------
7 Jan 06

Got the block-divided SOR to work, running on MPI processes (on same
machine). Then realized that the gather, to get the individual
solutions back into one big matrix is quite tricky.

One problem is that the order of the blocks generated by the MPI_CART
creation is row major, not column major like fortran, i.e. it goes
in the order 1 (0,0); 2 (0,1); 3 (0,2); 4 (1,0); 5 (1,1); ... etc.
Consequently, when you do a gather, the blocks come back not in the
order you want to store them in in fortran, which would be column
major. That makes it much more difficult to specify the gather, because
a gather puts the data from process-j, in the j-th storage position,
relative to the starting point. It would all be trivial if the blocks
were all the same size, and the order was right. 

It seems that this ought to be fixable by reversing the order of the
suffices. It would be too confusing to do this in the outer fortran
call and specification, I think. But presumably it could be done inside
the block-specification program. The problem can presumably be fixed
by reversing the order of dimensions in the calls to MPI_CARTs.
Yes that seems to work fixing all the MPI_CART calls (create, get, shift).

Now in principle one ought for equal sized blocks to be able to do a
simple gather. There are still major problems with unequal sized blocks.
I suppose that one can construct a generalized call that will interleave
the larger blocks. The iorig pointers to block start are now in the order
of mycartid. This enables iorig-1 to be used for the displacement array.
If all blockstypes were equal we could do
call MPI_[ALL]GATHERV(u,1,blocktype,u,[1],[iorig-1],blocktype,
	icommcart,ierr)
but the problem is that [iorig-1] means an array with values equal
to iorig(j)-1, because of the zero-based C offsets used in the MPI
routines, compared with the 1-based fortran pointers. It would be
possible to change my convention to zero-based. Then a few modifications
elsewhere would be necessary.

Alternatively, one could presumably not bother with all this reordering
of the mycartid, and instead manually reorder the blocks for the
purposes of calls. Do this by creating an array that translates 
mycartid into fortran order. In other words ifortorder(mycartid) =
fortran column major ordered indices, then iorig(ifortorder(j))-1
is the displacement of the jth block. 
	do j=1,nproc
	   idisp(j)=iorig(ifortorder(j))-1
	   irecvcounts(j)=1
	enddo
call MPI_[ALL]GATHERV(u,1,blocktype,u,irecvcounts,idisp,blocktype,
	icommcart,ierr)
Then none of the fiddling around with dimension reversal ought to be 
necessary. There might be a slight memory access hit because the blocks
will come in a strange order. For that reason, the dimension reversal
might still be worth it. 

But wait, it says that the equivalent receive is 
MPI_Recv(recvbuf+disp[i].extent(recvtype),recvtype,i...)
whereas I am taking the displacement to be in units of the underlying
REAL, not the extent of the structure: A problem.

On p207 there seems to be a thing called MPI_UB which is the equivalent
of a null send, but takes up space in the MPI Type. I guess this is 
within a struct. Apparently this is for setting the Upper Bound of a
structure, and there is a type MPI_LB for lower bound. They are not
listed in the type map but affect the value of extent. Apparently
"MPI_TYPE_STRUCT is the only constructor that allows MPI pseudo types
MPI_UB and LB"

-----------------------------------------------------------------------
16 Jan 06
Now have a 2d specific sor2dmpi and a general-dimension sormpi version
going that includes the MPI communications. The general dimension
version is not really fully tested at this stage. Also its Jacobi
radius has a rather different optimum for convergence.

After a day of work we seem to have a multidimensional version of sormpi
working. 

There is a bug somewhere because although different block divisions
of the first two dimensions give identical results, different block
numbers in the third dimension give subtly different results. This 
should not happen.

18 Jan 06
Found the bug, it was in the choice of parity when using the blocks
with the boundary excluded. The parity must be changed by ndims.


_______________________________________________________________________

Turning SORMPI into a solver for embedded boundaries.
_______________________________________________________________________

29 Dec 06

Implemented a test routine comparing solution with smt.out. This is
to discover any errors that I might accidentally introduce. smt.fixed is the
output with which to compare. Comes from the charge-ball case. 

Plan is to introduce boundary handling into this solver code.
For memory access optimization, the plan is to make the pointer to 
the boundary information part of cij, namely the element 2*ndims+1. 
For this I need to fix some addressing in the routines, but not very much.

Added the extra cij slot as cij(2*ndims+1,i,j,...). Works. 
Document minimally. 

Passed the object data obj(idata,jpointer) through to sorelaxgen, which
is where it will be needed for the treatment. Also passed the length
nobj of the obj data (leading dimension). Demonstrated calling it.

30 Dec 06

Now we need a way to implement the objects. I have decided to separate
the object implementation from the rest of the code. That way the instability
of geometric object representation is separated. One can use whatever 
representation one likes provided one has a way to translate it into the
object data that we are going to use. However, we need an object-data
api or specification for how the object data is to be represented. This
needs to be fairly compact, and also not to require lots of processing
during the iterations. There are questions about different numbers of 
dimensions. 

If we make the handling of object data through a function or subroutine
call, we can make the api a matter for the user to specify. However, if
it is too general, then lots of things have to be passed. That would
be cumbersome. It is probably best to make sure that the cij do really 
contain all of the weight from adjacent points that we wish to put into
the numerator. Then the effect of the object code will be to add some
constant quantities. It seems as if this is all it really needs to be able
to do. In fact the main thing appears to be to have the quantity (bdy in
ES2D) to add to numerator, and maybe diag, since this is not now reliably
calculated by adding cijs. For the purposes of potential solving, this may
be all one needs for any given point. 2 quantities. For field interpolation
other information is almost certainly needed.

1 Jan 07

Slow progress. Implemented a cijroutine that is called by mditerate to
set the cij. The cijset.f was an explicit iteration, that I don't really
need since I use mditerate.

Fiddled with getting it to avoid the boundaries properly. Now ok.

Set up series of smt.out to do checking with. smt.1 is the output
from the original, Jan 2006 verified version. smtround.1 is an alternative
output with reduced significant figures. The idea is that if we
get things nearly right, they will match.
i
smt.2 and smtround.2 are the versions that implement the cij in terms
of dxs. They differ by rounding arising from the redefinition. The
difference between smtround.1 .2 are all in the final place. Still 
some differences at the 10^-4 level. Move the test to smt.2 so we don't
have to retain the nint that prevents rounding errors.

Implemented much of the infrastructure for handling the potential
intersections. What remains is to construct potlsect which returns
for any point, dimension, and direction the fraction and potential
value for any intersection between it and its neighbour. It returns
fraction 1 if there's no intersection.

There's a trap with the offsetting of indi that needs to be fixed.

2 Jan 07

Fixed the trap. Rationalized the routines. Wrote a plotting routine
to display the results of the cij setting. There's a problem with that
to some extent that myid is not known prior to the sormpi call, so all
the nodes do plotting, unless you do it after calling. If you do it before,
then all nodes call. One approach is to just call ./sormpitest then. 
That crashes after the plot as it tries to sormpi. Never mind.

Test obj setting seems to work for one point.

Got a sphere apparently working in cij and its plotting, but potential
solution is crazy. Saved its output as smt.b1 for now. 

That problem was that I was not doing the obj treatment in the solver.
Implemented it. Seems to work. 

Switched to pure Poisson (not shielded Poisson) solver. Looks good.
Have to be careful with the external declaration.
Turned off charge density for the test saved as smtsphere.1. Had to turn
off the hand-set object data too. Done.

We seem to have a working solver. Ran a 80x80x36 case in about 10s
on laptop. Took 321 iterations.

3 Jan 07

I realize that there might be good reason to include additional data
into obj. A couple of ideas: 

1. the object number(s) that the intersections correspond to. This
would be useful for calculating the total flux out of a given
object. However, it would have to be specified for each of the
directions. So that would be a very cumbersome number of additional
elements, almost doubling the storage, if done directly. Needs thought.

2. the address of the cij (or ijk pointer) of the mesh node that this
object belongs to. This would enable one to scan the obj data, and refer
back to the mesh, rather than having to scan the mesh and refer to the 
obj data. 

To store the latter in a real is somewhat problematic, since the mesh 
might become larger than the integer available. Probably one way to 
solve that is to use an equivalence of obj to an integer.

Another issue is the namespace that obj is in needs to be observed now
that it is in common. Start by fixing that. Done

Separated the specific objects into 3dobjects.f
Took out the save from circleset and created smtsphere.2 to match.

Object information: probably the best way to handle this is NOT to put
the additional information we might want to retain into the sor_obj
data. Instead we can optionally create and populate additional object
data structures that parallel the sor_obj and give the information.
This would be implemented in routine potlsect, where the intersections
and fractions are calculated. There's a slight problem in that isor_oi
is incremented only after potlsect has returned, so syncronizing the 
additional data with isor_oi is a little tricky. One way to syncronize
would be to pass iset to potsect, telling it thereby whether we have
already incremented the isor_oi or not. Then one just writes the additional
data into isor_oi-1+iset. If we are storing the pointer to cij, we
may also have to pass that to potsect. The parallel information could
be integer.

11 Jan 07
Implementing general boundary conditions. 
Converted the fixed boundary to general form. Some thoughts about
how to pass the a,b,c. 

Also need to pass back more information for the continuity condition.
Do that by changing from passing in dpm the fractional distance. Instead
pass the total distance, so that dp = dpm(i)*fraction(i) etc.
Unfortunately this changes the rounding. So store this result as
smtsphere.3

The general condition does not seem to be working correctly. There's a 
problem with the inactive side, it seems. Need better diagnostics.
Implemented surfmark. This seems to show that the mismatch is really 
occurring at the surface, not on adjacent points. It also shows the directions
of the connections are not what one might intuit from the surface plot.
So you have to be careful.

13 Jan 07

Finally found my problem. It was that I was storing Cd in the object instead
of the DIFFERENCE Cd-Cij, which is what I should have been storing. 

Adjusted the namespace control by changing to a suffix rather than a prefix
_sor. This enables variables to determine their type in the usual way.
objcom.f fixed. ctl_sor also fixed.

Change to using a proper 0-1 range for the variables. Replace smt.out
to smtsphere.4 smtround.3.

Doing numerical checks, I find that the logarithmic derivative setting
is not giving correct solutions. I find that adjusting the coefficients
inside cijroutine can correct. It seems that the problem is with my
implementation of the form for the general BC. I make d+ equal to 
apb+b/apb times dxp1, but then for the opposite direction, I have not used
d- equal to this same value. I have used just dxp1. That is incorrect.

Rebuilt the cij routine using the proper values for dminus (and the full
continuity expression). Gives correct answers 40x40x16 has maximum
error for the 1/r case of .0032 and SD .001. 
For 40x40x40
 Max error=  0.00121030153  Standard Deviation=  0.000311983633
This seems to be second order small.
For 80x80x80 the oi_sor overflowed. Doubling it solves. then
 Max error= -0.000307714043  Standard Deviation=  9.30291717E-05
Reduced by faster than dx. Not quite factor of 4 in SD, but yes in max.
Looks as if we are getting second order accuracy.
For 20x20x20
 Max error= -0.00420894753  Standard Deviation=  0.00136070442
Definitely looks like dx^2.
For 10x10x10
 Max error= -0.0172496308  Standard Deviation=  0.00511154439
For 100x100x100
 Max error= -0.000233839804  Standard Deviation=  7.77521054E-05
This is getting closer to the eps convergence:
 mi_sor,k_sor,xjac_sor,del_sor 1210 401  0.999440014  1.9293886E-05 0
Wrote plotconv to plot the convergence of these cases. Very clearly
quadratic except for the 100 case where we seem to be getting less than
quadratic reduction, which is not surprising since we are not necessarily
converging the iterations to significantly better accuracy.

Running quantitative test case again finds rounding differences, but
zero rounded differences. So consolidate in smtsphere.5, smtround.5

Summary: We have the scheme working and tested with both fixed potential
and logarithmic gradient boundary conditions. 

To do: media. Rethinking stored data (for charge assignment, object
identification, reverse reference to cij, etc).

Longer term ideas: curvilinear coordinates.

What to store in the object data? Prior idea was fraction, potential. 
This works for a fixed potential boundary and gives enough information
to generate an interpolated position. With other types of boundary it
is not so clear what one should store. 

18 Jan 07

Begun adjustment of the object data. First generalize the code so
that ndata_sor is a parameter that describes the amount of data per
direction. Ensure that it works for ndata_sor=3, even if we are only
using the prior two spaces. Of course, at present the data is only 
being used by the plotting routines for a graphic display.

Then I can add 1 to the start of each of the object data descriptor
pointers:
      parameter (ndims_sor=3,ndata_sor=3)
      parameter (nobj_sor=1+2*ndims_sor*ndata_sor+3)
      parameter (idgs_sor=1+2*ndims_sor*ndata_sor+1)
      parameter (ibdy_sor=1+2*ndims_sor*ndata_sor+2)
      parameter (iflag_sor=1+2*ndims_sor*ndata_sor+3)

Need to correct things all the way through sormpi.f. Probably it is better
to remove the process of passing the object data through and rely on 
external calls. So implement ddn_sor(ip,dden,dnum) that adjusts the numerator
and denominator. Unfortunately this gives rounding error changes.
Make smt.6

Remove the passing of nobj, and obj from the sormpi and sorrelaxgen calls.
This is now entirely in the common and its effects within the sormpi call
are purely the dden and dnum adjustment via ddn_sor(ip,dden,dnum). There is
negligible hit on the speed.

27 Jan 07

Planning and development of box handling for incorporation of surface
descriptions through box analysis.

boxedge iterates over the edges of a box in levels going out from the
	base node 000... 

gaussij is a modification of the numerical recipes gauss-jordan elimination
	to make it robust to singular matrices. 

Before incorporating these into the sor, it is probably best to implement
the c/a, b/a aspects of the code. Did that.

Now got a call to boxedge going after the cijs have been set. It seems
to be giving sensible results. For example the replacement of intersections
shows that when replacement of a real intersection takes place (more often
than is nice) it is because of rounding in late digits. 

Developed wireframe plotting diagnostics in cijplot to tell if I am
doing the right thing. Plotted the true intersections. They look fine,
although there are box recuts that I don't understand being
tried. Perhaps that is a sign that I need a better points-chosing
scheme.  This does not validate the rest of the fractions,
though. Need some inspiration on how to verify the other fractions in
a graphic way.

31 Jan 07

Changed additional fraction calculations to use all the points on levels 
up to that being examined, and svdsol to fit a plane to them. This gives
results the same within rounding as the gaussij when the number of points
is 3, and similar results otherwise. 

There is still an issue about recutting the cell. 
I still do not seem to have a clear verification that I have got the
additional fractions correct. 

An idea. The plane equation is \sum fn_i^-1 x_i = \sum a_i x_i= 1. 

For a sphere this ought to be a tangential plane (approximately). We
could test whether it is by evaluating the distance of the sphere
center from the plane, which is (\sum a_i.xc_i -1)/|a|, and seeing if
this is equal to the sphere radius.
 
Note that any box that has one of its fractions unset (=1.) should be
considered to have no plane crossing it. Thus each unset fraction 
rules out all boxes in that direction. If a node has three unset fractions
then there is only one of its boxes that is cut. There are some of these
among the Added. But there are others that have more than 3 fractions set.

3 Feb 07

There seem to be persistent erroneous boxes among the vast majority of 
correct boxes. I am pursuing the idea that these arise because of the
overloading of the fraction settings. If all three of the fractions
for a particular box happen to get set by the adjacent boxes, then this
box will erroneously be intepreted as having a plane intersecting it.

The only way out of this that I see is to use the flags to indicate whether
a box is in fact cut by a plane or not. There are 2**dims possible boxes.
Thus if we use all of the 32 bits of a 4-byte word, we can cope with 
2**5, i.e. 5-dimensions. 3-D requires only 8 bits. The intrinsic 
btest(n,ipos) yields true if bit pos of integer n is set. This extension
is in the mil-spec and available on practically all compilers. There is 
also an intrinsic ibset(n,ipos) and ibclr(n,ipos) (I think I got those right)
Their availability is less certain. They are not mentioned in current
gfortran documentation. 

After substantial additional work, I found that the main problem was that
the handling was in the wrong place in boxedge and hence incorrect.
Moving it to the proper end of level gives good results. 
If we use the test that we don't account for planes that fail to intersect
the cell, we get only about 40 additional pointers for the correct planes.
We get no recuts. We get no errors above .01. Also we find that only 
6-intersection cases are found. This is the effect of the cut criterion.
If we remove the cut criterion, we get many more additional points which
have 3,4, or 5 intersections.

I think we have the thing working now. Flags signal whether a box is
relevant or not. They are set only according to the boxedge call.

The next thing to do is to implement the field (gradient) interpolation.

6 Feb 07

Further cases with different radii shows there are still some problems
to sort out. I had a test that replaced the fraction only if b/a,c/a
are zero. If you have a zero potential setting. That is the case even
for previously found fractions. Then problems arise. Therefore, I ought
to reset the fraction in the box code only if it has not been already
set, i.e. not adjust it smaller in succeeding calls, as had been the 
previous algorithm. I think that algorithm was supposed to handle
cases where it had been set by previous box calls (not direct fractions). 
Now choose only to permit fraction setting if fraction =1., i.e. it has
not previously been set at all.


2 Jun 07
Converted the object code to depend on reading an input file in which
different types of object can be specified. Also coded the ability
to detect whether we are inside (or outside) such objects.

9 June 07
Got the gradient interpolation working with correction of the node about
which interpolation is done to use the fraction information to determine
when we need to use something other than the nearest node. 
Packaged this into gradinterpcorrect. So the call is packaged.

15 Jun 07

Added ipointer to the idob_sor data: a reverse pointer back to the
u/c arrays. Created routine:  indexexpand(ndims,ifull,index,ix)
to obtain the multi-D index ix, from the pointer index.
Then added routine iregioninit(ndims,ifull) to initialize the 
iregion flags for the idob_sor to the insideall value telling what
region they are in. It cycles over all the object data that is
created by the cijroutine, uses indexexpand.

Thus we now have ipoint_sor and iregion_sor set to sensible values
after calling cijroutine and iregioninit. Then at any interpolation,
we can access the region of any point that has object-boundary data.
A point that does not have such data is not adjacent to any boundary.
Therefore, it is safe to assume that it is in the same region as
a nearby point that we might wish to compare with. 

20 Jun 07
Attempting to compare the gradinterpregion call with the other, we find
that the iregion values are apparently incorrect. Misaligned by 1 in
all dimensions. This appears to be because when the cijroutine is called,
it is called by mditerate, and this is relative to the origin
(2,2,2) in the 3-d cij array. Consequently the reverse pointer is 
set incorrectly. Or, to put it another way, the reverse pointer is
relative to the (2,2,2) origin. Fix this in the reverse lookup during
the regioninit. Now gradinterpregion gives identical results.

22 Jun 07
Got getfield working. It gives the same value as the raw calls. Also
seems to give plausible looking interpolations.

23 Jun07
Constructed fieldtest which plots a field profile along a radius of the
sphere and a chosen angle. It shows there's a bug in getfield. 
Made get3simple to do totally simple box interpolation with no 
gradient extrapolation and neglect of boundaries. 
Found the bug in getfield logic. Incorrectly handling the construction
of the inputs to boxinterp for general dimensions. Temporarily use
the 3-D version from get3simple for iteration. That works and now things
agree away from the boundaries.

16 July 07

Getting back to this. The fieldtest code is close to working, but it
currently does not do interpolation correctly, because it produces
cases where iflags(1)=0, contradicting the assumptions of the
box2interp version. 

Extended box2interp to cope with f00 absent. Then tests show that the 
errors are reduced by a factor of 2 in the prior worst cases. So this
really helps (getting the interpolation right). 

18 July 07

Implemented an extrapolation scheme that is used for a box in which
more than two nodes are absent. If an absent node has precisely one
(of two) neighbors present then extrapolate through the present node
using the value one step further away to the desired (absent) node.
Otherwise do nothing because if only one node is absent (i.e. a node
has two present neighbors) we are already doing the sensible extrapolation.

This code has the effect of reducing errors in E-field by about a
factor of 2 when it kicks in. But there are fairly significant errors
in which it does not. Generally we find now that the field error is
less than about 10% for the 16x16x20 mesh near the sphere
boundary. Since the boundary is at r=0.2, which defines the
scale-length of the phi-solution there, and the node spacing is 1/16 or 1/20.
which is 0.05 (optimistically), the ratio of node spacing to characteristic 
scale-length is 0.25. Therefore if we get 10% errors we aren't doing
badly. Going to twice the mesh numbers (half the spacing) the errors drop to 
about 2-3% max. Which seems to be falling like (s/l)^2 (or at least faster
than linearly). There was a strange result with 
 fieldtest -p -p1 -t2.4
some kind of error inside the sphere but not too bad.

Looks like the difference between using extrapolation or not is still only
about a factor of 2 at smaller s. The extrapolation does not change the 
order of interpolation (as expected). That order is presumably that we
are correct to first order, since we are doing linear E-interpolation.
The error being order (s/l)^2 thus makes sense.


23 Aug 07

Started padvnc.f for advancing.
Updated getfield to use general number of dimensions.
Removed extrapolation from it, but saved old version.
Removed passing of iLsc, just multiply iLs by (2*ndims+1).


24 Aug 07

Further cleaning and confirmation that padvnc is working.
Changed getfield so that it is permissible to pass the full mesh
position, not just the fractional mesh position. This gives a version
that can either be used by passing the local origin of arrays, or
be used with the global origin and full mesh position.

There is a puzzle about the following. It seem to be using incorrect 
offset (ought to be (ix-1)*iLs ) but it works as is.

      do id=1,ndims_mesh
c Offset to start of dimension-id-position array.
         ioff=ixnp(id)
c xn is the position array for each dimension arranged linearly.
c Find the index of xprime in the array xn:
         ix=interp(xn(ioff+1),ixnp(id+1)-ioff,x_part(id,i),xm)
         xfrac(id)=xm-ix
         x_part(ndimsx2+id,i)=xm
         ixp(id)=ix
         iu=iu+ix*iLs(id)
      enddo

Interp returns a value >=1. But if we have ix=1, that is the first 
value in the array, so the offset should be zero and iu zero.
In the interpolate.f file (ix-1) is used, correctly.
In padvnc.f it is not. The getfield calculation in fieldtest appears
to treat things correctly, because it uses u(ix,ix,ix) explicitly.

I think this is bound up with the fact that I've assumed that we can
treat the shifted/fractional and full position cases the same, but
perhaps we can't since the fractional treatment passes 0.5 but the
full position value at 0.5 is 1.5 reading from a lowest index of 1.

Actually I think the previous result was wrong and the present one is
right. Played around with the padvnc printing out the force and comparing
with analytic. The corrected results are within about 2%, but the orbit
closure stinks.

Yes this is the correct alignment. The orbit closure improves lots
as one goes to finer mesh. This agrees quite well with analytic approx.
Fixed some little problems. I think we can declare padvnc working 
using (ix-1) everywhere.

25 Aug 07

Completed the initial chargetomesh assignment code.

Thinking about how to handle changes to the boundary potential between
steps, which would be needed for floating cases. Introduced a new
iinter flag in dob_sor to indicate which object was intersected. (Not 
yet populated.)

There seem to be three levels of cij update that would make sense, in
increasing level of computational cost.

1. Directly scale the C/A and ibdy components by the changed potential
for a specific object whose potential is varying. Assumes that objects
are not moving and that the changing BC is fixed potential. And that
each node that intersects the changing object intersects no others.
Involves searching the existing dob_sor data.

2. Rerun cijroutine for those nodes which intersect a changed object.
Assumes objects not moving, but would work for arbitrary changes to 
BC(s). Just operate on the existing dob_sor data.

3. Do 2, but in addition, search nodes in the neighborhood of a moving
object to determine nodes that become boundary which weren't before
(and presumably those that stop being boundary but were before). 
This requires substantially more effort to be sure you don't miss new
nodes. But is still short of re-searching the entire mesh, which would
be very costly.

A further possibility would be to limit the intersection investigation to
only the object(s) that have changed. This might be a significant time
saving. In other words, ignore the other objects in calculating intersections.

Implemented writing of the object number into iinter_sor. Test is
that we use cijplot to plot wireframe with color equal to the number.
Works.

27 Aug 07

Looking into MPI id's and communication, with a view being able 
conveniently to call the setup before starting the sormpi solution.
This enables me to know what my process is etc.
There are currently two separate initializations. 

1. bbdydefine just sets the iorig vector which tells the block sizes
for the specified arrays and process arrangement

2. first call to bbdy sets up the communications and initializes MPI.

It hardly seems necessary to have these separate calls.
There's an issue about how iorig is saved. Currently it is only
saved during the operation of sormpi (lost on return). 
Certainly there's no need to do what is currently done which is
to call bbdydefine anew each time sormpi is called. It really ought
only to be needed the first time.

There is a general save in bbdy which is where most things are saved
(although apparently not iorig because it's an argument). I see no
real reason why iorig should not be saved. Indeed there does not seem
to be any reason why it normally should be accessed outside of 
bbdy, so it might simply be defined in a common (in case access is needed)
within bbdy, and not passed to it. Then bbdydefine would be called by
bbdy itself, not separately. If we keep the arguments to bbdy the same,
then iorig has to be saved in sormpi. That's a problem because if we
call bbdy outside of sormpi, and then think we've done setup, it won't
then be given to sormpi for subsequent storage, unless we reinitialize,
which would be a bad thing.

Better to delete iorig from the bbdy arguments. Places:
mpibbdy.f mpibbdytest.f sormpi.f bbdydecl.f
However, we also then need to add ifull to the bbdy arguments
otherwise the initialization doesn't know what they are.

Done all that. mpibbdytest works (2 processes). fieldtest works.
Fieldtest had no modifications to it to handle this change. 
The whole interface is unchanged in respect of anything outside sormpi.

Added the code for special case kc=-2 purely (re)initialization.
A prior call with kc=-2 will set up the communicator but not try
to actually do any communication.

This all works, but there's still a problem with calling the bbdy
directly, that is that all its (other) arguments don't end up in the
sormpi places, which is where they are needed. Probably the best
thing is to make the bbdy call through sormpi somehow. The integer
switch ictl is the right place to do this. 

Implemented that using the 3rd bit of ictl.

Found a big problem with the initial call causing sorrelaxgen errors
and segfaults. Traced one issue to new usage of iLs which is not 
correct now because in bbdy bbdydecl of integer iLs(ndims+1) has ndims
as a parameter but iLs not passed. That's ok in bbdytest, but not in
sormpi. The error is not detected by the compiler and then shows
up later only when the save is required. In general there's an issue
with using bbdydecl in sormpi because a number of the bbdydecl parameters
are not passed in to sormpi. Therefore they are really local definitions
and can't be sized by anything other than a parameter. 

Fixed that for now with a hack to make it a parameter. Annoying compiler
bug!

28 Aug 07

Initialization of 32x32x40 now takes about 4 seconds.  When thinking
about the initialization, and possible reinitialization if there are
boundary changes, it is clear that for a multiprocessing environment,
we ought to make each processor do its own cij initialization.  This
potentially makes a big time saving.

However, the way things are currently set up, the cartesian layout
is hidden from the main routine, but the main routine calls mditerate
of cijroutine. Thus the information is not available to divide the 
work between different processors.

There's another issue which is that cijroutine uses an indexed array
of object data. As it generates the array, the presumption is that
there is a 1-1 correspondence. If the cijroutine work were divided
between processors, then the would be a 1-many correspondence. The
object data corresponding to a particular pointer would refer to
different mesh- locations in different processors. That would have a
possible benefit in that the object data storage would be
distributed. But it would cause a problem in that we could not simply
gather the data together.  That is a MAJOR problem because the
interpolation routines currently assume that there's a cij with
corresponding pointers covering the whole of the volume. It would
perhaps be possible to segment the object data by offsetting the
pointer for each processor so as to keep the data separate. Then we
could gather the cij back to central location.  Unless we did this, I
shudder to think how we'd proceed. The cost is the segmentation of the 
object data storage, which might make it less efficient. The benefit
is presumably speed of recalculating cij.

Basically we need access to the iorig information in order to do much
with other processors, such as mditerate or iregioninit. Actually that
can be accessed through iorigcom. We might also need information about
our place in the cartesian communicator; this would be in the form of
iobindex or myorig. 

Looking into the partreduce and other mpi code. There's some awkward 
shuffling in sceptic. This is not necessary because using the argument
MPI_IN_PLACE instead of the send buffer causes in-place reduction, 
which avoids all the problems of shuffling. Much more elegant.

Implemented a basic mditerarg of psumtoq to calculate the charge
density. But we still need a way to calculate rhoinfinity which is passed.

Put a new solve of sormpi in the stepping loop. It shows a blip of
charge giving rise to a potential peak. (And also causes the particle
to veer off in a different direction. Probably that is some sort of
image-charge effect). 

On a single processor, a 32x32x40 solve of about 100 iterations takes ~1sec.
Not bad, although the resolution is not that great yet.

Working on horace. Found several compatibility problems. (gfortran)
1. Makefile does not use implied patterns correctly. Seems to be something
   I don't understand about the match-anything rule.
2. I made incorrect assumption about o implying integer. Gfortran detects
   that error and one with pwr2nd, because of checking for real arguments.
   Fixed.

Succeeded in making. Got some complaints about blanket saves. Seems to be
caused by saves in mpif.h.
Also got diagnostics about unused argument variables.

29 Aug 07

Corrected the sign of field to make attraction of the particle to image
charges correct. Things seem fine with last night's changes.
Code will compile and run on unity. There it is back to g77 not gfortran,
I think.

Reinjection. We need to have some sort of approach on this. There needs
to be some reinjection boundary identified, and some way to determine
where on that boundary and with what velocity the particles are going
to be reinjected. 
A spherical boundary is basically the sceptic situation.
A rectangular boundary might also be of interest.
Probably these should simply be provided as subroutines.

Incorporated a somewhat modified version of the old reinject.f routine
from sceptic. Sphere. Seems to work but not quantitatively
verified. The advancing by a partial step is to be incorporated into
the padvnc code. 

Next thing: Loading particles. I'm not wonderfully happy with sceptic
on this. But I don't know if there's a better way. 

30 Aug 07
Implemented pinit. But there's some kind of bug that causes it to hang
in some situations. That was incorrect iregion_part value.

31 Aug 07
Idea about different particle species. We could use if_part to identify
different species 0: no particle 1: species 1, ... They padvnc could be
adjusted to advance each species differently, and reinjection could likewise
reinject different species. This would make it essentially trivial to
generalize the code to make it PIC electrons (for example) or multiple
ion species.

Branched to ccpic as main. Then began packaging the diagnostics so
that we can clean up the code, and use consistent mesh sizes etc.
I realize we need to worry about the wrong field we are getting at the
edge. This is an error that needs to be understood and fixed.

1 Sep 07
Trying to fix the edge incorrect field. First, try to set the iregion
flags in all the edge object data. This sort of works, but I find that
there's a problem with the fact that the reverse pointer is in various
places assumed to be addressed relative to (1,2,2,2). This was because
of the problems with mditerate. 

Therefore change the way that mditerate is called with cijroutine.
This needs us to fix the passing of the ipin to mditerate to do the 
right thing, and to calculate the right indi and save the indinp
in mditerate.

Then there are two places where the 2,2,2 assumption must be removed.
One is in 3dobjects.f iregioninit, called by objstart.
The other is in cijroutine itself. Both fixed.

Now the edge iregion is initialized with iregion as well as the
intersections.  This improves the field diagnostic plot a great deal,
leaving only a tiny region actually outside the outer object where the
field is incorrect. I still don't quite understand why that's there.
I guess there's no reason it shouldn't be, since the presumption is that
getfield returns only the field corresponding to the iregion specified.
Actually, no, I understand this, inside solu3plot the iregion is set
to be the insideall of the actual position being plotted (and without
that one gets rubbish). So we are telling getfield that we want the 
value as if one were in the region in question. Improved solu3plot
to show the region.

Trying out non-uniform mesh. There are big problems. The autocolorcont
is now incorrect. But more important, the getfield is plainly incorrect
and it looks as if there might be a bug in getting the spacing. At least
the solution looks plausible!

3 Sep 07

Fixed the contouring.
Fixed getfield. It was indexing the position array slightly wrong.
Now things seem to be correct. However, I think a numerically 
analytic orbit ought to be examined to prove that we are getting the
correct orbit.

4 Sep 07

Did an installation of an exact circular orbit. Seems to give rather
good accuracy after 100 steps with dt from .1 to 1. (0.7 to 7
turns). In the vicinity of 1-2 % accuracy in the r over that
evolution. About 5% deviations with 400 steps (28 revolutions), but
still only 1-2% for 400 steps 3 revolutions (dt=.1). Thus there's some
effective collisionality but it is not terribly great. Allows many tens
of revolutions. This all with 32x32x40 mesh r=1-5.

Tried 100,000 particles with dt=1. Goes at about 2 iterations per second.
200,000 goes at about 1s per step. No significant difference with dt=.1
This is without -ffortran-bounds-check.

Using profiling and gprof we get

 22.81      3.17     3.17 23890368     0.00     0.00  gradinterp_
 15.11      5.27     2.10 24594336     0.00     0.00  gradlocalregion_
 14.75      7.32     2.05  6148584     0.00     0.00  getfield_
 10.54      8.79     1.47  6763776     0.00     0.00  circlesect_
  6.73      9.72     0.94  6148584     0.00     0.00  interp_

Most of the time is being spent getting the field and in its interpolation.
If we put a short-cut into gradlocalregion to do the calculation directly
if the icp0 is zero (not a boundary), then we get:
 time   seconds   seconds    calls   s/call   s/call  name    
 31.21      4.38     4.38 24594336     0.00     0.00  gradlocalregion_
 16.33      6.67     2.29  6148584     0.00     0.00  getfield_
 12.02      8.35     1.69  6763776     0.00     0.00  circlesect_
  7.49      9.40     1.05  8993020     0.00     0.00  inside_geom__
  6.78     10.35     0.95  6148584     0.00     0.00  interp_
  6.06     11.20     0.85      314     0.00     0.00  sorrelaxgen_
  4.49     11.83     0.63  3739985     0.00     0.00  gradinterp_

There's not much savings of time. The shortcut has added about 16%
(2.3s) to gradlocalregion, which came out of gradinterp. There's no
substantial total savings (<3%). This shows that it is really the
calculations that are costing us:

         uprime= (2.*x+dx0)/(dx0+dx1) * (up-u0)/dx1
     $        +(dx1-2.*x)/(dx0+dx1) * (u0-um)/dx0

If we reorganize that expression to
         uprime= ((2.*x+dx0) * (up-u0)/dx1
     $        +(dx1-2.*x) * (u0-um)/dx0)/(dx0+dx1)
We get
 time   seconds   seconds    calls   s/call   s/call  name    
 30.70      3.91     3.91 24594336     0.00     0.00  gradlocalregion_
 16.20      5.97     2.06  6148584     0.00     0.00  getfield_
  9.79      7.21     1.25  6763776     0.00     0.00  circlesect_
  8.22      8.26     1.05  8993020     0.00     0.00  inside_geom__
  7.47      9.21     0.95  6148584     0.00     0.00  interp_
  6.29     10.01     0.80      314     0.00     0.00  sorrelaxgen_
  3.85     10.50     0.49  3739985     0.00     0.00  gradinterp_
A small saving.
Similarly, we get a small saving from using icp0=cij(1) instead of cij(ix).
Reorganizing test gets time to 3.74s. Another small saving. But we have
not done much. Probably only a 10% saving.

The profiling conclusion is that the main cost is the parabolic
interpolation of the field-gradient, which is called four times for
each dimension for each particle at each step. By fiddling around 
inside that (gradlocalregion) implementing two conditional short-cuts,
I shaved about 30% off the routine for uniform scaling. Everything else
seems pretty marginal, and is not worth effort at this time.

time   seconds   seconds    calls   s/call   s/call  name    
 23.07      2.85     2.85 24594336     0.00     0.00  gradlocalregion_
 17.28      4.98     2.13  6148584     0.00     0.00  getfield_
 10.83      6.31     1.34  6763776     0.00     0.00  circlesect_
  7.38      7.22     0.91      314     0.00     0.00  sorrelaxgen_
  7.22      8.11     0.89  6148584     0.00     0.00  interp_
  7.18      9.00     0.89  8993020     0.00     0.00  inside_geom__
  6.12      9.75     0.76  3739985     0.00     0.00  gradinterp_

5 Sep 07

Reorganized some files to clean up the main program and put plotting
and some other stuff elsewhere. 

Checked the cvs that you really can build from it. 

Now we need to get the volumes correct for calculating the charge
density.  This is trivial for the core nodes. Only difficulty is for
those on object boundaries. In cijroutine and boxedge we do some
elaborate calculation of fractions, which are supposed to represent
the points that define a plane approximation to the bounding surface
cutting a particular box, in the case that the plane actually cuts the
cell surrounding the box (which it can do even if fractions are >1).
However for the CIC particle assignment, a particle contributes
partially to a node if it is in the _box_ (not just in the node-cell,
which extends to the half-node-spacing position). Therefore, it is not
clear that I have done the relevant calculation. There will be some
degree of volume reduction for every node that lies in the center of a
2x2x2 box which is intersected by the boundary.

Things would be different for NGP assignment. Then particles contribute
only if they are in the node-cell, in which case the fractions might have
some value. 

In either case, there is a problem in that currently particles are 
removed based on the iregion. That is not a planar approximation. 
If volumes are calculated on a different basis from the particle removal,
the charge-density calculation will not be done correctly. It would
therefore be inconsistent to use direct iregion evaluations for particles
but fraction-approximations for volumes.

One alternative approach would be a Monte-Carlo integration of the
volume by multiple calls to insideall. If the weighting were uniform
(which it is not) then this amounts to a random choice of region in or
out. So n points are distributed according to a binomial. The average
of a sample therefore has a mean number: np (where p is the
probability of acceptance) and variance np(1-p), so that S.D./mean =
sqrt((1-p)/np), and the uncertainty as a fraction of the total box
volume is sqrt(p(1-p)/n)<=sqrt(1/n)/2.  Consequently, to get 1%
accuracy requires n=50^2=2500. Rather a lot of random points (3 ran0
calls per point). If we used a uniform mesh in each direction, with
this total number of points, we would have 2500^1/3= 14 per direction.
That would give a worse result in terms of uncertainty (probably). 
Monte-Carlo is better in dimensions greater than 2.


11 Sep 07

Started volintegrate and volnode for node volume calculations.
Transitioned active code to using mditerarg instead of mditerate.
However, haven't prumed all mditerate because the calls of mditerarg
are a bit hokey, since they use inconsistent number of arguments.
This should not matter, since the arguments are at the end. However
it probably violates fortran standards. So probably ought to think
about making the calls consistent.

Calls to volintegrate amount to 2152, according to gprof, which is a
major hit on initiation: about 8 seconds. This is for the 16x16x20
mesh and 10000 points for the montecarlo integration. Total pointers
used is 2800, but that includes the edge. As mesh size increases
the boundary points scales like n^2. So a 100x100 mesh would 
probably be about 6^2=36 times longer. One approach might be to 
do this work shared between the nodes. That would reduce it back to
about 8 seconds. Of the used time, about half is calls to inside_geom
which is the routine that is called for each object to determine if
the point is inside it. 

It might well be worth saving the volume data in a file, to be read
back in, in most cases. Done, and seems to work.

12 Sep 07

Implemented a text plot of volume percentage. Seems to show things are
working ok (except for the external volumes). 

Increased the npoints parameter for monte-carlo to 10^5. This reduces the
random errors to <1% as expected. 

Made the volume for nodes external to active region 100%. But perhaps
the more appropriate thing would be to use their region for their
volume, or possibly a very large number so that the charge density
becomes negligible.

Discovered that if the network is in a certain type of unconnected state
then program takes for ever to initialize. This appears to be an MPI
problem.

How accurate does the volume have to be?

Well, the accuracy of the charge density is the real question. But
that has statistical fluctuations at a fractional level of 1/sqrt(N),
where N is the number of particles per cell. Thus, from a purely
statistical viewpoint, all we really need is to calculate the volume
from a monte-carlo sample of points much larger than the number of
particles in the cell. If we have a million cells (the upper range of
what might currently be managable) then 7 million particles would give
only 7 per cell.  That would make the use of 10000 points per cell
overkill.  By contrast, a 20x20x20 mesh has 8000 cells, and so we
would have a maximum of about 1000 particles per cell for this very
small mesh. Therefore, 10000 points is certainly enough to make the
statistical noise from volume calculation less than that from
particles.

There's something of a compensating factor: the particle number might
be averaged over many different realizations, while the volume remains
fixed and does not improve with averaging. Consequently, one might 
find that such quantities as the flux distribution, when averaged over
many steps, might show significant effects from cell volume errors.

14 Sep 07

Implemented a rhoinfcalc based on smax flux. Run with 1000000 particles
and zero sphere potential, it gives about 4000 reinjections per step
when it should give about 2400 or so. Thus the density inside is 
considerably less than rhoinf. Played around with this. Eventually found
that the problem is we don't have electrons turned on and so this gives
a strongly positive potential even with zero on the sphere.

We need to introduce debyelen and set it to some sensible value.
The solution of L\phi = q means that we either have to put the debyelen
scaling into the difference stencil or into q. That is we either have
to consider the equation to be \lambda^2 L\phi =\rho, 
or L\phi = \rho/\lambda^2.
The latter means that the "density" is stored as density/lambda^2.
The former means that the cij coefficients are scaled by \lambda^2.
From a computational viewpoint either is probably fine.
Actually I notice that the sceptic solver adjusts the overrelaxation
according as lambda is large or small. Probably this is not necessary
for ccpic, but might speed up things. In sceptic, I multiplied the 
poisson coefficients by debyelen^2. Perhaps that's what I should do here.
It's slightly awkward because dpm is passed up from the geometry routines.
I don't think the debyelen should be put into them. 

So put it into cijroutine. Then we can get small potential by making
the debyelen very long (with zero potential on sphere). Then we get
correct numbers of reinjections and sensible rho approximately one.
The sor convergence is a bit slower like this. We are committed to 
using mditerarg by this choice. Found that there's a problem, namely
that when setting derivative, there also needs to be a debylen 
factor in this approach. Seems to be fixed by scaling b as well.

Now we have a segfault from the -gt case. OK fixed that it was the 
adjustment I had made to slice3plot. But we still don't have the 
interpolation correct now. Ok one needed to fix another place in
cijroutine.

Need now to turn on the electrons. Doing this things are very broken
for few particles, but seem to work for many. Also there's a problem
outside the active region, where faddu is being added, though it 
shouldn't be. It might be possible to put the q equal to 1 in the 
outside region. That would cancel out the electron function.
But it looks like being a major bother.

18 Sep 07
Actually it is not a problem because volumes is set to a very large 
number outside the active region. This is easily used in psumtoq to
set q=1. Seems to work.

20 Sep 07

Now we'd like to get the code going with constant number of particles being
injected per unit time. This might be a more satisfactory solution than
calculating rhoinfinity each step based on the number that happens to be
reinjected. 

Done that. The total number of particles shows a decay over about 100 steps
of .1, by about 10%. This is presumably the readjustment of the initially
uniform density to reflect the proper final spatial distribution. 
Now we have the choice of specifying -ni the number of ions or specifying
-ri the rhoinfinity (density of ions at infinity). May not have the 
multiprocess numbers set correctly yet.

I wonder if there's a way to speed up the convergence to the final density
other than what we've always used in sceptic, which is large initial steps.
Once we vary the dt, we will need to vary the ninjcomp accordingly. 

Observed that there's an error in u inside the sphere. This arises because
the electron density is not being correctly compensated there. Changing
the setting of rho to be faddu(u) for an external region helps, but does
not fix this. The error mostly goes away if rho is set to zero. This is
a serious puzzle. Seems as if the solver is incorrect in the inner
sphere. It is possible there's an error in the algorithm for the case
when faddu is present. 

It's really bizarre, but putting the rho equal to minus faddu in the
inner region gives a perfectly flat profile. But in the outer region
you need +faddu. Explain that! Actually it seems to be an accident.
It does not work for other sphere potentials.

Found a bug in the faddu part of sorrelaxgen.f. Fixed it. But it's a
bit worrying for sceptic. It's ok. Sceptic uses a hard-wired 2-D sor,
not the sorrelaxgen. I think we now have a correct compensation for
electron density in the external regions.

Fixed some sliceplot bugs in rotation.

21 Sep 07

Things to do next maybe.

Reading and Writing the code state, fields and particles. Probably we
should keep the fields and particles in separate files since the fields
only need to be written by master, while each node must write its own
particles. How do we ensure that random numbers are the same thereafter
for a restart?

Tracking and saving particle exits (and entrances?). This depends to
some extent on the type of object. So we need a uniform way of
handling the documentation of flux distributions. This requires an api
for the object.

MPI communication of particle parameters. 

22 Sep 07

One thing slowing down my decisions is the problem of the overwhelming
amount of data. Even to save the fluxes at each step is going to take
a substantial amount if we do it for every object of 32. Of course we
will rarely or never want to do it for so many objects. Probably we
should not use a structured obj_flux(n_fluxmax,ngeomobjmax) data store
for the object storage, because this is going to be very inefficient.
Instead we should probably use a big storage space accessed by a dynamic
structure. E.g. only providing for the number of objects that actually
exists, or even the number for which we care about the flux. If that's
the case, then the main thing to specify is how that data is structured.
It could be structured the same in a binary output file.

27 Sep 07

Flux data might be structured as follows:

Level	Description					Name	No of values
0	Number of objects for which data is stored.	nof		1
0	Address of object header starts.		iof(nof)	nof

1	Number of quantities stored for this object.	nqf		1
1	Address of quantity starts.			iqf(nqf)	nqf

2	Number of positions for this quantity.		npf		1
2	Quantity descriptors/positions			qdf		npf

3	Timesteps							1

----- End of header----

3	Flux Data of quantity (nqf)					npf

When addressing a place to write flux data it is:

buff(time0+(time-1)*sum_{nof}(nqf*npf)+sum_{iof}(nqf*npf)+sum_{iqf}(npf)+ipf)

Thought again about using HDF5. However, this is really a library. There
does not seem to be any intention for it to be a data structure API for
simple programming. Therefore the only real reason for using HDF5 is to
make the data portable to someone else's calling of the HDF5 libraries.
That's not really what I am looking for. The principles of HDF5 do
embody some of what I am looking for: self-describing data strutures etc.

Perhaps we could reasonably choose the number of quantities to be fixed
so that we simply only need to specify the objects for which we are storing.
Then, the positions could be variable. 


29 Sep 07

Implemented structures in 3dcom.f and initializations in fluxdata.f.
Found that it was block data operating on big common blocks that caused
the very large executable. So removed that and put explicit initialization.

Implemented tallyexit in padvnc, with rudimentary particle flux counting.
It seems to work. Can subsequently obtain the fluxes.

Implement outputflux routine and corresponding readfluxfile routine to
write and read-back the flux data. 

Implement fluxdatatest to see that we really can read it back independently.
